{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o3J38vgH58W"
      },
      "source": [
        "# 4. LangChain の基礎\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YPfSvY5JH58X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Fu83XZH58Y"
      },
      "source": [
        "## 4.1. LangChain の概要\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3PURVpqH58Y"
      },
      "source": [
        "### LangChain のインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkG_wD9eH58Y",
        "outputId": "365061a7-169a-435e-d5bb-593c0c3069c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-core==0.3.0\n",
            "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-openai==0.2.0\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (1.33)\n",
            "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core==0.3.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (2.10.6)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core==0.3.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.0) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.3.0)\n",
            "Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, tiktoken, langsmith, langchain-core, langchain-openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.11\n",
            "    Uninstalling langsmith-0.3.11:\n",
            "      Successfully uninstalled langsmith-0.3.11\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.41\n",
            "    Uninstalling langchain-core-0.3.41:\n",
            "      Successfully uninstalled langchain-core-0.3.41\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.20 requires langchain-core<1.0.0,>=0.3.41, but you have langchain-core 0.3.0 which is incompatible.\n",
            "langchain-text-splitters 0.3.6 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 0.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.3.0 langchain-openai-0.2.0 langsmith-0.1.147 tenacity-8.5.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yYHjRniH58Y"
      },
      "source": [
        "### LangSmith のセットアップ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "VmZLepbyH58Y"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0aRGW62H58Y"
      },
      "source": [
        "## 4.2. LLM / Chat model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksP4btbVH58Y"
      },
      "source": [
        "### LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.493540Z",
          "iopub.status.busy": "2024-06-28T02:32:34.493370Z",
          "iopub.status.idle": "2024-06-28T02:32:36.949739Z",
          "shell.execute_reply": "2024-06-28T02:32:36.949284Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6JWyvvH58Y",
        "outputId": "5e519699-6c8e-4779-c786-94336464f49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "こんにちは\n",
            "\n",
            "こんにちは、私はAIのアシスタントです。あなたのお手伝いをすることができます。何かお困りのことはありますか？\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "ai_message = model.invoke(\"こんにちは\")\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyarBeD6H58Z"
      },
      "source": [
        "### Chat model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:36.951724Z",
          "iopub.status.busy": "2024-06-28T02:32:36.951536Z",
          "iopub.status.idle": "2024-06-28T02:32:39.034259Z",
          "shell.execute_reply": "2024-06-28T02:32:39.033764Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_d7HgWOH58Z",
        "outputId": "a1d7c44b-c2ff-4aaf-95db-a32bb2f46713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "はい、あなたの名前はジョンさんです。何か特別なことについてお話ししたいですか？\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"こんにちは！私はジョンと言います\"),\n",
        "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "    HumanMessage(content=\"私の名前がわかりますか？\"),\n",
        "]\n",
        "\n",
        "ai_message = model.invoke(messages)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkzWX6aTH58Z"
      },
      "source": [
        "### ストリーミング\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:39.036893Z",
          "iopub.status.busy": "2024-06-28T02:32:39.036468Z",
          "iopub.status.idle": "2024-06-28T02:32:40.188998Z",
          "shell.execute_reply": "2024-06-28T02:32:40.188511Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4HQIoFzH58Z",
        "outputId": "aefb32ac-a11d-4e4f-9898-91b878f3ba85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは！どのようにお手伝いできますか？"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"こんにちは！\"),\n",
        "]\n",
        "\n",
        "for chunk in model.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### youtube link\n",
        "\n",
        "TestableにするためにFakeをつかっているとのこと。\n",
        "- [ ] あとで見る。\n",
        "\n",
        "[【LangChainゆる勉強会#5】LangChainのテスト関連機能を動かす【ランチタイム開催】](https://www.youtube.com/live/BX9AgTxLLHY)"
      ],
      "metadata": {
        "id": "-ZAMk8d0MeeF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRZWdiiGH58Z"
      },
      "source": [
        "## 4.3. Prompt template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a18k7FmAH58Z"
      },
      "source": [
        "### PromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.191537Z",
          "iopub.status.busy": "2024-06-28T02:32:40.191222Z",
          "iopub.status.idle": "2024-06-28T02:32:40.197096Z",
          "shell.execute_reply": "2024-06-28T02:32:40.196704Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrvoiksyH58Z",
        "outputId": "4343ae34-44c2-4ed8-b66e-1dd8e496d405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレー\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"\\\n",
        "以下の料理のレシピを考えてください。\n",
        "\n",
        "料理名: {dish}\\\n",
        "\"\"\")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXR_3k3yH58Z"
      },
      "source": [
        "#### ＜補足：プロンプトの変数が 1 つの場合＞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RbeRrrqH58Z",
        "outputId": "2c3d2009-6026-42a0-c306-b912d0d532b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレー\n"
          ]
        }
      ],
      "source": [
        "prompt_value = prompt.invoke(\"カレー\")\n",
        "print(prompt_value.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkrjvLGH58Z"
      },
      "source": [
        "### ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.199009Z",
          "iopub.status.busy": "2024-06-28T02:32:40.198811Z",
          "iopub.status.idle": "2024-06-28T02:32:40.204792Z",
          "shell.execute_reply": "2024-06-28T02:32:40.204434Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uxUuF2GH58Z",
        "outputId": "e32dc608-ec65-495f-8979-845b4e70e350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='ユーザーが入力した料理のレシピを考えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='カレー', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYij-A-tH58a"
      },
      "source": [
        "### MessagesPlaceholder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.206823Z",
          "iopub.status.busy": "2024-06-28T02:32:40.206639Z",
          "iopub.status.idle": "2024-06-28T02:32:40.213139Z",
          "shell.execute_reply": "2024-06-28T02:32:40.212764Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOHPSpH7H58a",
        "outputId": "d2f2ba2c-d96d-4528-d116-d0d4d38e91c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='こんにちは！私はジョンと言います！', additional_kwargs={}, response_metadata={}), AIMessage(content='こんにちは、ジョンさん！どのようにお手伝いできますか？', additional_kwargs={}, response_metadata={}), HumanMessage(content='私の名前が分かりますか？', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
        "            AIMessage(\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "        ],\n",
        "        \"input\": \"私の名前が分かりますか？\",\n",
        "    }\n",
        ")\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toRe-HZpH58a"
      },
      "source": [
        "### LangSmith の Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.215357Z",
          "iopub.status.busy": "2024-06-28T02:32:40.215034Z",
          "iopub.status.idle": "2024-06-28T02:32:40.850572Z",
          "shell.execute_reply": "2024-06-28T02:32:40.850086Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1bfda4XH58a",
        "outputId": "4e70a04b-77c3-4653-f1f7-054e11a1e218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='ユーザーが入力した料理のレシピを考えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='カレー', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "prompt = client.pull_prompt(\"oshima/recipe\")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1mjE712H58a"
      },
      "source": [
        "### LangSmith Prompts\n",
        "\n",
        "オリジナルコード？の公開の仕方がわからない。\n",
        "- [ ] あとで調べる\n",
        "\n",
        "### （コラム）マルチモーダルモデルの入力の扱い\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.852551Z",
          "iopub.status.busy": "2024-06-28T02:32:40.852328Z",
          "iopub.status.idle": "2024-06-28T02:32:40.984619Z",
          "shell.execute_reply": "2024-06-28T02:32:40.984231Z"
        },
        "id": "eVx_uTOQH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"user\",\n",
        "            [\n",
        "                {\"type\": \"text\", \"text\": \"画像を説明してください。\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url}\"}},\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "image_url = \"https://raw.githubusercontent.com/yoshidashingo/langchain-book/main/assets/cover.jpg\"\n",
        "\n",
        "prompt_value = prompt.invoke({\"image_url\": image_url})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.986542Z",
          "iopub.status.busy": "2024-06-28T02:32:40.986385Z",
          "iopub.status.idle": "2024-06-28T02:32:49.421870Z",
          "shell.execute_reply": "2024-06-28T02:32:49.421368Z"
        },
        "id": "TCp_03n1H58a"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "ai_message = model.invoke(prompt_value)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hfgHDtfH58a"
      },
      "source": [
        "## 4.4. Output parser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChKLqintH58a"
      },
      "source": [
        "### PydanticOutputParser を使った Python オブジェクトへの変換\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.423970Z",
          "iopub.status.busy": "2024-06-28T02:32:49.423805Z",
          "iopub.status.idle": "2024-06-28T02:32:49.427124Z",
          "shell.execute_reply": "2024-06-28T02:32:49.426747Z"
        },
        "id": "UvtxtgjUH58a"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.428896Z",
          "iopub.status.busy": "2024-06-28T02:32:49.428630Z",
          "iopub.status.idle": "2024-06-28T02:32:49.430921Z",
          "shell.execute_reply": "2024-06-28T02:32:49.430585Z"
        },
        "id": "x6nK-2VVH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.432752Z",
          "iopub.status.busy": "2024-06-28T02:32:49.432482Z",
          "iopub.status.idle": "2024-06-28T02:32:49.435573Z",
          "shell.execute_reply": "2024-06-28T02:32:49.435232Z"
        },
        "id": "MfE6jxaYH58a"
      },
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.437304Z",
          "iopub.status.busy": "2024-06-28T02:32:49.437089Z",
          "iopub.status.idle": "2024-06-28T02:32:49.440212Z",
          "shell.execute_reply": "2024-06-28T02:32:49.439868Z"
        },
        "id": "PKS7QTtEH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"ユーザーが入力した料理のレシピを考えてください。\\n\\n\"\n",
        "            \"{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_with_format_instructions = prompt.partial(\n",
        "    format_instructions=format_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.441965Z",
          "iopub.status.busy": "2024-06-28T02:32:49.441750Z",
          "iopub.status.idle": "2024-06-28T02:32:49.446830Z",
          "shell.execute_reply": "2024-06-28T02:32:49.446490Z"
        },
        "id": "bZ75eeKiH58a"
      },
      "outputs": [],
      "source": [
        "prompt_value = prompt_with_format_instructions.invoke({\"dish\": \"カレー\"})\n",
        "print(\"=== role: system ===\")\n",
        "print(prompt_value.messages[0].content)\n",
        "print(\"=== role: user ===\")\n",
        "print(prompt_value.messages[1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.448688Z",
          "iopub.status.busy": "2024-06-28T02:32:49.448466Z",
          "iopub.status.idle": "2024-06-28T02:32:53.559871Z",
          "shell.execute_reply": "2024-06-28T02:32:53.559376Z"
        },
        "id": "BNf4bft-H58b"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "ai_message = model.invoke(prompt_value)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.561786Z",
          "iopub.status.busy": "2024-06-28T02:32:53.561632Z",
          "iopub.status.idle": "2024-06-28T02:32:53.572140Z",
          "shell.execute_reply": "2024-06-28T02:32:53.571665Z"
        },
        "id": "OMkxY5FVH58b"
      },
      "outputs": [],
      "source": [
        "recipe = output_parser.invoke(ai_message)\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ4yMQBtH58b"
      },
      "source": [
        "### StrOutputParser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.614251Z",
          "iopub.status.busy": "2024-06-28T02:32:53.614030Z",
          "iopub.status.idle": "2024-06-28T02:32:53.619857Z",
          "shell.execute_reply": "2024-06-28T02:32:53.619506Z"
        },
        "id": "Ys223i9PH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "ai_message = AIMessage(content=\"こんにちは。私はAIアシスタントです。\")\n",
        "ai_message = output_parser.invoke(ai_message)\n",
        "print(type(ai_message))\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHXfSzslH58b"
      },
      "source": [
        "## 4.5.Chain―LangChain Expression Language（LCEL）の概要\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxNXW91aH58b"
      },
      "source": [
        "### prompt と model の連鎖\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.621888Z",
          "iopub.status.busy": "2024-06-28T02:32:53.621584Z",
          "iopub.status.idle": "2024-06-28T02:32:53.674625Z",
          "shell.execute_reply": "2024-06-28T02:32:53.674143Z"
        },
        "id": "wF7sVtCLH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.676366Z",
          "iopub.status.busy": "2024-06-28T02:32:53.676220Z",
          "iopub.status.idle": "2024-06-28T02:32:53.678436Z",
          "shell.execute_reply": "2024-06-28T02:32:53.678095Z"
        },
        "id": "d66SoIXJH58b"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.680152Z",
          "iopub.status.busy": "2024-06-28T02:32:53.679899Z",
          "iopub.status.idle": "2024-06-28T02:32:59.546328Z",
          "shell.execute_reply": "2024-06-28T02:32:59.544073Z"
        },
        "id": "PVScbNeRH58b"
      },
      "outputs": [],
      "source": [
        "ai_message = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tFlo6BH58b"
      },
      "source": [
        "### StrOutputParser を連鎖に追加\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:59.555286Z",
          "iopub.status.busy": "2024-06-28T02:32:59.554550Z",
          "iopub.status.idle": "2024-06-28T02:33:05.105219Z",
          "shell.execute_reply": "2024-06-28T02:33:05.104783Z"
        },
        "id": "bLnoDqKmH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "output = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4kRicSTH58b"
      },
      "source": [
        "### PydanticOutputParser を使う連鎖\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.107198Z",
          "iopub.status.busy": "2024-06-28T02:33:05.107009Z",
          "iopub.status.idle": "2024-06-28T02:33:05.110329Z",
          "shell.execute_reply": "2024-06-28T02:33:05.109967Z"
        },
        "id": "CPBaR_cfH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.112162Z",
          "iopub.status.busy": "2024-06-28T02:33:05.111890Z",
          "iopub.status.idle": "2024-06-28T02:33:05.167716Z",
          "shell.execute_reply": "2024-06-28T02:33:05.167216Z"
        },
        "id": "OMpAaEnoH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\\n\\n{format_instructions}\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_with_format_instructions = prompt.partial(\n",
        "    format_instructions=output_parser.get_format_instructions()\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind(\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.169740Z",
          "iopub.status.busy": "2024-06-28T02:33:05.169590Z",
          "iopub.status.idle": "2024-06-28T02:33:05.172044Z",
          "shell.execute_reply": "2024-06-28T02:33:05.171669Z"
        },
        "id": "8XF-RBeiH58b"
      },
      "outputs": [],
      "source": [
        "chain = prompt_with_format_instructions | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.173987Z",
          "iopub.status.busy": "2024-06-28T02:33:05.173667Z",
          "iopub.status.idle": "2024-06-28T02:33:10.259404Z",
          "shell.execute_reply": "2024-06-28T02:33:10.258892Z"
        },
        "id": "Pw1DRiKdH58b"
      },
      "outputs": [],
      "source": [
        "recipe = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0kYEJrH58b"
      },
      "source": [
        "### （コラム）with_structured_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:10.261391Z",
          "iopub.status.busy": "2024-06-28T02:33:10.261230Z",
          "iopub.status.idle": "2024-06-28T02:33:12.288341Z",
          "shell.execute_reply": "2024-06-28T02:33:12.287844Z"
        },
        "id": "MMr0vMwuH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chain = prompt | model.with_structured_output(Recipe)\n",
        "\n",
        "recipe = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6CnUQZFH58c"
      },
      "source": [
        "## 4.6.LangChain の RAG に関するコンポーネント\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZnS5XajH58c"
      },
      "source": [
        "### Document loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-XOATuVH58c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community==0.3.0 GitPython==3.1.43"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzkTtTAVH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".mdx\")\n",
        "\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "raw_docs = loader.load()\n",
        "print(len(raw_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igNkZr-9H58c"
      },
      "source": [
        "### Document transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjWwFTq8H58c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-text-splitters==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG5uQ2kzH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "docs = text_splitter.split_documents(raw_docs)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqJ29UqH58c"
      },
      "source": [
        "### Embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-tYk8ruH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2_NJu6xH58c"
      },
      "outputs": [],
      "source": [
        "query = \"AWSのS3からデータを読み込むためのDocument loaderはありますか？\"\n",
        "\n",
        "vector = embeddings.embed_query(query)\n",
        "print(len(vector))\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ARkHRrH58c"
      },
      "source": [
        "### Vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKER-Xv8H58c"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Clb_bnH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miVLRMtdH58c"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SWSEHcNH58c"
      },
      "outputs": [],
      "source": [
        "query = \"AWSのS3からデータを読み込むためのDocument loaderはありますか？\"\n",
        "\n",
        "context_docs = retriever.invoke(query)\n",
        "print(f\"len = {len(context_docs)}\")\n",
        "\n",
        "first_doc = context_docs[0]\n",
        "print(f\"metadata = {first_doc.metadata}\")\n",
        "print(first_doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMgNHrwDH58c"
      },
      "source": [
        "### LCEL を使った RAG の Chain の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gHp7uyRH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "以下の文脈だけを踏まえて質問に回答してください。\n",
        "\n",
        "文脈: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "質問: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txLjdHWQH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "output = chain.invoke(query)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JihEpn1JH58d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}