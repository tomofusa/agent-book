{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o3J38vgH58W"
      },
      "source": [
        "# 4. LangChain の基礎\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YPfSvY5JH58X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Fu83XZH58Y"
      },
      "source": [
        "## 4.1. LangChain の概要\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3PURVpqH58Y"
      },
      "source": [
        "### LangChain のインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkG_wD9eH58Y",
        "outputId": "06577615-b424-4ab1-f389-ae9c9f45e09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-core==0.3.0\n",
            "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-openai==0.2.0\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (1.33)\n",
            "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core==0.3.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (2.10.6)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core==0.3.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.2.0) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (2.3.0)\n",
            "Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, tiktoken, langsmith, langchain-core, langchain-openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.13\n",
            "    Uninstalling langsmith-0.3.13:\n",
            "      Successfully uninstalled langsmith-0.3.13\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.43\n",
            "    Uninstalling langchain-core-0.3.43:\n",
            "      Successfully uninstalled langchain-core-0.3.43\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.3.6 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 0.3.0 which is incompatible.\n",
            "langchain 0.3.20 requires langchain-core<1.0.0,>=0.3.41, but you have langchain-core 0.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.3.0 langchain-openai-0.2.0 langsmith-0.1.147 tenacity-8.5.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yYHjRniH58Y"
      },
      "source": [
        "### LangSmith のセットアップ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "VmZLepbyH58Y"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0aRGW62H58Y"
      },
      "source": [
        "## 4.2. LLM / Chat model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksP4btbVH58Y"
      },
      "source": [
        "### LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.493540Z",
          "iopub.status.busy": "2024-06-28T02:32:34.493370Z",
          "iopub.status.idle": "2024-06-28T02:32:36.949739Z",
          "shell.execute_reply": "2024-06-28T02:32:36.949284Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6JWyvvH58Y",
        "outputId": "60e6a2f5-a92d-4ff5-a8c5-1c49d1fb4c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "こんにちは\n",
            "\n",
            "こんにちは、私はAIのアシスタントです。あなたのお手伝いをすることができます。何かお困りのことはありますか？\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "ai_message = model.invoke(\"こんにちは\")\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyarBeD6H58Z"
      },
      "source": [
        "### Chat model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:36.951724Z",
          "iopub.status.busy": "2024-06-28T02:32:36.951536Z",
          "iopub.status.idle": "2024-06-28T02:32:39.034259Z",
          "shell.execute_reply": "2024-06-28T02:32:39.033764Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_d7HgWOH58Z",
        "outputId": "b1e68e53-b9a3-4b15-b736-3b7f6e824d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "はい、あなたの名前はジョンさんです。何か特別なことについてお話ししたいことがありますか？\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"こんにちは！私はジョンと言います\"),\n",
        "    AIMessage(content=\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "    HumanMessage(content=\"私の名前がわかりますか？\"),\n",
        "]\n",
        "\n",
        "ai_message = model.invoke(messages)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkzWX6aTH58Z"
      },
      "source": [
        "### ストリーミング\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:39.036893Z",
          "iopub.status.busy": "2024-06-28T02:32:39.036468Z",
          "iopub.status.idle": "2024-06-28T02:32:40.188998Z",
          "shell.execute_reply": "2024-06-28T02:32:40.188511Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4HQIoFzH58Z",
        "outputId": "aefb32ac-a11d-4e4f-9898-91b878f3ba85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは！どのようにお手伝いできますか？"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful assistant.\"),\n",
        "    HumanMessage(\"こんにちは！\"),\n",
        "]\n",
        "\n",
        "for chunk in model.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### youtube link\n",
        "\n",
        "TestableにするためにFakeをつかっているとのこと。\n",
        "- [ ] あとで見る。\n",
        "\n",
        "[【LangChainゆる勉強会#5】LangChainのテスト関連機能を動かす【ランチタイム開催】](https://www.youtube.com/live/BX9AgTxLLHY)"
      ],
      "metadata": {
        "id": "-ZAMk8d0MeeF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRZWdiiGH58Z"
      },
      "source": [
        "## 4.3. Prompt template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a18k7FmAH58Z"
      },
      "source": [
        "### PromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.191537Z",
          "iopub.status.busy": "2024-06-28T02:32:40.191222Z",
          "iopub.status.idle": "2024-06-28T02:32:40.197096Z",
          "shell.execute_reply": "2024-06-28T02:32:40.196704Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrvoiksyH58Z",
        "outputId": "b5ee9106-c04e-496b-c869-58f87db1cb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレー\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"\\\n",
        "以下の料理のレシピを考えてください。\n",
        "\n",
        "料理名: {dish}\\\n",
        "\"\"\")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXR_3k3yH58Z"
      },
      "source": [
        "#### ＜補足：プロンプトの変数が 1 つの場合＞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RbeRrrqH58Z",
        "outputId": "4dd41efd-61b8-4683-cab3-497a16fd5f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレー\n"
          ]
        }
      ],
      "source": [
        "prompt_value = prompt.invoke(\"カレー\")\n",
        "print(prompt_value.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkrjvLGH58Z"
      },
      "source": [
        "### ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.199009Z",
          "iopub.status.busy": "2024-06-28T02:32:40.198811Z",
          "iopub.status.idle": "2024-06-28T02:32:40.204792Z",
          "shell.execute_reply": "2024-06-28T02:32:40.204434Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uxUuF2GH58Z",
        "outputId": "ed5585f2-9a98-47c1-a4e3-6d078cf84ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='ユーザーが入力した料理のレシピを考えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='カレー', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYij-A-tH58a"
      },
      "source": [
        "### MessagesPlaceholder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.206823Z",
          "iopub.status.busy": "2024-06-28T02:32:40.206639Z",
          "iopub.status.idle": "2024-06-28T02:32:40.213139Z",
          "shell.execute_reply": "2024-06-28T02:32:40.212764Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOHPSpH7H58a",
        "outputId": "598d5001-2189-4d97-9551-e606fa667954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='こんにちは！私はジョンと言います！', additional_kwargs={}, response_metadata={}), AIMessage(content='こんにちは、ジョンさん！どのようにお手伝いできますか？', additional_kwargs={}, response_metadata={}), HumanMessage(content='私の名前が分かりますか？', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_value = prompt.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"こんにちは！私はジョンと言います！\"),\n",
        "            AIMessage(\"こんにちは、ジョンさん！どのようにお手伝いできますか？\"),\n",
        "        ],\n",
        "        \"input\": \"私の名前が分かりますか？\",\n",
        "    }\n",
        ")\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toRe-HZpH58a"
      },
      "source": [
        "### LangSmith の Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.215357Z",
          "iopub.status.busy": "2024-06-28T02:32:40.215034Z",
          "iopub.status.idle": "2024-06-28T02:32:40.850572Z",
          "shell.execute_reply": "2024-06-28T02:32:40.850086Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1bfda4XH58a",
        "outputId": "23b38ba2-cd58-4fe1-b743-44232c608034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='ユーザーが入力した料理のレシピを考えてください。', additional_kwargs={}, response_metadata={}), HumanMessage(content='カレー', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "prompt = client.pull_prompt(\"oshima/recipe\")\n",
        "\n",
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "print(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1mjE712H58a"
      },
      "source": [
        "### LangSmith Prompts\n",
        "\n",
        "オリジナルコード？の公開の仕方がわからない。\n",
        "- [ ] あとで調べる\n",
        "\n",
        "### （コラム）マルチモーダルモデルの入力の扱い\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.852551Z",
          "iopub.status.busy": "2024-06-28T02:32:40.852328Z",
          "iopub.status.idle": "2024-06-28T02:32:40.984619Z",
          "shell.execute_reply": "2024-06-28T02:32:40.984231Z"
        },
        "id": "eVx_uTOQH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"user\",\n",
        "            [\n",
        "                {\"type\": \"text\", \"text\": \"画像を説明してください。\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url}\"}},\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "image_url = \"https://raw.githubusercontent.com/yoshidashingo/langchain-book/main/assets/cover.jpg\"\n",
        "\n",
        "prompt_value = prompt.invoke({\"image_url\": image_url})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:40.986542Z",
          "iopub.status.busy": "2024-06-28T02:32:40.986385Z",
          "iopub.status.idle": "2024-06-28T02:32:49.421870Z",
          "shell.execute_reply": "2024-06-28T02:32:49.421368Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCp_03n1H58a",
        "outputId": "b0c891d8-41bf-44b7-be4b-33e941495b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "この画像は本の表紙です。タイトルは「ChatGPT/ LangChainによるチャットシステム構築[実践]入門」で、著者は吉田真吾と大嶋勇樹です。表紙にはカラフルな鳥のイラストが描かれています。内容としては、大規模言語モデルを本番システムで活用するための基礎知識と実践的なハンズオンについて説明しているようです。OpenAI APIやLangChainの活用方法などが含まれています。\n"
          ]
        }
      ],
      "source": [
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "ai_message = model.invoke(prompt_value)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hfgHDtfH58a"
      },
      "source": [
        "## 4.4. Output parser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChKLqintH58a"
      },
      "source": [
        "### PydanticOutputParser を使った Python オブジェクトへの変換\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.423970Z",
          "iopub.status.busy": "2024-06-28T02:32:49.423805Z",
          "iopub.status.idle": "2024-06-28T02:32:49.427124Z",
          "shell.execute_reply": "2024-06-28T02:32:49.426747Z"
        },
        "id": "UvtxtgjUH58a"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.428896Z",
          "iopub.status.busy": "2024-06-28T02:32:49.428630Z",
          "iopub.status.idle": "2024-06-28T02:32:49.430921Z",
          "shell.execute_reply": "2024-06-28T02:32:49.430585Z"
        },
        "id": "x6nK-2VVH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.432752Z",
          "iopub.status.busy": "2024-06-28T02:32:49.432482Z",
          "iopub.status.idle": "2024-06-28T02:32:49.435573Z",
          "shell.execute_reply": "2024-06-28T02:32:49.435232Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfE6jxaYH58a",
        "outputId": "4141d00e-d022-41ff-b69c-6cca8c1127ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.437304Z",
          "iopub.status.busy": "2024-06-28T02:32:49.437089Z",
          "iopub.status.idle": "2024-06-28T02:32:49.440212Z",
          "shell.execute_reply": "2024-06-28T02:32:49.439868Z"
        },
        "id": "PKS7QTtEH58a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"ユーザーが入力した料理のレシピを考えてください。\\n\\n\"\n",
        "            \"{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_with_format_instructions = prompt.partial(\n",
        "    format_instructions=format_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.441965Z",
          "iopub.status.busy": "2024-06-28T02:32:49.441750Z",
          "iopub.status.idle": "2024-06-28T02:32:49.446830Z",
          "shell.execute_reply": "2024-06-28T02:32:49.446490Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ75eeKiH58a",
        "outputId": "fc43fb6e-5c18-4f02-f6c3-482e32cabd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== role: system ===\n",
            "ユーザーが入力した料理のレシピを考えてください。\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "=== role: user ===\n",
            "カレー\n"
          ]
        }
      ],
      "source": [
        "prompt_value = prompt_with_format_instructions.invoke({\"dish\": \"カレー\"})\n",
        "print(\"=== role: system ===\")\n",
        "print(prompt_value.messages[0].content)\n",
        "print(\"=== role: user ===\")\n",
        "print(prompt_value.messages[1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:49.448688Z",
          "iopub.status.busy": "2024-06-28T02:32:49.448466Z",
          "iopub.status.idle": "2024-06-28T02:32:53.559871Z",
          "shell.execute_reply": "2024-06-28T02:32:53.559376Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNf4bft-H58b",
        "outputId": "c243a08b-f74f-455a-ca53-a9f34f6f7e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ingredients\": [\n",
            "    \"鶏肉 500g\",\n",
            "    \"玉ねぎ 2個\",\n",
            "    \"にんじん 1本\",\n",
            "    \"じゃがいも 2個\",\n",
            "    \"カレールー 1箱\",\n",
            "    \"水 800ml\",\n",
            "    \"サラダ油 大さじ2\",\n",
            "    \"塩 適量\",\n",
            "    \"こしょう 適量\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"鶏肉は一口大に切り、塩とこしょうをふる。\",\n",
            "    \"玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切る。\",\n",
            "    \"鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。\",\n",
            "    \"鶏肉を加え、表面が白くなるまで炒める。\",\n",
            "    \"にんじんとじゃがいもを加え、全体をよく混ぜる。\",\n",
            "    \"水を加え、煮立ったらアクを取り、弱火で20分煮る。\",\n",
            "    \"カレールーを加え、さらに10分煮込む。\",\n",
            "    \"味を見て、必要に応じて塩で調整する。\",\n",
            "    \"ご飯と一緒に盛り付けて完成。\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "ai_message = model.invoke(prompt_value)\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.561786Z",
          "iopub.status.busy": "2024-06-28T02:32:53.561632Z",
          "iopub.status.idle": "2024-06-28T02:32:53.572140Z",
          "shell.execute_reply": "2024-06-28T02:32:53.571665Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMkxY5FVH58b",
        "outputId": "9c858f02-521f-45d8-f16f-4c0969bd85ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量'] steps=['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り、弱火で20分煮る。', 'カレールーを加え、さらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。']\n"
          ]
        }
      ],
      "source": [
        "recipe = output_parser.invoke(ai_message)\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ4yMQBtH58b"
      },
      "source": [
        "### StrOutputParser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.614251Z",
          "iopub.status.busy": "2024-06-28T02:32:53.614030Z",
          "iopub.status.idle": "2024-06-28T02:32:53.619857Z",
          "shell.execute_reply": "2024-06-28T02:32:53.619506Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys223i9PH58b",
        "outputId": "84e39b5e-e518-472a-e540-aed12194d879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "こんにちは。私はAIアシスタントです。\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "ai_message = AIMessage(content=\"こんにちは。私はAIアシスタントです。\")\n",
        "ai_message = output_parser.invoke(ai_message)\n",
        "print(type(ai_message))\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHXfSzslH58b"
      },
      "source": [
        "## 4.5.Chain―LangChain Expression Language（LCEL）の概要\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxNXW91aH58b"
      },
      "source": [
        "### prompt と model の連鎖\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.621888Z",
          "iopub.status.busy": "2024-06-28T02:32:53.621584Z",
          "iopub.status.idle": "2024-06-28T02:32:53.674625Z",
          "shell.execute_reply": "2024-06-28T02:32:53.674143Z"
        },
        "id": "wF7sVtCLH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.676366Z",
          "iopub.status.busy": "2024-06-28T02:32:53.676220Z",
          "iopub.status.idle": "2024-06-28T02:32:53.678436Z",
          "shell.execute_reply": "2024-06-28T02:32:53.678095Z"
        },
        "id": "d66SoIXJH58b"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:53.680152Z",
          "iopub.status.busy": "2024-06-28T02:32:53.679899Z",
          "iopub.status.idle": "2024-06-28T02:32:59.546328Z",
          "shell.execute_reply": "2024-06-28T02:32:59.544073Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVScbNeRH58b",
        "outputId": "78fec21f-367e-434a-ceb9-938ff98d2c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
            "\n",
            "### 材料（4人分）\n",
            "- 鶏肉（もも肉または胸肉）: 400g\n",
            "- 玉ねぎ: 2個\n",
            "- にんじん: 1本\n",
            "- じゃがいも: 2個\n",
            "- カレールー: 1箱（約200g）\n",
            "- 水: 800ml\n",
            "- サラダ油: 大さじ2\n",
            "- 塩: 適量\n",
            "- 胡椒: 適量\n",
            "- お好みでガーリックパウダーや生姜: 適量\n",
            "\n",
            "### 作り方\n",
            "1. **材料の下ごしらえ**:\n",
            "   - 鶏肉は一口大に切り、塩と胡椒を振って下味をつけます。\n",
            "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
            "\n",
            "2. **炒める**:\n",
            "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
            "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
            "\n",
            "3. **野菜を加える**:\n",
            "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
            "\n",
            "4. **煮る**:\n",
            "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、蓋をして中火にし、約15分煮ます。\n",
            "\n",
            "5. **カレールーを加える**:\n",
            "   - 火を止めてカレールーを加え、よく溶かします。再び弱火にし、10分ほど煮込みます。お好みでガーリックパウダーや生姜を加えて風味を調整します。\n",
            "\n",
            "6. **仕上げ**:\n",
            "   - 味を見て、必要であれば塩で調整します。全体がなじんだら火を止めます。\n",
            "\n",
            "7. **盛り付け**:\n",
            "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
            "\n",
            "### おすすめのトッピング\n",
            "- チーズ\n",
            "- 生卵（温泉卵や目玉焼き）\n",
            "- 青ねぎやパセリの刻んだもの\n",
            "\n",
            "この基本のカレーはアレンジがしやすいので、野菜や肉を変えて自分好みのカレーを楽しんでください！\n"
          ]
        }
      ],
      "source": [
        "ai_message = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(ai_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tFlo6BH58b"
      },
      "source": [
        "### StrOutputParser を連鎖に追加\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:59.555286Z",
          "iopub.status.busy": "2024-06-28T02:32:59.554550Z",
          "iopub.status.idle": "2024-06-28T02:33:05.105219Z",
          "shell.execute_reply": "2024-06-28T02:33:05.104783Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLnoDqKmH58b",
        "outputId": "e68d3d5e-318e-4454-f634-f27096e708bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カレーのレシピをご紹介します！以下は基本的なチキンカレーのレシピです。\n",
            "\n",
            "### 材料（4人分）\n",
            "- 鶏もも肉：400g\n",
            "- 玉ねぎ：2個\n",
            "- にんにく：2片\n",
            "- 生姜：1片\n",
            "- トマト：1個（またはトマト缶400g）\n",
            "- カレーパウダー：大さじ2\n",
            "- 塩：小さじ1\n",
            "- 黒胡椒：少々\n",
            "- サラダ油：大さじ2\n",
            "- 水：400ml\n",
            "- ココナッツミルク（お好みで）：200ml\n",
            "- パクチー（お好みで）：適量\n",
            "\n",
            "### 作り方\n",
            "1. **下ごしらえ**：\n",
            "   - 鶏もも肉は一口大に切り、塩と黒胡椒を振っておきます。\n",
            "   - 玉ねぎは薄切り、にんにくと生姜はみじん切りにします。\n",
            "   - トマトは角切りにします。\n",
            "\n",
            "2. **炒める**：\n",
            "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
            "   - にんにくと生姜を加え、香りが立つまでさらに炒めます。\n",
            "\n",
            "3. **鶏肉を加える**：\n",
            "   - 鶏もも肉を鍋に加え、表面が白くなるまで炒めます。\n",
            "\n",
            "4. **スパイスを加える**：\n",
            "   - カレーパウダーを加え、全体に絡めるように炒めます。\n",
            "\n",
            "5. **トマトと水を加える**：\n",
            "   - トマトを加え、全体を混ぜたら水を加えます。沸騰したら、弱火にして蓋をし、約20分煮込みます。\n",
            "\n",
            "6. **仕上げ**：\n",
            "   - お好みでココナッツミルクを加え、さらに5分煮込みます。味を見て、必要であれば塩で調整します。\n",
            "\n",
            "7. **盛り付け**：\n",
            "   - お皿に盛り付け、お好みでパクチーを散らして完成です。\n",
            "\n",
            "### 提供方法\n",
            "ご飯やナンと一緒にお召し上がりください。辛さが足りない場合は、チリパウダーやハラペーニョを加えて調整できます。\n",
            "\n",
            "お楽しみください！\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "output = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4kRicSTH58b"
      },
      "source": [
        "### PydanticOutputParser を使う連鎖\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.107198Z",
          "iopub.status.busy": "2024-06-28T02:33:05.107009Z",
          "iopub.status.idle": "2024-06-28T02:33:05.110329Z",
          "shell.execute_reply": "2024-06-28T02:33:05.109967Z"
        },
        "id": "CPBaR_cfH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.112162Z",
          "iopub.status.busy": "2024-06-28T02:33:05.111890Z",
          "iopub.status.idle": "2024-06-28T02:33:05.167716Z",
          "shell.execute_reply": "2024-06-28T02:33:05.167216Z"
        },
        "id": "OMpAaEnoH58b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\\n\\n{format_instructions}\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt_with_format_instructions = prompt.partial(\n",
        "    format_instructions=output_parser.get_format_instructions()\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind(\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.169740Z",
          "iopub.status.busy": "2024-06-28T02:33:05.169590Z",
          "iopub.status.idle": "2024-06-28T02:33:05.172044Z",
          "shell.execute_reply": "2024-06-28T02:33:05.171669Z"
        },
        "id": "8XF-RBeiH58b"
      },
      "outputs": [],
      "source": [
        "chain = prompt_with_format_instructions | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:05.173987Z",
          "iopub.status.busy": "2024-06-28T02:33:05.173667Z",
          "iopub.status.idle": "2024-06-28T02:33:10.259404Z",
          "shell.execute_reply": "2024-06-28T02:33:10.258892Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw1DRiKdH58b",
        "outputId": "1bd7e4b5-d27e-4fd6-a5da-7637b2073bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量'] steps=['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんとじゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り除く。', '弱火にして、蓋をして約20分煮る。', 'カレールーを加え、よく溶かしてさらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。']\n"
          ]
        }
      ],
      "source": [
        "recipe = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recipe.ingredients)\n",
        "print(recipe.steps)\n",
        "print(recipe.schema_json)\n",
        "print(recipe.dict)\n",
        "print(recipe.model_dump_json)\n",
        "print(recipe.json)"
      ],
      "metadata": {
        "id": "7l2D0dvN_vWb",
        "outputId": "53cbe7ef-166a-464f-afc4-4008209c80d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量']\n",
            "['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんとじゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り除く。', '弱火にして、蓋をして約20分煮る。', 'カレールーを加え、よく溶かしてさらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。']\n",
            "<bound method BaseModel.schema_json of <class '__main__.Recipe'>>\n",
            "<bound method BaseModel.dict of Recipe(ingredients=['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量'], steps=['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんとじゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り除く。', '弱火にして、蓋をして約20分煮る。', 'カレールーを加え、よく溶かしてさらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。'])>\n",
            "<bound method BaseModel.model_dump_json of Recipe(ingredients=['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量'], steps=['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんとじゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り除く。', '弱火にして、蓋をして約20分煮る。', 'カレールーを加え、よく溶かしてさらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。'])>\n",
            "<bound method BaseModel.json of Recipe(ingredients=['鶏肉 500g', '玉ねぎ 2個', 'にんじん 1本', 'じゃがいも 2個', 'カレールー 1箱', '水 800ml', 'サラダ油 大さじ2', '塩 適量', 'こしょう 適量'], steps=['鶏肉は一口大に切り、塩とこしょうをふる。', '玉ねぎは薄切り、にんじんとじゃがいもは一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを炒めて透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、全体をよく混ぜる。', '水を加え、煮立ったらアクを取り除く。', '弱火にして、蓋をして約20分煮る。', 'カレールーを加え、よく溶かしてさらに10分煮込む。', '味を見て、必要に応じて塩で調整する。', 'ご飯と一緒に盛り付けて完成。'])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0kYEJrH58b"
      },
      "source": [
        "### （コラム）with_structured_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:10.261391Z",
          "iopub.status.busy": "2024-06-28T02:33:10.261230Z",
          "iopub.status.idle": "2024-06-28T02:33:12.288341Z",
          "shell.execute_reply": "2024-06-28T02:33:12.287844Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMr0vMwuH58c",
        "outputId": "e9a155dd-e02d-4294-b7a1-1262c193e402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['鶏肉', '玉ねぎ', 'にんじん', 'じゃがいも', 'カレールー', '水', 'サラダ油', '塩', 'こしょう'] steps=['鶏肉を一口大に切り、塩とこしょうで下味をつける。', '玉ねぎを薄切りにし、にんじんとじゃがいもを一口大に切る。', '鍋にサラダ油を熱し、玉ねぎを透明になるまで炒める。', '鶏肉を加え、表面が白くなるまで炒める。', 'にんじんとじゃがいもを加え、さらに炒める。', '水を加え、沸騰させた後、アクを取り除く。', '蓋をして、中火で約15分煮る。', 'カレールーを加え、さらに5分煮込む。', '全体がなじんだら、火を止めて盛り付ける。']\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chain = prompt | model.with_structured_output(Recipe)\n",
        "\n",
        "recipe = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6CnUQZFH58c"
      },
      "source": [
        "## 4.6.LangChain の RAG に関するコンポーネント\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZnS5XajH58c"
      },
      "source": [
        "### Document loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-XOATuVH58c",
        "outputId": "1ae0d7a0-92cd-4d13-fb3b-e2327856f7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community==0.3.0 in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: GitPython==3.1.43 in /usr/local/lib/python3.11/dist-packages (3.1.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (0.3.20)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (0.3.44)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython==3.1.43) (4.0.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.43) (5.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community==0.3.0 GitPython==3.1.43"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzkTtTAVH58c",
        "outputId": "bae085fe-59bd-4319-f1ac-3eccac979ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "405\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".mdx\")\n",
        "\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "raw_docs = loader.load()\n",
        "print(len(raw_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igNkZr-9H58c"
      },
      "source": [
        "### Document transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWwFTq8H58c",
        "outputId": "cfdc0c17-a9eb-49cd-eeb7-23a379cca06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-text-splitters==0.3.0\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters==0.3.0) (0.3.44)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (2.10.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (2.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters==0.3.0) (1.3.1)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-text-splitters\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.20 requires langchain-text-splitters<1.0.0,>=0.3.6, but you have langchain-text-splitters 0.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-text-splitters-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-text-splitters==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG5uQ2kzH58c",
        "outputId": "a57ee21c-bfb2-4cb9-95fc-dd5ee01965f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6803, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3302, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1851, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1639, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 9269, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2579, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 17814, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1700, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1135, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1126, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1098, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1433, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1300, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1166, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1351, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1756, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1643, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1091, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1313, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1422, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1662, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1123, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1174, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1936, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1441, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1498, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1046, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1057, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1363, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1432, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1662, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1281, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1752, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1705, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1266, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1764, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1378, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1596, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1063, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1540, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1888, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1739, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 42090, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1147, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5835, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1819, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1182, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3563, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1053, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1140, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 5053, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1006, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1651, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2096, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8206, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2933, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2134, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1915, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1129, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1021, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1393, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2357, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2373, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2989, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2736, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1123, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1048, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1083, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1014, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1044, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1012, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1438, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1141, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1036, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1137, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1113, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1337, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1162, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1158, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3603, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1330, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1188, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1170, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1443, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3138, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1121, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1076, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1487, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1045, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1419\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "docs = text_splitter.split_documents(raw_docs)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "id": "JnVRAx68Q1XP",
        "outputId": "875056b5-3dd6-4825-c4a8-5220d1e81138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='# SQL Database Chain\\n\\nThis example demonstrates the use of the `SQLDatabaseChain` for answering questions over a SQL database.\\n\\nUnder the hood, LangChain uses SQLAlchemy to connect to SQL databases. The `SQLDatabaseChain` can therefore be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, [Databricks](/docs/ecosystem/integrations/databricks.html) and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. For example, a connection to MySQL requires an appropriate connector such as PyMySQL. A URI for a MySQL connection might look like: `mysql+pymysql://user:pass@some_mysql_db_address/db_name`.\\n\\nThis demonstration uses SQLite and the example Chinook database.\\nTo set it up, follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\n```\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\n```\\n\\n**NOTE:** For data-sensitive projects, you can specify `return_direct=True` in the `SQLDatabaseChain` initialization to directly return the output of the SQL query without any additional formatting. This prevents the LLM from seeing any contents within the database. Note, however, the LLM still has access to the database scheme (i.e. dialect, table and key names) by default.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many employees are there?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there?\\n    SQLQuery:'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='/workspace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\\n      sample_rows = connection.execute(command)\\n\\n\\n    SELECT COUNT(*) FROM \"Employee\";\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees.\\n    > Finished chain.\\n\\n    \\'There are 8 employees.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Use Query Checker\\nSometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM. You can simply specify this option when creating the chain:\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many albums by Aerosmith?\")\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many albums by Aerosmith?\\n    SQLQuery:SELECT COUNT(*) FROM Album WHERE ArtistId = 3;\\n    SQLResult: [(1,)]\\n    Answer:There is 1 album by Aerosmith.\\n    > Finished chain.\\n\\n    \\'There is 1 album by Aerosmith.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Customize Prompt\\nYou can also customize the prompt that is used. Here is an example prompting it to understand that foobar is the same as the Employee table\\n\\n\\n```python\\nfrom langchain.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\nUse the following format:\\n\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"\\n\\nOnly use the following tables:\\n\\n{table_info}\\n\\nIf someone asks for the table foobar, they really mean the employee table.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='Question: {input}\"\"\"\\nPROMPT = PromptTemplate(\\n    input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\\n)\\n```\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)\\n```\\n\\n\\n```python\\ndb_chain.run(\"How many employees are there in the foobar table?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there in the foobar table?\\n    SQLQuery:SELECT COUNT(*) FROM Employee;\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees in the foobar table.\\n    > Finished chain.\\n\\n    \\'There are 8 employees in the foobar table.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Return Intermediate Steps\\n\\nYou can also return the intermediate steps of the SQLDatabaseChain. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True, use_query_checker=True, return_intermediate_steps=True)\\n```\\n\\n\\n```python\\nresult = db_chain(\"How many employees are there in the foobar table?\")\\nresult[\"intermediate_steps\"]\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many employees are there in the foobar table?\\n    SQLQuery:SELECT COUNT(*) FROM Employee;\\n    SQLResult: [(8,)]\\n    Answer:There are 8 employees in the foobar table.\\n    > Finished chain.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='[{\\'input\\': \\'How many employees are there in the foobar table?\\\\nSQLQuery:SELECT COUNT(*) FROM Employee;\\\\nSQLResult: [(8,)]\\\\nAnswer:\\',\\n      \\'top_k\\': \\'5\\',\\n      \\'dialect\\': \\'sqlite\\',\\n      \\'table_info\\': \\'\\\\nCREATE TABLE \"Artist\" (\\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Artist table:\\\\nArtistId\\\\tName\\\\n1\\\\tAC/DC\\\\n2\\\\tAccept\\\\n3\\\\tAerosmith\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Employee\" (\\\\n\\\\t\"EmployeeId\" INTEGER NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(30), \\\\n\\\\t\"ReportsTo\" INTEGER, \\\\n\\\\t\"BirthDate\" DATETIME, \\\\n\\\\t\"HireDate\" DATETIME, \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60), \\\\n\\\\tPRIMARY KEY (\"EmployeeId\"), \\\\n\\\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Employee table:\\\\nEmployeeId\\\\tLastName\\\\tFirstName\\\\tTitle\\\\tReportsTo\\\\tBirthDate\\\\tHireDate\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\n1\\\\tAdams\\\\tAndrew\\\\tGeneral Manager\\\\tNone\\\\t1962-02-18 00:00:00\\\\t2002-08-14 00:00:00\\\\t11120 Jasper Ave NW\\\\tEdmonton\\\\tAB\\\\tCanada\\\\tT5K 2N1\\\\t+1 (780) 428-9482\\\\t+1 (780) 428-3457\\\\tandrew@chinookcorp.com\\\\n2\\\\tEdwards\\\\tNancy\\\\tSales Manager\\\\t1\\\\t1958-12-08 00:00:00\\\\t2002-05-01 00:00:00\\\\t825 8 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 2T3\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-3322\\\\tnancy@chinookcorp.com\\\\n3\\\\tPeacock\\\\tJane\\\\tSales Support Agent\\\\t2\\\\t1973-08-29 00:00:00\\\\t2002-04-01 00:00:00\\\\t1111 6 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 5M5\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-6712\\\\tjane@chinookcorp.com\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Genre\" (\\\\n\\\\t\"GenreId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"GenreId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Genre table:\\\\nGenreId\\\\tName\\\\n1\\\\tRock\\\\n2\\\\tJazz\\\\n3\\\\tMetal\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"MediaType\" (\\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"MediaTypeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from MediaType table:\\\\nMediaTypeId\\\\tName\\\\n1\\\\tMPEG audio file\\\\n2\\\\tProtected AAC audio file\\\\n3\\\\tProtected MPEG-4 video file\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n3\\\\tTV Shows\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Album\" (\\\\n\\\\t\"AlbumId\" INTEGER NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(160) NOT NULL, \\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"AlbumId\"), \\\\n\\\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Album table:\\\\nAlbumId\\\\tTitle\\\\tArtistId\\\\n1\\\\tFor Those About To Rock We Salute You\\\\t1\\\\n2\\\\tBalls to the Wall\\\\t2\\\\n3\\\\tRestless and Wild\\\\t2\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Invoice\" (\\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceDate\" DATETIME NOT NULL, \\\\n\\\\t\"BillingAddress\" NVARCHAR(70), \\\\n\\\\t\"BillingCity\" NVARCHAR(40), \\\\n\\\\t\"BillingState\" NVARCHAR(40), \\\\n\\\\t\"BillingCountry\" NVARCHAR(40), \\\\n\\\\t\"BillingPostalCode\" NVARCHAR(10), \\\\n\\\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceId\"), \\\\n\\\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Invoice table:\\\\nInvoiceId\\\\tCustomerId\\\\tInvoiceDate\\\\tBillingAddress\\\\tBillingCity\\\\tBillingState\\\\tBillingCountry\\\\tBillingPostalCode\\\\tTotal\\\\n1\\\\t2\\\\t2009-01-01 00:00:00\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t1.98\\\\n2\\\\t4\\\\t2009-01-02 00:00:00\\\\tUllevålsveien 14\\\\tOslo\\\\tNone\\\\tNorway\\\\t0171\\\\t3.96\\\\n3\\\\t8\\\\t2009-01-03 00:00:00\\\\tGrétrystraat 63\\\\tBrussels\\\\tNone\\\\tBelgium\\\\t1000\\\\t5.94\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Track\" (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL, \\\\n\\\\t\"AlbumId\" INTEGER, \\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"GenreId\" INTEGER, \\\\n\\\\t\"Composer\" NVARCHAR(220), \\\\n\\\\t\"Milliseconds\" INTEGER NOT NULL, \\\\n\\\\t\"Bytes\" INTEGER, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\\\n\\\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\\\n\\\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tAlbumId\\\\tMediaTypeId\\\\tGenreId\\\\tComposer\\\\tMilliseconds\\\\tBytes\\\\tUnitPrice\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\t1\\\\t1\\\\t1\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\t343719\\\\t11170334\\\\t0.99\\\\n2\\\\tBalls to the Wall\\\\t2\\\\t2\\\\t1\\\\tNone\\\\t342562\\\\t5510424\\\\t0.99\\\\n3\\\\tFast As a Shark\\\\t3\\\\t2\\\\t1\\\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\\\t230619\\\\t3990994\\\\t0.99\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"InvoiceLine\" (\\\\n\\\\t\"InvoiceLineId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\t\"Quantity\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceLineId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from InvoiceLine table:\\\\nInvoiceLineId\\\\tInvoiceId\\\\tTrackId\\\\tUnitPrice\\\\tQuantity\\\\n1\\\\t1\\\\t2\\\\t0.99\\\\t1\\\\n2\\\\t1\\\\t4\\\\t0.99\\\\t1\\\\n3\\\\t2\\\\t6\\\\t0.99\\\\t1\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"PlaylistTrack\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from PlaylistTrack table:\\\\nPlaylistId\\\\tTrackId\\\\n1\\\\t3402\\\\n1\\\\t3389\\\\n1\\\\t3390\\\\n*/\\',\\n      \\'stop\\': [\\'\\\\nSQLResult:\\']},\\n     \\'SELECT COUNT(*) FROM Employee;\\',\\n     {\\'query\\': \\'SELECT COUNT(*) FROM Employee;\\', \\'dialect\\': \\'sqlite\\'},\\n     \\'SELECT COUNT(*) FROM Employee;\\',\\n     \\'[(8,)]\\']\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='</CodeOutputBlock>\\n\\n## Adding Memory\\n\\nHow to add memory to a SQLDatabaseChain:\\n\\n```python\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\n```\\n\\nSet up the SQLDatabase and LLM\\n\\n```python\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\n```\\n\\nSet up the memory\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nmemory = ConversationBufferMemory()\\n```\\n\\nNow we need to add a place for memory in the prompt template\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\nPROMPT_SUFFIX = \"\"\"Only use the following tables:\\n{table_info}\\n\\nPrevious Conversation:\\n{history}\\n\\nQuestion: {input}\"\"\"'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\\n\\nNever query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\n\"\"\"\\n\\nPROMPT = PromptTemplate.from_template(\\n    _DEFAULT_TEMPLATE + PROMPT_SUFFIX,\\n)\\n```\\n\\nNow let\\'s create and run out chain'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True, memory=memory)\\ndb_chain.run(\"name one employee\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    > Entering new SQLDatabaseChain chain...\\n    name one employee\\n    SQLQuery:SELECT FirstName, LastName FROM Employee LIMIT 1\\n    SQLResult: [(\\'Andrew\\', \\'Adams\\')]\\n    Answer:Andrew Adams\\n    > Finished chain.\\n\\n    \\'Andrew Adams\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n```python\\ndb_chain.run(\"how many letters in their name?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    > Entering new SQLDatabaseChain chain...\\n    how many letters in their name?\\n    SQLQuery:SELECT LENGTH(FirstName) + LENGTH(LastName) AS \\'NameLength\\' FROM Employee WHERE FirstName = \\'Andrew\\' AND LastName = \\'Adams\\'\\n    SQLResult: [(11,)]\\n    Answer:Andrew Adams has 11 letters in their name.\\n    > Finished chain.\\n\\n    \\'Andrew Adams has 11 letters in their name.\\'\\n```\\n\\n</CodeOutputBlock>'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='## Choosing how to limit the number of rows returned\\nIf you are querying for several rows of a table you can select the maximum number of results you want to get by using the \\'top_k\\' parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True, top_k=3)\\n```\\n\\n\\n```python\\ndb_chain.run(\"What are some example tracks by composer Johann Sebastian Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='> Entering new SQLDatabaseChain chain...\\n    What are some example tracks by composer Johann Sebastian Bach?\\n    SQLQuery:SELECT Name FROM Track WHERE Composer = \\'Johann Sebastian Bach\\' LIMIT 3\\n    SQLResult: [(\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',)]\\n    Answer:Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\n    > Finished chain.\\n\\n    \\'Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\'\\n```\\n\\n</CodeOutputBlock>'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='## Adding example rows from each table\\nSometimes, the format of the data is not obvious and it is optimal to include a sample of rows from the tables in the prompt to allow the LLM to understand the data before providing a final query. Here we will use this feature to let the LLM know that artists are saved with their full names by providing two rows from the `Track` table.\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\'], # we include only one table to save tokens in the prompt :)\\n    sample_rows_in_table_info=2)\\n```\\n\\nThe sample rows are added to the prompt after each corresponding table\\'s column information:\\n\\n\\n```python\\nprint(db.table_info)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='CREATE TABLE \"Track\" (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"AlbumId\" INTEGER,\\n    \\t\"MediaTypeId\" INTEGER NOT NULL,\\n    \\t\"GenreId\" INTEGER,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\t\"Milliseconds\" INTEGER NOT NULL,\\n    \\t\"Bytes\" INTEGER,\\n    \\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\n    \\tPRIMARY KEY (\"TrackId\"),\\n    \\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\n    \\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\n    \\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n    )\\n\\n    /*\\n    2 rows from Track table:\\n    TrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n    1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n    2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n    */\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, use_query_checker=True, verbose=True)\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='> Entering new SQLDatabaseChain chain...\\n    What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\", \"Composer\" FROM \"Track\" WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5\\n    SQLResult: [(\\'American Woman\\', \\'B. Cummings/G. Peterson/M.J. Kale/R. Bachman\\'), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Johann Sebastian Bach\\'), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Johann Sebastian Bach\\'), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', \\'Johann Sebastian Bach\\'), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\', \\'Johann Sebastian Bach\\')]\\n    Answer:Tracks by Bach include \\'American Woman\\', \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', and \\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\'.\\n    > Finished chain.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='\\'Tracks by Bach include \\\\\\'American Woman\\\\\\', \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\', and \\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\'.\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n### Custom Table Info\\nIn some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first `sample_rows_in_table_info` sample rows. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse or provide more information to the model. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='This information can be provided as a dictionary with table names as the keys and table information as the values. For example, let\\'s provide a custom definition and sample rows for the Track table with only a few columns:\\n\\n\\n```python\\ncustom_table_info = {\\n    \"Track\": \"\"\"CREATE TABLE Track (\\n\\t\"TrackId\" INTEGER NOT NULL,\\n\\t\"Name\" NVARCHAR(200) NOT NULL,\\n\\t\"Composer\" NVARCHAR(220),\\n\\tPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\"\"\"\\n}\\n```\\n\\n\\n```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'Track\\', \\'Playlist\\'],\\n    sample_rows_in_table_info=2,\\n    custom_table_info=custom_table_info)\\n\\nprint(db.table_info)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='CREATE TABLE \"Playlist\" (\\n    \\t\"PlaylistId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(120),\\n    \\tPRIMARY KEY (\"PlaylistId\")\\n    )\\n\\n    /*\\n    2 rows from Playlist table:\\n    PlaylistId\\tName\\n    1\\tMusic\\n    2\\tMovies\\n    */\\n\\n    CREATE TABLE Track (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\tPRIMARY KEY (\"TrackId\")\\n    )\\n    /*\\n    3 rows from Track table:\\n    TrackId\\tName\\tComposer\\n    1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n    2\\tBalls to the Wall\\tNone\\n    3\\tMy favorite song ever\\tThe coolest composer of all time\\n    */\\n```\\n\\n</CodeOutputBlock>\\n\\nNote how our custom table definition and sample rows for `Track` overrides the `sample_rows_in_table_info` parameter. Tables that are not overridden by `custom_table_info`, in this example `Playlist`, will have their table info gathered automatically as usual.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='> Entering new SQLDatabaseChain chain...\\n    What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\n    SQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\n    Answer:text=\\'You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: \"Question here\"\\\\nSQLQuery: \"SQL Query to run\"\\\\nSQLResult: \"Result of the SQLQuery\"\\\\nAnswer: \"Final answer here\"\\\\n\\\\nOnly use the following tables:\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\\\n\\\\nQuestion: What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\'\\n    You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\n    Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\n    Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\n    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='Use the following format:\\n\\n    Question: \"Question here\"\\n    SQLQuery: \"SQL Query to run\"\\n    SQLResult: \"Result of the SQLQuery\"\\n    Answer: \"Final answer here\"\\n\\n    Only use the following tables:\\n\\n    CREATE TABLE \"Playlist\" (\\n    \\t\"PlaylistId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(120),\\n    \\tPRIMARY KEY (\"PlaylistId\")\\n    )\\n\\n    /*\\n    2 rows from Playlist table:\\n    PlaylistId\\tName\\n    1\\tMusic\\n    2\\tMovies\\n    */\\n\\n    CREATE TABLE Track (\\n    \\t\"TrackId\" INTEGER NOT NULL,\\n    \\t\"Name\" NVARCHAR(200) NOT NULL,\\n    \\t\"Composer\" NVARCHAR(220),\\n    \\tPRIMARY KEY (\"TrackId\")\\n    )\\n    /*\\n    3 rows from Track table:\\n    TrackId\\tName\\tComposer\\n    1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n    2\\tBalls to the Wall\\tNone\\n    3\\tMy favorite song ever\\tThe coolest composer of all time\\n    */'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='Question: What are some example tracks by Bach?\\n    SQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\n    SQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\n    Answer:\\n    {\\'input\\': \\'What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\', \\'top_k\\': \\'5\\', \\'dialect\\': \\'sqlite\\', \\'table_info\\': \\'\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\', \\'stop\\': [\\'\\\\nSQLResult:\\']}\\n    Examples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\'Goldberg Variations\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\n    > Finished chain.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='\\'Examples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\\\\\'Goldberg Variations\\\\\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\'\\n```\\n\\n</CodeOutputBlock>\\n\\n### SQL Views\\n\\nIn some case, the table schema can be hidden behind a JSON or JSONB column. Adding row samples into the prompt might help won\\'t always describe the data perfectly.\\n\\nFor this reason, a custom SQL views can help.\\n\\n```sql\\nCREATE VIEW accounts_v AS\\n    select id, firstname, lastname, email, created_at, updated_at,\\n        cast(stats->>\\'total_post\\' as int) as total_post,\\n        cast(stats->>\\'total_comments\\' as int) as total_comments,\\n        cast(stats->>\\'ltv\\' as int) as ltv\\n\\n        FROM accounts;\\n```\\n\\nThen limit the tables visible from SQLDatabase to the created view.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\ndb = SQLDatabase.from_uri(\\n    \"sqlite:///../../../../notebooks/Chinook.db\",\\n    include_tables=[\\'accounts_v\\']) # we include only the view\\n```\\n\\n## SQLDatabaseSequentialChain\\n\\nChain for querying SQL database that is a sequential chain.\\n\\nThe chain is as follows:\\n\\n    1. Based on the query, determine which tables to use.\\n    2. Based on those tables, call the normal SQL database chain.\\n\\nThis is useful in cases where the number of tables in the database is large.\\n\\n\\n```python\\nfrom langchain_experimental.sql import SQLDatabaseSequentialChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\n```\\n\\n\\n```python\\nchain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=True)\\n```\\n\\n\\n```python\\nchain.run(\"How many employees are also customers?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseSequentialChain chain...\\n    Table names to use:\\n    [\\'Employee\\', \\'Customer\\']'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content=\"> Entering new SQLDatabaseChain chain...\\n    How many employees are also customers?\\n    SQLQuery:SELECT COUNT(*) FROM Employee e INNER JOIN Customer c ON e.EmployeeId = c.SupportRepId;\\n    SQLResult: [(59,)]\\n    Answer:59 employees are also customers.\\n    > Finished chain.\\n\\n    > Finished chain.\\n\\n    '59 employees are also customers.'\\n```\\n\\n</CodeOutputBlock>\\n\\n## Using Local Language Models\\n\\n\\nSometimes you may not have the luxury of using OpenAI or other service-hosted large language model. You can, ofcourse, try to use the `SQLDatabaseChain` with a local model, but will quickly realize that most models you can run locally even with a large GPU struggle to generate the right output.\\n\\n\\n```python\\nimport logging\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2TokenizerFast, pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM\\nfrom langchain_huggingface import HuggingFacePipeline\"), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='# Note: This model requires a large GPU, e.g. an 80GB A100. See documentation for other ways to run private non-OpenAI models.\\nmodel_id = \"google/flan-ul2\"\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, temperature=0)\\n\\ndevice_id = -1  # default to no-GPU, but use GPU and half precision mode if available\\nif torch.cuda.is_available():\\n    device_id = 0\\n    try:\\n        model = model.half()\\n    except RuntimeError as exc:\\n        logging.warn(f\"Could not run model in half precision mode: {str(exc)}\")\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\npipe = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=device_id)\\n\\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    Loading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  4.11s/it]\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\", include_tables=[\\'Customer\\'])\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, verbose=True, return_intermediate_steps=True, use_query_checker=True)\\n```\\n\\nThis model should work for very simple SQL queries, as long as you use the query checker as specified above, e.g.:\\n\\n\\n```python\\nlocal_chain(\"How many customers are there?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are there?\\n    SQLQuery:'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    SELECT count(*) FROM Customer\\n    SQLResult: [(59,)]\\n    Answer:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\\n\\n\\n    [59]\\n    > Finished chain.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='{\\'query\\': \\'How many customers are there?\\',\\n     \\'result\\': \\'[59]\\',\\n     \\'intermediate_steps\\': [{\\'input\\': \\'How many customers are there?\\\\nSQLQuery:SELECT count(*) FROM Customer\\\\nSQLResult: [(59,)]\\\\nAnswer:\\',\\n       \\'top_k\\': \\'5\\',\\n       \\'dialect\\': \\'sqlite\\',\\n       \\'table_info\\': \\'\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\',\\n       \\'stop\\': [\\'\\\\nSQLResult:\\']},\\n      \\'SELECT count(*) FROM Customer\\',\\n      {\\'query\\': \\'SELECT count(*) FROM Customer\\', \\'dialect\\': \\'sqlite\\'},\\n      \\'SELECT count(*) FROM Customer\\',\\n      \\'[(59,)]\\']}\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='</CodeOutputBlock>\\n\\nEven this relatively large model will most likely fail to generate more complicated SQL by itself. However, you can log its inputs and outputs so that you can hand-correct them and use the corrected examples for few-shot prompt examples later. In practice, you could log any executions of your chain that raise exceptions (as shown in the example below) or get direct user feedback in cases where the results are incorrect (but did not raise an exception).\\n\\n\\n```bash\\npoetry run pip install pyyaml langchain_chroma\\nimport yaml\\n```\\n\\n<CodeOutputBlock lang=\"bash\">\\n\\n```\\n    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\\n    To disable this warning, you can either:\\n    \\t- Avoid using `tokenizers` before the fork if possible\\n    \\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='11842.36s - pydevd: Sending message related to process being replaced timed-out after 5 seconds'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='Requirement already satisfied: pyyaml in /workspace/langchain/.venv/lib/python3.9/site-packages (6.0)\\n    Requirement already satisfied: chromadb in /workspace/langchain/.venv/lib/python3.9/site-packages (0.3.21)\\n    Requirement already satisfied: pandas>=1.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.0.1)\\n    Requirement already satisfied: requests>=2.28 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.28.2)\\n    Requirement already satisfied: pydantic>=1.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.7)\\n    Requirement already satisfied: hnswlib>=0.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.0)\\n    Requirement already satisfied: clickhouse-connect>=0.5.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.5.20)\\n    Requirement already satisfied: sentence-transformers>=2.2.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.2.2)\\n    Requirement already satisfied: duckdb>=0.7.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.1)\\n    Requirement already satisfied: fastapi>=0.85.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.95.1)\\n    Requirement already satisfied: uvicorn[standard]>=0.18.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.21.1)\\n    Requirement already satisfied: numpy>=1.21.6 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.3)\\n    Requirement already satisfied: posthog>=2.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1)\\n    Requirement already satisfied: certifi in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\\n    Requirement already satisfied: urllib3>=1.26 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\\n    Requirement already satisfied: pytz in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\\n    Requirement already satisfied: zstandard in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\\n    Requirement already satisfied: lz4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\\n    Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from fastapi>=0.85.1->chromadb) (0.26.1)\\n    Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\\n    Requirement already satisfied: tzdata>=2022.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2023.3)\\n    Requirement already satisfied: six>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\\n    Requirement already satisfied: monotonic>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\\n    Requirement already satisfied: backoff>=1.10.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\\n    Requirement already satisfied: typing-extensions>=4.2.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (4.5.0)\\n    Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.1.0)\\n    Requirement already satisfied: idna<4,>=2.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.4)\\n    Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.28.1)\\n    Requirement already satisfied: tqdm in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\\n    Requirement already satisfied: torch>=1.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.13.1)\\n    Requirement already satisfied: torchvision in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.14.1)\\n    Requirement already satisfied: scikit-learn in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\\n    Requirement already satisfied: scipy in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.9.3)\\n    Requirement already satisfied: nltk in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\\n    Requirement already satisfied: sentencepiece in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.98)\\n    Requirement already satisfied: huggingface-hub>=0.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.13.4)\\n    Requirement already satisfied: click>=7.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\\n    Requirement already satisfied: h11>=0.8 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\\n    Requirement already satisfied: httptools>=0.5.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\\n    Requirement already satisfied: python-dotenv>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\\n    Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\\n    Requirement already satisfied: watchfiles>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\\n    Requirement already satisfied: websockets>=10.4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.2)\\n    Requirement already satisfied: filelock in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.12.0)\\n    Requirement already satisfied: packaging>=20.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.1)\\n    Requirement already satisfied: anyio<5,>=3.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\\n    Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\n    Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (8.5.0.96)\\n    Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.10.3.66)\\n    Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\n    Requirement already satisfied: setuptools in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (67.7.1)\\n    Requirement already satisfied: wheel in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (0.40.0)\\n    Requirement already satisfied: regex!=2019.12.17 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2023.3.23)\\n    Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\\n    Requirement already satisfied: joblib in /workspace/langchain/.venv/lib/python3.9/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.2.0)\\n    Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\\n    Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (9.5.0)\\n    Requirement already satisfied: sniffio>=1.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='</CodeOutputBlock>\\n\\n\\n```python\\nfrom typing import Dict\\n\\nQUERY = \"List all the customer first names that start with \\'a\\'\"\\n\\ndef _parse_example(result: Dict) -> Dict:\\n    sql_cmd_key = \"sql_cmd\"\\n    sql_result_key = \"sql_result\"\\n    table_info_key = \"table_info\"\\n    input_key = \"input\"\\n    final_answer_key = \"answer\"\\n\\n    _example = {\\n        \"input\": result.get(\"query\"),\\n    }\\n\\n    steps = result.get(\"intermediate_steps\")\\n    answer_key = sql_cmd_key # the first one\\n    for step in steps:\\n        # The steps are in pairs, a dict (input) followed by a string (output).\\n        # Unfortunately there is no schema but you can look at the input key of the\\n        # dict to see what the output is supposed to be\\n        if isinstance(step, dict):\\n            # Grab the table info from input dicts in the intermediate steps once\\n            if table_info_key not in _example:\\n                _example[table_info_key] = step.get(table_info_key)'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='if input_key in step:\\n                if step[input_key].endswith(\"SQLQuery:\"):\\n                    answer_key = sql_cmd_key # this is the SQL generation input\\n                if step[input_key].endswith(\"Answer:\"):\\n                    answer_key = final_answer_key # this is the final answer input\\n            elif sql_cmd_key in step:\\n                _example[sql_cmd_key] = step[sql_cmd_key]\\n                answer_key = sql_result_key # this is SQL execution input\\n        elif isinstance(step, str):\\n            # The preceding element should have set the answer_key\\n            _example[answer_key] = step\\n    return _example\\n\\nexample: any\\ntry:\\n    result = local_chain(QUERY)\\n    print(\"*** Query succeeded\")\\n    example = _parse_example(result)\\nexcept Exception as exc:\\n    print(\"*** Query failed\")\\n    result = {\\n        \"query\": QUERY,\\n        \"intermediate_steps\": exc.intermediate_steps\\n    }\\n    example = _parse_example(result)'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='# print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline\\nyaml_example = yaml.dump(example, allow_unicode=True)\\nprint(\"\\\\n\" + yaml_example)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    List all the customer first names that start with \\'a\\'\\n    SQLQuery:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn('), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content=\"SELECT firstname FROM customer WHERE firstname LIKE '%a%'\\n    SQLResult: [('François',), ('František',), ('Helena',), ('Astrid',), ('Daan',), ('Kara',), ('Eduardo',), ('Alexandre',), ('Fernanda',), ('Mark',), ('Frank',), ('Jack',), ('Dan',), ('Kathy',), ('Heather',), ('Frank',), ('Richard',), ('Patrick',), ('Julia',), ('Edward',), ('Martha',), ('Aaron',), ('Madalena',), ('Hannah',), ('Niklas',), ('Camille',), ('Marc',), ('Wyatt',), ('Isabelle',), ('Ladislav',), ('Lucas',), ('Johannes',), ('Stanisław',), ('Joakim',), ('Emma',), ('Mark',), ('Manoj',), ('Puja',)]\\n    Answer:\\n\\n    /workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\n      warnings.warn(\"), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content=\"[('François', 'Frantiek', 'Helena', 'Astrid', 'Daan', 'Kara', 'Eduardo', 'Alexandre', 'Fernanda', 'Mark', 'Frank', 'Jack', 'Dan', 'Kathy', 'Heather', 'Frank', 'Richard', 'Patrick', 'Julia', 'Edward', 'Martha', 'Aaron', 'Madalena', 'Hannah', 'Niklas', 'Camille', 'Marc', 'Wyatt', 'Isabelle', 'Ladislav', 'Lucas', 'Johannes', 'Stanisaw', 'Joakim', 'Emma', 'Mark', 'Manoj', 'Puja']\\n    > Finished chain.\\n    *** Query succeeded\"), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='answer: \\'[(\\'\\'François\\'\\', \\'\\'Frantiek\\'\\', \\'\\'Helena\\'\\', \\'\\'Astrid\\'\\', \\'\\'Daan\\'\\', \\'\\'Kara\\'\\',\\n      \\'\\'Eduardo\\'\\', \\'\\'Alexandre\\'\\', \\'\\'Fernanda\\'\\', \\'\\'Mark\\'\\', \\'\\'Frank\\'\\', \\'\\'Jack\\'\\', \\'\\'Dan\\'\\',\\n      \\'\\'Kathy\\'\\', \\'\\'Heather\\'\\', \\'\\'Frank\\'\\', \\'\\'Richard\\'\\', \\'\\'Patrick\\'\\', \\'\\'Julia\\'\\', \\'\\'Edward\\'\\',\\n      \\'\\'Martha\\'\\', \\'\\'Aaron\\'\\', \\'\\'Madalena\\'\\', \\'\\'Hannah\\'\\', \\'\\'Niklas\\'\\', \\'\\'Camille\\'\\', \\'\\'Marc\\'\\',\\n      \\'\\'Wyatt\\'\\', \\'\\'Isabelle\\'\\', \\'\\'Ladislav\\'\\', \\'\\'Lucas\\'\\', \\'\\'Johannes\\'\\', \\'\\'Stanisaw\\'\\', \\'\\'Joakim\\'\\',\\n      \\'\\'Emma\\'\\', \\'\\'Mark\\'\\', \\'\\'Manoj\\'\\', \\'\\'Puja\\'\\']\\'\\n    input: List all the customer first names that start with \\'a\\'\\n    sql_cmd: SELECT firstname FROM customer WHERE firstname LIKE \\'%a%\\'\\n    sql_result: \\'[(\\'\\'François\\'\\',), (\\'\\'František\\'\\',), (\\'\\'Helena\\'\\',), (\\'\\'Astrid\\'\\',), (\\'\\'Daan\\'\\',),\\n      (\\'\\'Kara\\'\\',), (\\'\\'Eduardo\\'\\',), (\\'\\'Alexandre\\'\\',), (\\'\\'Fernanda\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Frank\\'\\',),\\n      (\\'\\'Jack\\'\\',), (\\'\\'Dan\\'\\',), (\\'\\'Kathy\\'\\',), (\\'\\'Heather\\'\\',), (\\'\\'Frank\\'\\',), (\\'\\'Richard\\'\\',),\\n      (\\'\\'Patrick\\'\\',), (\\'\\'Julia\\'\\',), (\\'\\'Edward\\'\\',), (\\'\\'Martha\\'\\',), (\\'\\'Aaron\\'\\',), (\\'\\'Madalena\\'\\',),\\n      (\\'\\'Hannah\\'\\',), (\\'\\'Niklas\\'\\',), (\\'\\'Camille\\'\\',), (\\'\\'Marc\\'\\',), (\\'\\'Wyatt\\'\\',), (\\'\\'Isabelle\\'\\',),\\n      (\\'\\'Ladislav\\'\\',), (\\'\\'Lucas\\'\\',), (\\'\\'Johannes\\'\\',), (\\'\\'Stanisław\\'\\',), (\\'\\'Joakim\\'\\',),\\n      (\\'\\'Emma\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Manoj\\'\\',), (\\'\\'Puja\\'\\',)]\\'\\n    table_info: \"\\\\nCREATE TABLE \\\\\"Customer\\\\\" (\\\\n\\\\t\\\\\"CustomerId\\\\\" INTEGER NOT NULL, \\\\n\\\\t\\\\\\n      \\\\\"FirstName\\\\\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\\\\\"LastName\\\\\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\\\\\\n      \\\\\"Company\\\\\" NVARCHAR(80), \\\\n\\\\t\\\\\"Address\\\\\" NVARCHAR(70), \\\\n\\\\t\\\\\"City\\\\\" NVARCHAR(40),\\\\\\n      \\\\ \\\\n\\\\t\\\\\"State\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"Country\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"PostalCode\\\\\" NVARCHAR(10),\\\\\\n      \\\\ \\\\n\\\\t\\\\\"Phone\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Fax\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Email\\\\\" NVARCHAR(60)\\\\\\n      \\\\ NOT NULL, \\\\n\\\\t\\\\\"SupportRepId\\\\\" INTEGER, \\\\n\\\\tPRIMARY KEY (\\\\\"CustomerId\\\\\"), \\\\n\\\\t\\\\\\n      FOREIGN KEY(\\\\\"SupportRepId\\\\\") REFERENCES \\\\\"Employee\\\\\" (\\\\\"EmployeeId\\\\\")\\\\n)\\\\n\\\\n/*\\\\n\\\\\\n      3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\t\\\\\\n      City\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\t\\\\\\n      Embraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\t\\\\\\n      São José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\t\\\\\\n      luisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\t\\\\\\n      None\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\t\\\\\\n      Tremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\t\\\\\\n      None\\\\tftremblay@gmail.com\\\\t3\\\\n*/\"'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```\\n\\n</CodeOutputBlock>\\n\\nRun the snippet above a few times, or log exceptions in your deployed environment, to collect lots of examples of inputs, table_info and sql_cmd generated by your language model. The sql_cmd values will be incorrect and you can manually fix them up to build a collection of examples, e.g. here we are using YAML to keep a neat record of our inputs and corrected SQL output that we can build up over time.'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```python\\nYAML_EXAMPLES = \"\"\"\\n- input: How many customers are not from Brazil?\\n  table_info: |\\n    CREATE TABLE \"Customer\" (\\n      \"CustomerId\" INTEGER NOT NULL,\\n      \"FirstName\" NVARCHAR(40) NOT NULL,\\n      \"LastName\" NVARCHAR(20) NOT NULL,\\n      \"Company\" NVARCHAR(80),\\n      \"Address\" NVARCHAR(70),\\n      \"City\" NVARCHAR(40),\\n      \"State\" NVARCHAR(40),\\n      \"Country\" NVARCHAR(40),\\n      \"PostalCode\" NVARCHAR(10),\\n      \"Phone\" NVARCHAR(24),\\n      \"Fax\" NVARCHAR(24),\\n      \"Email\" NVARCHAR(60) NOT NULL,\\n      \"SupportRepId\" INTEGER,\\n      PRIMARY KEY (\"CustomerId\"),\\n      FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n    )\\n  sql_cmd: SELECT COUNT(*) FROM \"Customer\" WHERE NOT \"Country\" = \"Brazil\";\\n  sql_result: \"[(54,)]\"\\n  answer: 54 customers are not from Brazil.\\n- input: list all the genres that start with \\'r\\'\\n  table_info: |\\n    CREATE TABLE \"Genre\" (\\n      \"GenreId\" INTEGER NOT NULL,\\n      \"Name\" NVARCHAR(120),\\n      PRIMARY KEY (\"GenreId\")\\n    )'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='/*\\n    3 rows from Genre table:\\n    GenreId\\tName\\n    1\\tRock\\n    2\\tJazz\\n    3\\tMetal\\n    */\\n  sql_cmd: SELECT \"Name\" FROM \"Genre\" WHERE \"Name\" LIKE \\'r%\\';\\n  sql_result: \"[(\\'Rock\\',), (\\'Rock and Roll\\',), (\\'Reggae\\',), (\\'R&B/Soul\\',)]\"\\n  answer: The genres that start with \\'r\\' are Rock, Rock and Roll, Reggae and R&B/Soul.\\n\"\"\"\\n```\\n\\nNow that you have some examples (with manually corrected output SQL), you can do few-shot prompt seeding the usual way:\\n\\n\\n```python\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate\\nfrom langchain.chains.sql_database.prompt import _sqlite_prompt, PROMPT_SUFFIX\\nfrom langchain_huggingface import HuggingFaceEmbeddings\\nfrom langchain.prompts.example_selector.semantic_similarity import SemanticSimilarityExampleSelector\\nfrom langchain_chroma import Chroma'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='example_prompt = PromptTemplate(\\n    input_variables=[\"table_info\", \"input\", \"sql_cmd\", \"sql_result\", \"answer\"],\\n    template=\"{table_info}\\\\n\\\\nQuestion: {input}\\\\nSQLQuery: {sql_cmd}\\\\nSQLResult: {sql_result}\\\\nAnswer: {answer}\",\\n)\\n\\nexamples_dict = yaml.safe_load(YAML_EXAMPLES)\\n\\nlocal_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='example_selector = SemanticSimilarityExampleSelector.from_examples(\\n                        # This is the list of examples available to select from.\\n                        examples_dict,\\n                        # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\\n                        local_embeddings,\\n                        # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\\n                        Chroma,  # type: ignore\\n                        # This is the number of examples to produce and include per prompt\\n                        k=min(3, len(examples_dict)),\\n                    )\\n\\nfew_shot_prompt = FewShotPromptTemplate(\\n    example_selector=example_selector,\\n    example_prompt=example_prompt,\\n    prefix=_sqlite_prompt + \"Here are some examples:\",\\n    suffix=PROMPT_SUFFIX,\\n    input_variables=[\"table_info\", \"input\", \"top_k\"],\\n)\\n```\\n\\n<CodeOutputBlock lang=\"python\">'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='```\\n    Using embedded DuckDB without persistence: data will be transient\\n```\\n\\n</CodeOutputBlock>\\n\\nThe model should do better now with this few-shot prompt, especially for inputs similar to the examples you have seeded it with.\\n\\n\\n```python\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, prompt=few_shot_prompt, use_query_checker=True, verbose=True, return_intermediate_steps=True)\\n```\\n\\n\\n```python\\nresult = local_chain(\"How many customers are from Brazil?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are from Brazil?\\n    SQLQuery:SELECT count(*) FROM Customer WHERE Country = \"Brazil\";\\n    SQLResult: [(5,)]\\n    Answer:[5]\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nresult = local_chain(\"How many customers are not from Brazil?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```'), Document(metadata={'source': 'cookbook/sql_db_qa.mdx', 'file_path': 'cookbook/sql_db_qa.mdx', 'file_name': 'sql_db_qa.mdx', 'file_type': '.mdx'}, page_content='> Entering new SQLDatabaseChain chain...\\n    How many customers are not from Brazil?\\n    SQLQuery:SELECT count(*) FROM customer WHERE country NOT IN (SELECT country FROM customer WHERE country = \\'Brazil\\')\\n    SQLResult: [(54,)]\\n    Answer:54 customers are not from Brazil.\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nresult = local_chain(\"How many customers are there in total?\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n\\n\\n    > Entering new SQLDatabaseChain chain...\\n    How many customers are there in total?\\n    SQLQuery:SELECT count(*) FROM Customer;\\n    SQLResult: [(59,)]\\n    Answer:There are 59 customers in total.\\n    > Finished chain.\\n```\\n\\n</CodeOutputBlock>'), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain's open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from '@theme/ThemedImage';\\nimport useBaseUrl from '@docusaurus/useBaseUrl';\"), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content='<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture'), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"The LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\"), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"If you're looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\"), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you'll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\"), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\"), Document(metadata={'source': 'docs/docs/introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_name': 'introduction.mdx', 'file_type': '.mdx'}, page_content=\"### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you're developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\"), Document(metadata={'source': 'docs/docs/people.mdx', 'file_path': 'docs/docs/people.mdx', 'file_name': 'people.mdx', 'file_type': '.mdx'}, page_content='---\\nhide_table_of_contents: true\\n---\\n\\nimport People from \"@theme/People\";\\n\\n# People\\n\\nThere are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish 🌐!\\n\\nThis page highlights a few of those folks who have dedicated their time to the open-source repo in the form of direct contributions and reviews.\\n\\n## Top reviewers\\n\\nAs LangChain has grown, the amount of surface area that maintainers cover has grown as well.\\n\\nThank you to the following folks who have gone above and beyond in reviewing incoming PRs 🙏!\\n\\n<People type=\"top_reviewers\"></People>\\n\\n## Top recent contributors\\n\\nThe list below contains contributors who have had the most PRs merged in the last three months, weighted (imperfectly) by impact.\\n\\nThank you all so much for your time and efforts in making LangChain better ❤️!\\n\\n<People type=\"top_recent_contributors\" count=\"20\"></People>\\n\\n## Core maintainers\\n\\nHello there 👋!'), Document(metadata={'source': 'docs/docs/people.mdx', 'file_path': 'docs/docs/people.mdx', 'file_name': 'people.mdx', 'file_type': '.mdx'}, page_content='We\\'re LangChain\\'s core maintainers. If you\\'ve spent time in the community, you\\'ve probably crossed paths\\nwith at least one of us already. \\n\\n<People type=\"maintainers\"></People>\\n\\n## Top all-time contributors\\n\\nAnd finally, this is an all-time list of all-stars who have made significant contributions to the framework 🌟:\\n\\n<People type=\"top_contributors\"></People>\\n\\nWe\\'re so thankful for your support!\\n\\nAnd one more thank you to [@tiangolo](https://github.com/tiangolo) for inspiration via FastAPI\\'s [excellent people page](https://fastapi.tiangolo.com/fastapi-people).'), Document(metadata={'source': 'docs/docs/_templates/integration.mdx', 'file_path': 'docs/docs/_templates/integration.mdx', 'file_name': 'integration.mdx', 'file_type': '.mdx'}, page_content='[comment: Please, a reference example here \"docs/integrations/arxiv.md\"]::\\n[comment: Use this template to create a new .md file in \"docs/integrations/\"]::\\n\\n# Title_REPLACE_ME\\n\\n[comment: Only one Tile/H1 is allowed!]::\\n\\n>\\n[comment: Description: After reading this description, a reader should decide if this integration is good enough to try/follow reading OR]::\\n[comment: go to read the next integration doc. ]::\\n[comment: Description should include a link to the source for follow reading.]::\\n\\n## Installation and Setup\\n\\n[comment: Installation and Setup: All necessary additional package installations and setups for Tokens, etc]::\\n\\n```bash\\npip install package_name_REPLACE_ME\\n```\\n\\n[comment: OR this text:]::\\n\\nThere isn\\'t any special setup for it.'), Document(metadata={'source': 'docs/docs/_templates/integration.mdx', 'file_path': 'docs/docs/_templates/integration.mdx', 'file_name': 'integration.mdx', 'file_type': '.mdx'}, page_content='[comment: The next H2/## sections with names of the integration modules, like \"LLM\", \"Text Embedding Models\", etc]::\\n[comment: see \"Modules\" in the \"index.html\" page]::\\n[comment: Each H2 section should include a link to an example(s) and a Python code with the import of the integration class]::\\n[comment: Below are several example sections. Remove all unnecessary sections. Add all necessary sections not provided here.]::\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/INCLUDE_REAL_NAME).\\n\\n```python\\nfrom langchain_community.llms import integration_class_REPLACE_ME\\n```\\n\\n## Text Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/INCLUDE_REAL_NAME).\\n\\n```python\\nfrom langchain_community.embeddings import integration_class_REPLACE_ME\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/INCLUDE_REAL_NAME).\\n\\n```python\\nfrom langchain_community.chat_models import integration_class_REPLACE_ME\\n```\\n\\n## Document Loader'), Document(metadata={'source': 'docs/docs/_templates/integration.mdx', 'file_path': 'docs/docs/_templates/integration.mdx', 'file_name': 'integration.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/document_loaders/INCLUDE_REAL_NAME).\\n\\n```python\\nfrom langchain_community.document_loaders import integration_class_REPLACE_ME\\n```'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='# arXiv\\n            \\nLangChain implements the latest research in the field of Natural Language Processing.\\nThis page contains `arXiv` papers referenced in the LangChain Documentation, API Reference,\\n Templates, and Cookbooks.\\n\\nFrom the opposite direction, scientists use `LangChain` in research and reference it in the research papers. \\n\\n`arXiv` papers with references to:\\n [LangChain](https://arxiv.org/search/?query=langchain&searchtype=all&source=header) | [LangGraph](https://arxiv.org/search/?query=langgraph&searchtype=all&source=header) | [LangSmith](https://arxiv.org/search/?query=langsmith&searchtype=all&source=header)\\n\\n## Summary'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='| arXiv id / Title | Authors | Published date 🔻 | LangChain Documentation|\\n|------------------|---------|-------------------|------------------------|\\n| `2403.14403v2` [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](http://arxiv.org/abs/2403.14403v2) | Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al. | 2024&#8209;03&#8209;21 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2402.03620v1` [Self-Discover: Large Language Models Self-Compose Reasoning Structures](http://arxiv.org/abs/2402.03620v1) | Pei Zhou, Jay Pujara, Xiang Ren,  et al. | 2024&#8209;02&#8209;06 | `Cookbook:` [Self-Discover](https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb)\\n| `2402.03367v2` [RAG-Fusion: a New Take on Retrieval-Augmented Generation](http://arxiv.org/abs/2402.03367v2) | Zackary Rackauckas | 2024&#8209;01&#8209;31 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2401.18059v1` [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](http://arxiv.org/abs/2401.18059v1) | Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al. | 2024&#8209;01&#8209;31 | `Cookbook:` [Raptor](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)\\n| `2401.15884v2` [Corrective Retrieval Augmented Generation](http://arxiv.org/abs/2401.15884v2) | Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al. | 2024&#8209;01&#8209;29 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Cookbook:` [Langgraph Crag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb)\\n| `2401.08500v1` [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](http://arxiv.org/abs/2401.08500v1) | Tal Ridnik, Dedy Kredo, Itamar Friedman | 2024&#8209;01&#8209;16 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2401.04088v1` [Mixtral of Experts](http://arxiv.org/abs/2401.04088v1) | Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al. | 2024&#8209;01&#8209;08 | `Cookbook:` [Together Ai](https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb)\\n| `2312.06648v2` [Dense X Retrieval: What Retrieval Granularity Should We Use?](http://arxiv.org/abs/2312.06648v2) | Tong Chen, Hongwei Wang, Sihao Chen,  et al. | 2023&#8209;12&#8209;11 | `Template:` [propositional-retrieval](https://python.langchain.com/docs/templates/propositional-retrieval)\\n| `2311.09210v1` [Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](http://arxiv.org/abs/2311.09210v1) | Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al. | 2023&#8209;11&#8209;15 | `Template:` [chain-of-note-wiki](https://python.langchain.com/docs/templates/chain-of-note-wiki)\\n| `2310.11511v1` [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](http://arxiv.org/abs/2310.11511v1) | Akari Asai, Zeqiu Wu, Yizhong Wang,  et al. | 2023&#8209;10&#8209;17 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Cookbook:` [Langgraph Self Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb)\\n| `2310.06117v2` [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](http://arxiv.org/abs/2310.06117v2) | Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al. | 2023&#8209;10&#8209;09 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `Template:` [stepback-qa-prompting](https://python.langchain.com/docs/templates/stepback-qa-prompting), `Cookbook:` [Stepback-Qa](https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb)\\n| `2307.15337v3` [Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation](http://arxiv.org/abs/2307.15337v3) | Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al. | 2023&#8209;07&#8209;28 | `Template:` [skeleton-of-thought](https://python.langchain.com/docs/templates/skeleton-of-thought)\\n| `2307.09288v2` [Llama 2: Open Foundation and Fine-Tuned Chat Models](http://arxiv.org/abs/2307.09288v2) | Hugo Touvron, Louis Martin, Kevin Stone,  et al. | 2023&#8209;07&#8209;18 | `Cookbook:` [Semi Structured Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)\\n| `2307.03172v3` [Lost in the Middle: How Language Models Use Long Contexts](http://arxiv.org/abs/2307.03172v3) | Nelson F. Liu, Kevin Lin, John Hewitt,  et al. | 2023&#8209;07&#8209;06 | `Docs:` [docs/how_to/long_context_reorder](https://python.langchain.com/docs/how_to/long_context_reorder)\\n| `2305.14283v3` [Query Rewriting for Retrieval-Augmented Large Language Models](http://arxiv.org/abs/2305.14283v3) | Xinbei Ma, Yeyun Gong, Pengcheng He,  et al. | 2023&#8209;05&#8209;23 | `Template:` [rewrite-retrieve-read](https://python.langchain.com/docs/templates/rewrite-retrieve-read), `Cookbook:` [Rewrite](https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb)\\n| `2305.08291v1` [Large Language Model Guided Tree-of-Thought](http://arxiv.org/abs/2305.08291v1) | Jieyi Long | 2023&#8209;05&#8209;15 | `API:` [langchain_experimental.tot](https://python.langchain.com/api_reference/experimental/tot.html), `Cookbook:` [Tree Of Thought](https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb)\\n| `2305.04091v3` [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](http://arxiv.org/abs/2305.04091v3) | Lei Wang, Wanyu Xu, Yihuai Lan,  et al. | 2023&#8209;05&#8209;06 | `Cookbook:` [Plan And Execute Agent](https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb)\\n| `2305.02156v1` [Zero-Shot Listwise Document Reranking with a Large Language Model](http://arxiv.org/abs/2305.02156v1) | Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al. | 2023&#8209;05&#8209;03 | `Docs:` [docs/how_to/contextual_compression](https://python.langchain.com/docs/how_to/contextual_compression), `API:` [langchain...LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#)\\n| `2304.08485v2` [Visual Instruction Tuning](http://arxiv.org/abs/2304.08485v2) | Haotian Liu, Chunyuan Li, Qingyang Wu,  et al. | 2023&#8209;04&#8209;17 | `Cookbook:` [Semi Structured Multi Modal Rag Llama2](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb), [Semi Structured And Multi Modal Rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb)\\n| `2304.03442v2` [Generative Agents: Interactive Simulacra of Human Behavior](http://arxiv.org/abs/2304.03442v2) | Joon Sung Park, Joseph C. O\\'Brien, Carrie J. Cai,  et al. | 2023&#8209;04&#8209;07 | `Cookbook:` [Generative Agents Interactive Simulacra Of Human Behavior](https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb), [Multiagent Bidding](https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb)\\n| `2303.17760v2` [CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society](http://arxiv.org/abs/2303.17760v2) | Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al. | 2023&#8209;03&#8209;31 | `Cookbook:` [Camel Role Playing](https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb)\\n| `2303.17580v4` [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](http://arxiv.org/abs/2303.17580v4) | Yongliang Shen, Kaitao Song, Xu Tan,  et al. | 2023&#8209;03&#8209;30 | `API:` [langchain_experimental.autonomous_agents](https://python.langchain.com/api_reference/experimental/autonomous_agents.html), `Cookbook:` [Hugginggpt](https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb)\\n| `2301.10226v4` [A Watermark for Large Language Models](http://arxiv.org/abs/2301.10226v4) | John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al. | 2023&#8209;01&#8209;24 | `API:` [langchain_community...OCIModelDeploymentTGI](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI), [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n| `2212.10496v1` [Precise Zero-Shot Dense Retrieval without Relevance Labels](http://arxiv.org/abs/2212.10496v1) | Luyu Gao, Xueguang Ma, Jimmy Lin,  et al. | 2022&#8209;12&#8209;20 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts), `API:` [langchain...HypotheticalDocumentEmbedder](https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder), `Template:` [hyde](https://python.langchain.com/docs/templates/hyde), `Cookbook:` [Hypothetical Document Embeddings](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)\\n| `2212.08073v1` [Constitutional AI: Harmlessness from AI Feedback](http://arxiv.org/abs/2212.08073v1) | Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al. | 2022&#8209;12&#8209;15 | `Docs:` [docs/versions/migrating_chains/constitutional_chain](https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain)\\n| `2212.07425v3` [Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments](http://arxiv.org/abs/2212.07425v3) | Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al. | 2022&#8209;12&#8209;12 | `API:` [langchain_experimental.fallacy_removal](https://python.langchain.com/api_reference/experimental/fallacy_removal.html)\\n| `2211.13892v2` [Complementary Explanations for Effective In-Context Learning](http://arxiv.org/abs/2211.13892v2) | Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al. | 2022&#8209;11&#8209;25 | `API:` [langchain_core...MaxMarginalRelevanceExampleSelector](https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector)\\n| `2211.10435v2` [PAL: Program-aided Language Models](http://arxiv.org/abs/2211.10435v2) | Luyu Gao, Aman Madaan, Shuyan Zhou,  et al. | 2022&#8209;11&#8209;18 | `API:` [langchain_experimental.pal_chain](https://python.langchain.com/api_reference/experimental/pal_chain.html), [langchain_experimental...PALChain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain), `Cookbook:` [Program Aided Language Model](https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb)\\n| `2210.11934v2` [An Analysis of Fusion Functions for Hybrid Retrieval](http://arxiv.org/abs/2210.11934v2) | Sebastian Bruch, Siyu Gai, Amir Ingber | 2022&#8209;10&#8209;21 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2210.03629v3` [ReAct: Synergizing Reasoning and Acting in Language Models](http://arxiv.org/abs/2210.03629v3) | Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al. | 2022&#8209;10&#8209;06 | `Docs:` [docs/integrations/tools/ionic_shopping](https://python.langchain.com/docs/integrations/tools/ionic_shopping), [docs/integrations/providers/cohere](https://python.langchain.com/docs/integrations/providers/cohere), [docs/concepts](https://python.langchain.com/docs/concepts), `API:` [langchain...create_react_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent), [langchain...TrajectoryEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain)\\n| `2209.10785v2` [Deep Lake: a Lakehouse for Deep Learning](http://arxiv.org/abs/2209.10785v2) | Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al. | 2022&#8209;09&#8209;22 | `Docs:` [docs/integrations/providers/activeloop_deeplake](https://python.langchain.com/docs/integrations/providers/activeloop_deeplake)\\n| `2205.13147v4` [Matryoshka Representation Learning](http://arxiv.org/abs/2205.13147v4) | Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al. | 2022&#8209;05&#8209;26 | `Docs:` [docs/integrations/providers/snowflake](https://python.langchain.com/docs/integrations/providers/snowflake)\\n| `2205.12654v1` [Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages](http://arxiv.org/abs/2205.12654v1) | Kevin Heffernan, Onur Çelebi, Holger Schwenk | 2022&#8209;05&#8209;25 | `API:` [langchain_community...LaserEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings)\\n| `2204.00498v1` [Evaluating the Text-to-SQL Capabilities of Large Language Models](http://arxiv.org/abs/2204.00498v1) | Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau | 2022&#8209;03&#8209;15 | `Docs:` [docs/tutorials/sql_qa](https://python.langchain.com/docs/tutorials/sql_qa), `API:` [langchain_community...SQLDatabase](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase), [langchain_community...SparkSQL](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL)\\n| `2202.00666v5` [Locally Typical Sampling](http://arxiv.org/abs/2202.00666v5) | Clara Meister, Tiago Pimentel, Gian Wiher,  et al. | 2022&#8209;02&#8209;01 | `API:` [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)\\n| `2112.01488v3` [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](http://arxiv.org/abs/2112.01488v3) | Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al. | 2021&#8209;12&#8209;02 | `Docs:` [docs/integrations/retrievers/ragatouille](https://python.langchain.com/docs/integrations/retrievers/ragatouille), [docs/integrations/providers/ragatouille](https://python.langchain.com/docs/integrations/providers/ragatouille), [docs/concepts](https://python.langchain.com/docs/concepts), [docs/integrations/providers/dspy](https://python.langchain.com/docs/integrations/providers/dspy)\\n| `2103.00020v1` [Learning Transferable Visual Models From Natural Language Supervision](http://arxiv.org/abs/2103.00020v1) | Alec Radford, Jong Wook Kim, Chris Hallacy,  et al. | 2021&#8209;02&#8209;26 | `API:` [langchain_experimental.open_clip](https://python.langchain.com/api_reference/experimental/open_clip.html)\\n| `2005.14165v4` [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165v4) | Tom B. Brown, Benjamin Mann, Nick Ryder,  et al. | 2020&#8209;05&#8209;28 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `2005.11401v4` [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](http://arxiv.org/abs/2005.11401v4) | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020&#8209;05&#8209;22 | `Docs:` [docs/concepts](https://python.langchain.com/docs/concepts)\\n| `1909.05858v2` [CTRL: A Conditional Transformer Language Model for Controllable Generation](http://arxiv.org/abs/1909.05858v2) | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019&#8209;09&#8209;11 | `API:` [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='## Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\\n\\n- **Authors:** Soyeong Jeong, Jinheon Baek, Sukmin Cho,  et al.\\n- **arXiv id:** [2403.14403v2](http://arxiv.org/abs/2403.14403v2)  **Published Date:** 2024-03-21\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Retrieval-Augmented Large Language Models (LLMs), which incorporate the\\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\\nas a promising approach to enhancing response accuracy in several tasks, such\\nas Question-Answering (QA). However, even though there are various approaches\\ndealing with queries of different complexities, they either handle simple\\nqueries with unnecessary computational overhead or fail to adequately address\\ncomplex multi-step queries; yet, not all user requests fall into only one of\\nthe simple or complex categories. In this work, we propose a novel adaptive QA\\nframework, that can dynamically select the most suitable strategy for\\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\\nbased on the query complexity. Also, this selection process is operationalized\\nwith a classifier, which is a smaller LM trained to predict the complexity\\nlevel of incoming queries with automatically collected labels, obtained from\\nactual predicted outcomes of models and inherent inductive biases in datasets.\\nThis approach offers a balanced strategy, seamlessly adapting between the\\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\\nmethods, in response to a range of query complexities. We validate our model on\\na set of open-domain QA datasets, covering multiple query complexities, and\\nshow that ours enhances the overall efficiency and accuracy of QA systems,\\ncompared to relevant baselines including the adaptive retrieval approaches.\\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\\n                \\n## Self-Discover: Large Language Models Self-Compose Reasoning Structures'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Pei Zhou, Jay Pujara, Xiang Ren,  et al.\\n- **arXiv id:** [2402.03620v1](http://arxiv.org/abs/2402.03620v1)  **Published Date:** 2024-02-06\\n- **LangChain:**\\n\\n   - **Cookbook:** [self-discover](https://github.com/langchain-ai/langchain/blob/master/cookbook/self-discover.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\\nare challenging for typical prompting methods. Core to the framework is a\\nself-discovery process where LLMs select multiple atomic reasoning modules such\\nas critical thinking and step-by-step thinking, and compose them into an\\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\\nsubstantially improves GPT-4 and PaLM 2's performance on challenging reasoning\\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\\nthe self-discovered reasoning structures are universally applicable across\\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\\ncommonalities with human reasoning patterns.\\n                \\n## RAG-Fusion: a New Take on Retrieval-Augmented Generation\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Zackary Rackauckas\\n- **arXiv id:** [2402.03367v2](http://arxiv.org/abs/2402.03367v2)  **Published Date:** 2024-01-31\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** Infineon has identified a need for engineers, account managers, and customers\\nto rapidly obtain product information. This problem is traditionally addressed\\nwith retrieval-augmented generation (RAG) chatbots, but in this study, I\\nevaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion\\ncombines RAG and reciprocal rank fusion (RRF) by generating multiple queries,\\nreranking them with reciprocal scores and fusing the documents and scores.\\nThrough manually evaluating answers on accuracy, relevance, and\\ncomprehensiveness, I found that RAG-Fusion was able to provide accurate and\\ncomprehensive answers due to the generated queries contextualizing the original\\nquery from various perspectives. However, some answers strayed off topic when\\nthe generated queries' relevance to the original query is insufficient. This\\nresearch marks significant progress in artificial intelligence (AI) and natural\\nlanguage processing (NLP) applications and demonstrates transformations in a\\nglobal and multi-industry context.\\n                \\n## RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Parth Sarthi, Salman Abdullah, Aditi Tuli,  et al.\\n- **arXiv id:** [2401.18059v1](http://arxiv.org/abs/2401.18059v1)  **Published Date:** 2024-01-31\\n- **LangChain:**\\n\\n   - **Cookbook:** [RAPTOR](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Retrieval-augmented language models can better adapt to changes in world\\nstate and incorporate long-tail knowledge. However, most existing methods\\nretrieve only short contiguous chunks from a retrieval corpus, limiting\\nholistic understanding of the overall document context. We introduce the novel\\napproach of recursively embedding, clustering, and summarizing chunks of text,\\nconstructing a tree with differing levels of summarization from the bottom up.\\nAt inference time, our RAPTOR model retrieves from this tree, integrating\\ninformation across lengthy documents at different levels of abstraction.\\nControlled experiments show that retrieval with recursive summaries offers\\nsignificant improvements over traditional retrieval-augmented LMs on several\\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\\nwith the use of GPT-4, we can improve the best performance on the QuALITY\\nbenchmark by 20% in absolute accuracy.\\n                \\n## Corrective Retrieval Augmented Generation'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Shi-Qi Yan, Jia-Chen Gu, Yun Zhu,  et al.\\n- **arXiv id:** [2401.15884v2](http://arxiv.org/abs/2401.15884v2)  **Published Date:** 2024-01-29\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Cookbook:** [langgraph_crag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_crag.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.\\n                \\n## Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Tal Ridnik, Dedy Kredo, Itamar Friedman\\n- **arXiv id:** [2401.08500v1](http://arxiv.org/abs/2401.08500v1)  **Published Date:** 2024-01-16\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Code generation problems differ from common natural language problems - they\\nrequire matching the exact syntax of the target language, identifying happy\\npaths and edge cases, paying attention to numerous small details in the problem\\nspec, and addressing other code-specific issues and requirements. Hence, many\\nof the optimizations and tricks that have been successful in natural language\\ngeneration may not be effective for code tasks. In this work, we propose a new\\napproach to code generation by LLMs, which we call AlphaCodium - a test-based,\\nmulti-stage, code-oriented iterative flow, that improves the performances of\\nLLMs on code problems. We tested AlphaCodium on a challenging code generation\\ndataset called CodeContests, which includes competitive programming problems\\nfrom platforms such as Codeforces. The proposed flow consistently and\\nsignificantly improves results. On the validation set, for example, GPT-4\\naccuracy (pass@5) increased from 19% with a single well-designed direct prompt\\nto 44% with the AlphaCodium flow. Many of the principles and best practices\\nacquired in this work, we believe, are broadly applicable to general code\\ngeneration tasks. Full implementation is available at:\\nhttps://github.com/Codium-ai/AlphaCodium\\n                \\n## Mixtral of Experts'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,  et al.\\n- **arXiv id:** [2401.04088v1](http://arxiv.org/abs/2401.04088v1)  **Published Date:** 2024-01-08\\n- **LangChain:**\\n\\n   - **Cookbook:** [together_ai](https://github.com/langchain-ai/langchain/blob/master/cookbook/together_ai.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\\nMixtral has the same architecture as Mistral 7B, with the difference that each\\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\\neach layer, a router network selects two experts to process the current state\\nand combine their outputs. Even though each token only sees two experts, the\\nselected experts can be different at each timestep. As a result, each token has\\naccess to 47B parameters, but only uses 13B active parameters during inference.\\nMixtral was trained with a context size of 32k tokens and it outperforms or\\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\\nmultilingual benchmarks. We also provide a model fine-tuned to follow\\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\\nthe base and instruct models are released under the Apache 2.0 license.\\n                \\n## Dense X Retrieval: What Retrieval Granularity Should We Use?'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Tong Chen, Hongwei Wang, Sihao Chen,  et al.\\n- **arXiv id:** [2312.06648v2](http://arxiv.org/abs/2312.06648v2)  **Published Date:** 2023-12-11\\n- **LangChain:**\\n\\n   - **Template:** [propositional-retrieval](https://python.langchain.com/docs/templates/propositional-retrieval)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Dense retrieval has become a prominent method to obtain relevant context or\\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\\non a retrieval corpus at inference time, an often-overlooked design choice is\\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\\nsentence. We discover that the retrieval unit choice significantly impacts the\\nperformance of both retrieval and downstream tasks. Distinct from the typical\\napproach of using passages or sentences, we introduce a novel retrieval unit,\\nproposition, for dense retrieval. Propositions are defined as atomic\\nexpressions within text, each encapsulating a distinct factoid and presented in\\na concise, self-contained natural language format. We conduct an empirical\\ncomparison of different retrieval granularity. Our results reveal that\\nproposition-based retrieval significantly outperforms traditional passage or\\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\\nalso enhances the performance of downstream QA tasks, since the retrieved texts\\nare more condensed with question-relevant information, reducing the need for\\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\\ninformation.\\n                \\n## Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Wenhao Yu, Hongming Zhang, Xiaoman Pan,  et al.\\n- **arXiv id:** [2311.09210v1](http://arxiv.org/abs/2311.09210v1)  **Published Date:** 2023-11-15\\n- **LangChain:**\\n\\n   - **Template:** [chain-of-note-wiki](https://python.langchain.com/docs/templates/chain-of-note-wiki)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Retrieval-augmented language models (RALMs) represent a substantial\\nadvancement in the capabilities of large language models, notably in reducing\\nfactual hallucination by leveraging external knowledge sources. However, the\\nreliability of the retrieved information is not always guaranteed. The\\nretrieval of irrelevant data can lead to misguided responses, and potentially\\ncausing the model to overlook its inherent knowledge, even when it possesses\\nadequate information to address the query. Moreover, standard RALMs often\\nstruggle to assess whether they possess adequate knowledge, both intrinsic and\\nretrieved, to provide an accurate answer. In situations where knowledge is\\nlacking, these systems should ideally respond with \"unknown\" when the answer is\\nunattainable. In response to these challenges, we introduces Chain-of-Noting\\n(CoN), a novel approach aimed at improving the robustness of RALMs in facing\\nnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\\nCoN is to generate sequential reading notes for retrieved documents, enabling a\\nthorough evaluation of their relevance to the given question and integrating\\nthis information to formulate the final answer. We employed ChatGPT to create\\ntraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\\nOur experiments across four open-domain QA benchmarks show that RALMs equipped\\nwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\\naverage improvement of +7.9 in EM score given entirely noisy retrieved\\ndocuments and +10.5 in rejection rates for real-time questions that fall\\noutside the pre-training knowledge scope.\\n                \\n## Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Akari Asai, Zeqiu Wu, Yizhong Wang,  et al.\\n- **arXiv id:** [2310.11511v1](http://arxiv.org/abs/2310.11511v1)  **Published Date:** 2023-10-17\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Cookbook:** [langgraph_self_rag](https://github.com/langchain-ai/langchain/blob/master/cookbook/langgraph_self_rag.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** Despite their remarkable capabilities, large language models (LLMs) often\\nproduce responses containing factual inaccuracies due to their sole reliance on\\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\\nknowledge, decreases such issues. However, indiscriminately retrieving and\\nincorporating a fixed number of retrieved passages, regardless of whether\\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\\ncan lead to unhelpful response generation. We introduce a new framework called\\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\\nquality and factuality through retrieval and self-reflection. Our framework\\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\\ngenerates and reflects on retrieved passages and its own generations using\\nspecial tokens, called reflection tokens. Generating reflection tokens makes\\nthe LM controllable during the inference phase, enabling it to tailor its\\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\\n13B parameters) significantly outperforms state-of-the-art LLMs and\\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in\\nimproving factuality and citation accuracy for long-form generations relative\\nto these models.\\n                \\n## Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,  et al.\\n- **arXiv id:** [2310.06117v2](http://arxiv.org/abs/2310.06117v2)  **Published Date:** 2023-10-09\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **Template:** [stepback-qa-prompting](https://python.langchain.com/docs/templates/stepback-qa-prompting)\\n   - **Cookbook:** [stepback-qa](https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** We present Step-Back Prompting, a simple prompting technique that enables\\nLLMs to do abstractions to derive high-level concepts and first principles from\\ninstances containing specific details. Using the concepts and principles to\\nguide reasoning, LLMs significantly improve their abilities in following a\\ncorrect reasoning path towards the solution. We conduct experiments of\\nStep-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe\\nsubstantial performance gains on various challenging reasoning-intensive tasks\\nincluding STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back\\nPrompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\\n                \\n## Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Xuefei Ning, Zinan Lin, Zixuan Zhou,  et al.\\n- **arXiv id:** [2307.15337v3](http://arxiv.org/abs/2307.15337v3)  **Published Date:** 2023-07-28\\n- **LangChain:**\\n\\n   - **Template:** [skeleton-of-thought](https://python.langchain.com/docs/templates/skeleton-of-thought)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** This work aims at decreasing the end-to-end generation latency of large\\nlanguage models (LLMs). One of the major causes of the high generation latency\\nis the sequential decoding approach adopted by almost all state-of-the-art\\nLLMs. In this work, motivated by the thinking and writing process of humans, we\\npropose Skeleton-of-Thought (SoT), which first guides LLMs to generate the\\nskeleton of the answer, and then conducts parallel API calls or batched\\ndecoding to complete the contents of each skeleton point in parallel. Not only\\ndoes SoT provide considerable speed-ups across 12 LLMs, but it can also\\npotentially improve the answer quality on several question categories. SoT is\\nan initial attempt at data-centric optimization for inference efficiency, and\\nshowcases the potential of eliciting high-quality answers by explicitly\\nplanning the answer structure in language.\\n                \\n## Llama 2: Open Foundation and Fine-Tuned Chat Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Hugo Touvron, Louis Martin, Kevin Stone,  et al.\\n- **arXiv id:** [2307.09288v2](http://arxiv.org/abs/2307.09288v2)  **Published Date:** 2023-07-18\\n- **LangChain:**\\n\\n   - **Cookbook:** [Semi_Structured_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n                \\n## Lost in the Middle: How Language Models Use Long Contexts\\n\\n- **Authors:** Nelson F. Liu, Kevin Lin, John Hewitt,  et al.\\n- **arXiv id:** [2307.03172v3](http://arxiv.org/abs/2307.03172v3)  **Published Date:** 2023-07-06\\n- **LangChain:**'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Documentation:** [docs/how_to/long_context_reorder](https://python.langchain.com/docs/how_to/long_context_reorder)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** While recent language models have the ability to take long contexts as input,\\nrelatively little is known about how well they use longer context. We analyze\\nthe performance of language models on two tasks that require identifying\\nrelevant information in their input contexts: multi-document question answering\\nand key-value retrieval. We find that performance can degrade significantly\\nwhen changing the position of relevant information, indicating that current\\nlanguage models do not robustly make use of information in long input contexts.\\nIn particular, we observe that performance is often highest when relevant\\ninformation occurs at the beginning or end of the input context, and\\nsignificantly degrades when models must access relevant information in the\\nmiddle of long contexts, even for explicitly long-context models. Our analysis\\nprovides a better understanding of how language models use their input context\\nand provides new evaluation protocols for future long-context language models.\\n                \\n## Query Rewriting for Retrieval-Augmented Large Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Xinbei Ma, Yeyun Gong, Pengcheng He,  et al.\\n- **arXiv id:** [2305.14283v3](http://arxiv.org/abs/2305.14283v3)  **Published Date:** 2023-05-23\\n- **LangChain:**\\n\\n   - **Template:** [rewrite-retrieve-read](https://python.langchain.com/docs/templates/rewrite-retrieve-read)\\n   - **Cookbook:** [rewrite](https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large Language Models (LLMs) play powerful, black-box readers in the\\nretrieve-then-read pipeline, making remarkable progress in knowledge-intensive\\ntasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of\\nthe previous retrieve-then-read for the retrieval-augmented LLMs from the\\nperspective of the query rewriting. Unlike prior studies focusing on adapting\\neither the retriever or the reader, our approach pays attention to the\\nadaptation of the search query itself, for there is inevitably a gap between\\nthe input text and the needed knowledge in retrieval. We first prompt an LLM to\\ngenerate the query, then use a web search engine to retrieve contexts.\\nFurthermore, to better align the query to the frozen modules, we propose a\\ntrainable scheme for our pipeline. A small language model is adopted as a\\ntrainable rewriter to cater to the black-box LLM reader. The rewriter is\\ntrained using the feedback of the LLM reader by reinforcement learning.\\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice\\nQA. Experiments results show consistent performance improvement, indicating\\nthat our framework is proven effective and scalable, and brings a new framework\\nfor retrieval-augmented LLM.\\n                \\n## Large Language Model Guided Tree-of-Thought'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Jieyi Long\\n- **arXiv id:** [2305.08291v1](http://arxiv.org/abs/2305.08291v1)  **Published Date:** 2023-05-15\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.tot](https://python.langchain.com/api_reference/experimental/tot.html)\\n   - **Cookbook:** [tree_of_thought](https://github.com/langchain-ai/langchain/blob/master/cookbook/tree_of_thought.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\\napproach aimed at improving the problem-solving capabilities of auto-regressive\\nlarge language models (LLMs). The ToT technique is inspired by the human mind's\\napproach for solving complex reasoning tasks through trial and error. In this\\nprocess, the human mind explores the solution space through a tree-like thought\\nprocess, allowing for backtracking when necessary. To implement ToT as a\\nsoftware system, we augment an LLM with additional modules including a prompter\\nagent, a checker module, a memory module, and a ToT controller. In order to\\nsolve a given problem, these modules engage in a multi-round conversation with\\nthe LLM. The memory module records the conversation and state history of the\\nproblem solving process, which allows the system to backtrack to the previous\\nsteps of the thought-process and explore other directions from there. To verify\\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\\nsignificantly increase the success rate of Sudoku puzzle solving. Our\\nimplementation of the ToT-based Sudoku solver is available on [GitHub](https://github.com/jieyilong/tree-of-thought-puzzle-solver).\\n                \\n## Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Lei Wang, Wanyu Xu, Yihuai Lan,  et al.\\n- **arXiv id:** [2305.04091v3](http://arxiv.org/abs/2305.04091v3)  **Published Date:** 2023-05-06\\n- **LangChain:**\\n\\n   - **Cookbook:** [plan_and_execute_agent](https://github.com/langchain-ai/langchain/blob/master/cookbook/plan_and_execute_agent.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large language models (LLMs) have recently been shown to deliver impressive\\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\\nreasoning steps and improve their reasoning task accuracy. To eliminate the\\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\\n\"Let\\'s think step by step\" as an input prompt to LLMs. Despite the success of\\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\\nmissing-step errors, and semantic misunderstanding errors. To address the\\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\\ntwo components: first, devising a plan to divide the entire task into smaller\\nsubtasks, and then carrying out the subtasks according to the plan. To address\\nthe calculation errors and improve the quality of generated reasoning steps, we\\nextend PS prompting with more detailed instructions and derive PS+ prompting.\\nWe evaluate our proposed prompting strategy on ten datasets across three\\nreasoning problems. The experimental results over GPT-3 show that our proposed\\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\\nreasoning problem. The code can be found at\\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\\n                \\n## Zero-Shot Listwise Document Reranking with a Large Language Model'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Xueguang Ma, Xinyu Zhang, Ronak Pradeep,  et al.\\n- **arXiv id:** [2305.02156v1](http://arxiv.org/abs/2305.02156v1)  **Published Date:** 2023-05-03\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/how_to/contextual_compression](https://python.langchain.com/docs/how_to/contextual_compression)\\n   - **API Reference:** [langchain...LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html#)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Supervised ranking methods based on bi-encoder or cross-encoder architectures\\nhave shown success in multi-stage text ranking tasks, but they require large\\namounts of relevance judgments as training data. In this work, we propose\\nListwise Reranker with a Large Language Model (LRL), which achieves strong\\nreranking effectiveness without using any task-specific training data.\\nDifferent from the existing pointwise ranking methods, where documents are\\nscored independently and ranked according to the scores, LRL directly generates\\na reordered list of document identifiers given the candidate documents.\\nExperiments on three TREC web search datasets demonstrate that LRL not only\\noutperforms zero-shot pointwise methods when reranking first-stage retrieval\\nresults, but can also act as a final-stage reranker to improve the top-ranked\\nresults of a pointwise method for improved efficiency. Additionally, we apply\\nour approach to subsets of MIRACL, a recent multilingual retrieval dataset,\\nwith results showing its potential to generalize across different languages.\\n                \\n## Visual Instruction Tuning'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Haotian Liu, Chunyuan Li, Qingyang Wu,  et al.\\n- **arXiv id:** [2304.08485v2](http://arxiv.org/abs/2304.08485v2)  **Published Date:** 2023-04-17\\n- **LangChain:**\\n\\n   - **Cookbook:** [Semi_structured_multi_modal_RAG_LLaMA2](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb), [Semi_structured_and_multi_modal_RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Instruction tuning large language models (LLMs) using machine-generated\\ninstruction-following data has improved zero-shot capabilities on new tasks,\\nbut the idea is less explored in the multimodal field. In this paper, we\\npresent the first attempt to use language-only GPT-4 to generate multimodal\\nlanguage-image instruction-following data. By instruction tuning on such\\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\\nend-to-end trained large multimodal model that connects a vision encoder and\\nLLM for general-purpose visual and language understanding.Our early experiments\\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\\nGPT-4 generated visual instruction tuning data, our model and code base\\npublicly available.\\n                \\n## Generative Agents: Interactive Simulacra of Human Behavior'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"- **Authors:** Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai,  et al.\\n- **arXiv id:** [2304.03442v2](http://arxiv.org/abs/2304.03442v2)  **Published Date:** 2023-04-07\\n- **LangChain:**\\n\\n   - **Cookbook:** [generative_agents_interactive_simulacra_of_human_behavior](https://github.com/langchain-ai/langchain/blob/master/cookbook/generative_agents_interactive_simulacra_of_human_behavior.ipynb), [multiagent_bidding](https://github.com/langchain-ai/langchain/blob/master/cookbook/multiagent_bidding.ipynb)\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Believable proxies of human behavior can empower interactive applications\\nranging from immersive environments to rehearsal spaces for interpersonal\\ncommunication to prototyping tools. In this paper, we introduce generative\\nagents--computational software agents that simulate believable human behavior.\\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\\nwhile authors write; they form opinions, notice each other, and initiate\\nconversations; they remember and reflect on days past as they plan the next\\nday. To enable generative agents, we describe an architecture that extends a\\nlarge language model to store a complete record of the agent\\'s experiences\\nusing natural language, synthesize those memories over time into higher-level\\nreflections, and retrieve them dynamically to plan behavior. We instantiate\\ngenerative agents to populate an interactive sandbox environment inspired by\\nThe Sims, where end users can interact with a small town of twenty five agents\\nusing natural language. In an evaluation, these generative agents produce\\nbelievable individual and emergent social behaviors: for example, starting with\\nonly a single user-specified notion that one agent wants to throw a Valentine\\'s\\nDay party, the agents autonomously spread invitations to the party over the\\nnext two days, make new acquaintances, ask each other out on dates to the\\nparty, and coordinate to show up for the party together at the right time. We\\ndemonstrate through ablation that the components of our agent\\narchitecture--observation, planning, and reflection--each contribute critically\\nto the believability of agent behavior. By fusing large language models with\\ncomputational, interactive agents, this work introduces architectural and\\ninteraction patterns for enabling believable simulations of human behavior.\\n                \\n## CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,  et al.\\n- **arXiv id:** [2303.17760v2](http://arxiv.org/abs/2303.17760v2)  **Published Date:** 2023-03-31\\n- **LangChain:**\\n\\n   - **Cookbook:** [camel_role_playing](https://github.com/langchain-ai/langchain/blob/master/cookbook/camel_role_playing.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** The rapid advancement of chat-based language models has led to remarkable\\nprogress in complex task-solving. However, their success heavily relies on\\nhuman input to guide the conversation, which can be challenging and\\ntime-consuming. This paper explores the potential of building scalable\\ntechniques to facilitate autonomous cooperation among communicative agents, and\\nprovides insight into their \"cognitive\" processes. To address the challenges of\\nachieving autonomous cooperation, we propose a novel communicative agent\\nframework named role-playing. Our approach involves using inception prompting\\nto guide chat agents toward task completion while maintaining consistency with\\nhuman intentions. We showcase how role-playing can be used to generate\\nconversational data for studying the behaviors and capabilities of a society of\\nagents, providing a valuable resource for investigating conversational language\\nmodels. In particular, we conduct comprehensive studies on\\ninstruction-following cooperation in multi-agent settings. Our contributions\\ninclude introducing a novel communicative agent framework, offering a scalable\\napproach for studying the cooperative behaviors and capabilities of multi-agent\\nsystems, and open-sourcing our library to support research on communicative\\nagents and beyond: https://github.com/camel-ai/camel.\\n                \\n## HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Yongliang Shen, Kaitao Song, Xu Tan,  et al.\\n- **arXiv id:** [2303.17580v4](http://arxiv.org/abs/2303.17580v4)  **Published Date:** 2023-03-30\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.autonomous_agents](https://python.langchain.com/api_reference/experimental/autonomous_agents.html)\\n   - **Cookbook:** [hugginggpt](https://github.com/langchain-ai/langchain/blob/master/cookbook/hugginggpt.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Solving complicated AI tasks with different domains and modalities is a key\\nstep toward artificial general intelligence. While there are numerous AI models\\navailable for various domains and modalities, they cannot handle complicated AI\\ntasks autonomously. Considering large language models (LLMs) have exhibited\\nexceptional abilities in language understanding, generation, interaction, and\\nreasoning, we advocate that LLMs could act as a controller to manage existing\\nAI models to solve complicated AI tasks, with language serving as a generic\\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\\nrequest, select models according to their function descriptions available in\\nHugging Face, execute each subtask with the selected AI model, and summarize\\nthe response according to the execution results. By leveraging the strong\\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\\nmodalities and domains and achieve impressive results in language, vision,\\nspeech, and other challenging tasks, which paves a new way towards the\\nrealization of artificial general intelligence.\\n                \\n## A Watermark for Large Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** John Kirchenbauer, Jonas Geiping, Yuxin Wen,  et al.\\n- **arXiv id:** [2301.10226v4](http://arxiv.org/abs/2301.10226v4)  **Published Date:** 2023-01-24\\n- **LangChain:**'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **API Reference:** [langchain_community...OCIModelDeploymentTGI](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI.html#langchain_community.llms.oci_data_science_model_deployment_endpoint.OCIModelDeploymentTGI), [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Potential harms of large language models can be mitigated by watermarking\\nmodel output, i.e., embedding signals into generated text that are invisible to\\nhumans but algorithmically detectable from a short span of tokens. We propose a\\nwatermarking framework for proprietary language models. The watermark can be\\nembedded with negligible impact on text quality, and can be detected using an\\nefficient open-source algorithm without access to the language model API or\\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\\nbefore a word is generated, and then softly promoting use of green tokens\\nduring sampling. We propose a statistical test for detecting the watermark with\\ninterpretable p-values, and derive an information-theoretic framework for\\nanalyzing the sensitivity of the watermark. We test the watermark using a\\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\\nfamily, and discuss robustness and security.\\n                \\n## Precise Zero-Shot Dense Retrieval without Relevance Labels'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Luyu Gao, Xueguang Ma, Jimmy Lin,  et al.\\n- **arXiv id:** [2212.10496v1](http://arxiv.org/abs/2212.10496v1)  **Published Date:** 2022-12-20\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **API Reference:** [langchain...HypotheticalDocumentEmbedder](https://api.python.langchain.com/en/latest/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html#langchain.chains.hyde.base.HypotheticalDocumentEmbedder)\\n   - **Template:** [hyde](https://python.langchain.com/docs/templates/hyde)\\n   - **Cookbook:** [hypothetical_document_embeddings](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** While dense retrieval has been shown effective and efficient across tasks and\\nlanguages, it remains difficult to create effective fully zero-shot dense\\nretrieval systems when no relevance label is available. In this paper, we\\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\\nquery, HyDE first zero-shot instructs an instruction-following language model\\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\\nrelevance patterns but is unreal and may contain false details. Then, an\\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\\ndocument into an embedding vector. This vector identifies a neighborhood in the\\ncorpus embedding space, where similar real documents are retrieved based on\\nvector similarity. This second step ground the generated document to the actual\\ncorpus, with the encoder's dense bottleneck filtering out the incorrect\\ndetails. Our experiments show that HyDE significantly outperforms the\\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).\\n                \\n## Constitutional AI: Harmlessness from AI Feedback\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Yuntao Bai, Saurav Kadavath, Sandipan Kundu,  et al.\\n- **arXiv id:** [2212.08073v1](http://arxiv.org/abs/2212.08073v1)  **Published Date:** 2022-12-15\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/versions/migrating_chains/constitutional_chain](https://python.langchain.com/docs/versions/migrating_chains/constitutional_chain)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** As AI systems become more capable, we would like to enlist their help to\\nsupervise other AIs. We experiment with methods for training a harmless AI\\nassistant through self-improvement, without any human labels identifying\\nharmful outputs. The only human oversight is provided through a list of rules\\nor principles, and so we refer to the method as 'Constitutional AI'. The\\nprocess involves both a supervised learning and a reinforcement learning phase.\\nIn the supervised phase we sample from an initial model, then generate\\nself-critiques and revisions, and then finetune the original model on revised\\nresponses. In the RL phase, we sample from the finetuned model, use a model to\\nevaluate which of the two samples is better, and then train a preference model\\nfrom this dataset of AI preferences. We then train with RL using the preference\\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\\nresult we are able to train a harmless but non-evasive AI assistant that\\nengages with harmful queries by explaining its objections to them. Both the SL\\nand RL methods can leverage chain-of-thought style reasoning to improve the\\nhuman-judged performance and transparency of AI decision making. These methods\\nmake it possible to control AI behavior more precisely and with far fewer human\\nlabels.\\n                \\n## Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande,  et al.\\n- **arXiv id:** [2212.07425v3](http://arxiv.org/abs/2212.07425v3)  **Published Date:** 2022-12-12\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.fallacy_removal](https://python.langchain.com/api_reference/experimental/fallacy_removal.html)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** The spread of misinformation, propaganda, and flawed argumentation has been\\namplified in the Internet era. Given the volume of data and the subtlety of\\nidentifying violations of argumentation norms, supporting information analytics\\ntasks, like content moderation, with trustworthy methods that can identify\\nlogical fallacies is essential. In this paper, we formalize prior theoretical\\nwork on logical fallacies into a comprehensive three-stage evaluation framework\\nof detection, coarse-grained, and fine-grained classification. We adapt\\nexisting evaluation datasets for each stage of the evaluation. We employ three\\nfamilies of robust and explainable methods based on prototype reasoning,\\ninstance-based reasoning, and knowledge injection. The methods combine language\\nmodels with background knowledge and explainable mechanisms. Moreover, we\\naddress data sparsity with strategies for data augmentation and curriculum\\nlearning. Our three-stage framework natively consolidates prior datasets and\\nmethods from existing tasks, like propaganda detection, serving as an\\noverarching evaluation testbed. We extensively evaluate these methods on our\\ndatasets, focusing on their robustness and explainability. Our results provide\\ninsight into the strengths and weaknesses of the methods on different\\ncomponents and fallacy classes, indicating that fallacy identification is a\\nchallenging task that may require specialized forms of reasoning to capture\\nvarious classes. We share our open-source code and data on GitHub to support\\nfurther work on logical fallacy identification.\\n                \\n## Complementary Explanations for Effective In-Context Learning'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Xi Ye, Srinivasan Iyer, Asli Celikyilmaz,  et al.\\n- **arXiv id:** [2211.13892v2](http://arxiv.org/abs/2211.13892v2)  **Published Date:** 2022-11-25\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_core...MaxMarginalRelevanceExampleSelector](https://api.python.langchain.com/en/latest/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html#langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large language models (LLMs) have exhibited remarkable capabilities in\\nlearning from explanations in prompts, but there has been limited understanding\\nof exactly how these explanations function or why they are effective. This work\\naims to better understand the mechanisms by which explanations are used for\\nin-context learning. We first study the impact of two different factors on the\\nperformance of prompts with explanations: the computation trace (the way the\\nsolution is decomposed) and the natural language used to express the prompt. By\\nperturbing explanations on three controlled tasks, we show that both factors\\ncontribute to the effectiveness of explanations. We further study how to form\\nmaximally effective sets of explanations for solving a given test query. We\\nfind that LLMs can benefit from the complementarity of the explanation set:\\ndiverse reasoning skills shown by different exemplars can lead to better\\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\\nselection approach for constructing exemplar sets that are both relevant as\\nwell as complementary, which successfully improves the in-context learning\\nperformance across three real-world tasks on multiple LLMs.\\n                \\n## PAL: Program-aided Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Luyu Gao, Aman Madaan, Shuyan Zhou,  et al.\\n- **arXiv id:** [2211.10435v2](http://arxiv.org/abs/2211.10435v2)  **Published Date:** 2022-11-18\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.pal_chain](https://python.langchain.com/api_reference/experimental/pal_chain.html), [langchain_experimental...PALChain](https://api.python.langchain.com/en/latest/pal_chain/langchain_experimental.pal_chain.base.PALChain.html#langchain_experimental.pal_chain.base.PALChain)\\n   - **Cookbook:** [program_aided_language_model](https://github.com/langchain-ai/langchain/blob/master/cookbook/program_aided_language_model.ipynb)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large language models (LLMs) have recently demonstrated an impressive ability\\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\\nexamples at test time (\"few-shot prompting\"). Much of this success can be\\nattributed to prompting methods such as \"chain-of-thought\\'\\', which employ LLMs\\nfor both understanding the problem description by decomposing it into steps, as\\nwell as solving each step of the problem. While LLMs seem to be adept at this\\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\\nmistakes in the solution part, even when the problem is decomposed correctly.\\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\\nthat uses the LLM to read natural language problems and generate programs as\\nthe intermediate reasoning steps, but offloads the solution step to a runtime\\nsuch as a Python interpreter. With PAL, decomposing the natural language\\nproblem into runnable steps remains the only learning task for the LLM, while\\nsolving is delegated to the interpreter. We demonstrate this synergy between a\\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\\nthese natural language reasoning tasks, generating code using an LLM and\\nreasoning using a Python interpreter leads to more accurate results than much\\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\\npublicly available at http://reasonwithpal.com/ .\\n                \\n## An Analysis of Fusion Functions for Hybrid Retrieval'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Sebastian Bruch, Siyu Gai, Amir Ingber\\n- **arXiv id:** [2210.11934v2](http://arxiv.org/abs/2210.11934v2)  **Published Date:** 2022-10-21\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** We study hybrid search in text retrieval where lexical and semantic search\\nare fused together with the intuition that the two are complementary in how\\nthey model relevance. In particular, we examine fusion by a convex combination\\n(CC) of lexical and semantic scores, as well as the Reciprocal Rank Fusion\\n(RRF) method, and identify their advantages and potential pitfalls. Contrary to\\nexisting studies, we find RRF to be sensitive to its parameters; that the\\nlearning of a CC fusion is generally agnostic to the choice of score\\nnormalization; that CC outperforms RRF in in-domain and out-of-domain settings;\\nand finally, that CC is sample efficient, requiring only a small set of\\ntraining examples to tune its only parameter to a target domain.\\n                \\n## ReAct: Synergizing Reasoning and Acting in Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Shunyu Yao, Jeffrey Zhao, Dian Yu,  et al.\\n- **arXiv id:** [2210.03629v3](http://arxiv.org/abs/2210.03629v3)  **Published Date:** 2022-10-06\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/tools/ionic_shopping](https://python.langchain.com/docs/integrations/tools/ionic_shopping), [docs/integrations/providers/cohere](https://python.langchain.com/docs/integrations/providers/cohere), [docs/concepts](https://python.langchain.com/docs/concepts)\\n   - **API Reference:** [langchain...create_react_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent), [langchain...TrajectoryEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html#langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io\\n                \\n## Deep Lake: a Lakehouse for Deep Learning'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan,  et al.\\n- **arXiv id:** [2209.10785v2](http://arxiv.org/abs/2209.10785v2)  **Published Date:** 2022-09-22\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/providers/activeloop_deeplake](https://python.langchain.com/docs/integrations/providers/activeloop_deeplake)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Traditional data lakes provide critical data infrastructure for analytical\\nworkloads by enabling time travel, running SQL queries, ingesting data with\\nACID transactions, and visualizing petabyte-scale datasets on cloud storage.\\nThey allow organizations to break down data silos, unlock data-driven\\ndecision-making, improve operational efficiency, and reduce costs. However, as\\ndeep learning usage increases, traditional data lakes are not well-designed for\\napplications such as natural language processing (NLP), audio processing,\\ncomputer vision, and applications involving non-tabular datasets. This paper\\npresents Deep Lake, an open-source lakehouse for deep learning applications\\ndeveloped at Activeloop. Deep Lake maintains the benefits of a vanilla data\\nlake with one key difference: it stores complex data, such as images, videos,\\nannotations, as well as tabular data, in the form of tensors and rapidly\\nstreams the data over the network to (a) Tensor Query Language, (b) in-browser\\nvisualization engine, or (c) deep learning frameworks without sacrificing GPU\\nutilization. Datasets stored in Deep Lake can be accessed from PyTorch,\\nTensorFlow, JAX, and integrate with numerous MLOps tools.\\n                \\n## Matryoshka Representation Learning'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Aditya Kusupati, Gantavya Bhatt, Aniket Rege,  et al.\\n- **arXiv id:** [2205.13147v4](http://arxiv.org/abs/2205.13147v4)  **Published Date:** 2022-05-26\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/providers/snowflake](https://python.langchain.com/docs/integrations/providers/snowflake)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Learned representations are a central component in modern ML systems, serving\\na multitude of downstream tasks. When training such representations, it is\\noften the case that computational and statistical constraints for each\\ndownstream task are unknown. In this context rigid, fixed capacity\\nrepresentations can be either over or under-accommodating to the task at hand.\\nThis leads us to ask: can we design a flexible representation that can adapt to\\nmultiple downstream tasks with varying computational resources? Our main\\ncontribution is Matryoshka Representation Learning (MRL) which encodes\\ninformation at different granularities and allows a single embedding to adapt\\nto the computational constraints of downstream tasks. MRL minimally modifies\\nexisting representation learning pipelines and imposes no additional cost\\nduring inference and deployment. MRL learns coarse-to-fine representations that\\nare at least as accurate and rich as independently trained low-dimensional\\nrepresentations. The flexibility within the learned Matryoshka Representations\\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\\nlong-tail few-shot classification, all while being as robust as the original\\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\\nare open-sourced at https://github.com/RAIVNLab/MRL.\\n                \\n## Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Kevin Heffernan, Onur Çelebi, Holger Schwenk\\n- **arXiv id:** [2205.12654v1](http://arxiv.org/abs/2205.12654v1)  **Published Date:** 2022-05-25\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_community...LaserEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.laser.LaserEmbeddings.html#langchain_community.embeddings.laser.LaserEmbeddings)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Scaling multilingual representation learning beyond the hundred most frequent\\nlanguages is challenging, in particular to cover the long tail of low-resource\\nlanguages. A promising approach has been to train one-for-all multilingual\\nmodels capable of cross-lingual transfer, but these models often suffer from\\ninsufficient capacity and interference between unrelated languages. Instead, we\\nmove away from this approach and focus on training multiple language (family)\\nspecific representations, but most prominently enable all languages to still be\\nencoded in the same representational space. To achieve this, we focus on\\nteacher-student training, allowing all encoders to be mutually compatible for\\nbitext mining, and enabling fast learning of new languages. We introduce a new\\nteacher-student training scheme which combines supervised and self-supervised\\ntraining, allowing encoders to take advantage of monolingual training data,\\nwhich is valuable in the low-resource setting.\\n  Our approach significantly outperforms the original LASER encoder. We study\\nvery low-resource languages and handle 50 African languages, many of which are\\nnot covered by any other model. For these languages, we train sentence\\nencoders, mine bitexts, and validate the bitexts by training NMT systems.\\n                \\n## Evaluating the Text-to-SQL Capabilities of Large Language Models'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau\\n- **arXiv id:** [2204.00498v1](http://arxiv.org/abs/2204.00498v1)  **Published Date:** 2022-03-15\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/tutorials/sql_qa](https://python.langchain.com/docs/tutorials/sql_qa)\\n   - **API Reference:** [langchain_community...SQLDatabase](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase), [langchain_community...SparkSQL](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.spark_sql.SparkSQL.html#langchain_community.utilities.spark_sql.SparkSQL)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** We perform an empirical evaluation of Text-to-SQL capabilities of the Codex\\nlanguage model. We find that, without any finetuning, Codex is a strong\\nbaseline on the Spider benchmark; we also analyze the failure modes of Codex in\\nthis setting. Furthermore, we demonstrate on the GeoQuery and Scholar\\nbenchmarks that a small number of in-domain examples provided in the prompt\\nenables Codex to perform better than state-of-the-art models finetuned on such\\nfew-shot examples.\\n                \\n## Locally Typical Sampling\\n\\n- **Authors:** Clara Meister, Tiago Pimentel, Gian Wiher,  et al.\\n- **arXiv id:** [2202.00666v5](http://arxiv.org/abs/2202.00666v5)  **Published Date:** 2022-02-01\\n- **LangChain:**'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **API Reference:** [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** Today's probabilistic language generators fall short when it comes to\\nproducing coherent and fluent text despite the fact that the underlying models\\nperform well under standard metrics, e.g., perplexity. This discrepancy has\\npuzzled the language generation community for the last few years. In this work,\\nwe posit that the abstraction of natural language generation as a discrete\\nstochastic process--which allows for an information-theoretic analysis--can\\nprovide new insights into the behavior of probabilistic language generators,\\ne.g., why high-probability texts can be dull or repetitive. Humans use language\\nas a means of communicating information, aiming to do so in a simultaneously\\nefficient and error-minimizing manner; in fact, psycholinguistics research\\nsuggests humans choose each word in a string with this subconscious goal in\\nmind. We formally define the set of strings that meet this criterion: those for\\nwhich each word has an information content close to the expected information\\ncontent, i.e., the conditional entropy of our model. We then propose a simple\\nand efficient procedure for enforcing this criterion when generating from\\nprobabilistic models, which we call locally typical sampling. Automatic and\\nhuman evaluations show that, in comparison to nucleus and top-k sampling,\\nlocally typical sampling offers competitive performance (in both abstractive\\nsummarization and story generation) in terms of quality while consistently\\nreducing degenerate repetitions.\\n                \\n## ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,  et al.\\n- **arXiv id:** [2112.01488v3](http://arxiv.org/abs/2112.01488v3)  **Published Date:** 2021-12-02\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/integrations/retrievers/ragatouille](https://python.langchain.com/docs/integrations/retrievers/ragatouille), [docs/integrations/providers/ragatouille](https://python.langchain.com/docs/integrations/providers/ragatouille), [docs/concepts](https://python.langchain.com/docs/concepts), [docs/integrations/providers/dspy](https://python.langchain.com/docs/integrations/providers/dspy)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Neural information retrieval (IR) has greatly advanced search and other\\nknowledge-intensive language tasks. While many neural IR methods encode queries\\nand documents into single-vector representations, late interaction models\\nproduce multi-vector representations at the granularity of each token and\\ndecompose relevance modeling into scalable token-level computations. This\\ndecomposition has been shown to make late interaction more effective, but it\\ninflates the space footprint of these models by an order of magnitude. In this\\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\\ncompression mechanism with a denoised supervision strategy to simultaneously\\nimprove the quality and space footprint of late interaction. We evaluate\\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\\nquality within and outside the training domain while reducing the space\\nfootprint of late interaction models by 6--10$\\\\times$.\\n                \\n## Learning Transferable Visual Models From Natural Language Supervision'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Alec Radford, Jong Wook Kim, Chris Hallacy,  et al.\\n- **arXiv id:** [2103.00020v1](http://arxiv.org/abs/2103.00020v1)  **Published Date:** 2021-02-26\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_experimental.open_clip](https://python.langchain.com/api_reference/experimental/open_clip.html)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined object categories. This restricted form of supervision limits\\ntheir generality and usability since additional labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones) enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.\\n                \\n## Language Models are Few-Shot Learners'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Tom B. Brown, Benjamin Mann, Nick Ryder,  et al.\\n- **arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4)  **Published Date:** 2020-05-28\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content=\"**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\\na specific task. While typically task-agnostic in architecture, this method\\nstill requires task-specific fine-tuning datasets of thousands or tens of\\nthousands of examples. By contrast, humans can generally perform a new language\\ntask from only a few examples or from simple instructions - something which\\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\\neven reaching competitiveness with prior state-of-the-art fine-tuning\\napproaches. Specifically, we train GPT-3, an autoregressive language model with\\n175 billion parameters, 10x more than any previous non-sparse language model,\\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\\napplied without any gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the model. GPT-3\\nachieves strong performance on many NLP datasets, including translation,\\nquestion-answering, and cloze tasks, as well as several tasks that require\\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\\nas well as some datasets where GPT-3 faces methodological issues related to\\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\\nof news articles which human evaluators have difficulty distinguishing from\\narticles written by humans. We discuss broader societal impacts of this finding\\nand of GPT-3 in general.\\n                \\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\\n- **arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4)  **Published Date:** 2020-05-22\\n- **LangChain:**\\n\\n   - **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.\\n                \\n## CTRL: A Conditional Transformer Language Model for Controllable Generation'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='- **Authors:** Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al.\\n- **arXiv id:** [1909.05858v2](http://arxiv.org/abs/1909.05858v2)  **Published Date:** 2019-09-11\\n- **LangChain:**\\n\\n   - **API Reference:** [langchain_huggingface...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint), [langchain_community...HuggingFaceTextGenInference](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html#langchain_community.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference), [langchain_community...HuggingFaceEndpoint](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint.html#langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint)'), Document(metadata={'source': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_path': 'docs/docs/additional_resources/arxiv_references.mdx', 'file_name': 'arxiv_references.mdx', 'file_type': '.mdx'}, page_content='**Abstract:** Large-scale language models show promising text generation capabilities, but\\nusers cannot easily control particular aspects of the generated text. We\\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\\ntrained to condition on control codes that govern style, content, and\\ntask-specific behavior. Control codes were derived from structure that\\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\\nlearning while providing more explicit control over text generation. These\\ncodes also allow CTRL to predict which parts of the training data are most\\nlikely given a sequence. This provides a potential method for analyzing large\\namounts of data via model-based source attribution. We have released multiple\\nfull-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.'), Document(metadata={'source': 'docs/docs/additional_resources/dependents.mdx', 'file_path': 'docs/docs/additional_resources/dependents.mdx', 'file_name': 'dependents.mdx', 'file_type': '.mdx'}, page_content='# Dependents\\n\\nDependents stats for `langchain-ai/langchain`\\n\\n[![](https://img.shields.io/static/v1?label=Used%20by&message=41717&color=informational&logo=slickpic)](https://github.com/langchain-ai/langchain/network/dependents)\\n[![](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=538&color=informational&logo=slickpic)](https://github.com/langchain-ai/langchain/network/dependents)\\n[![](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=41179&color=informational&logo=slickpic)](https://github.com/langchain-ai/langchain/network/dependents)\\n\\n\\n[update: `2023-12-08`; only dependent repositories with Stars > 100]'), Document(metadata={'source': 'docs/docs/additional_resources/dependents.mdx', 'file_path': 'docs/docs/additional_resources/dependents.mdx', 'file_name': 'dependents.mdx', 'file_type': '.mdx'}, page_content='| Repository | Stars  |\\n| :--------  | -----: |\\n|[AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer) | 46514 |\\n|[imartinez/privateGPT](https://github.com/imartinez/privateGPT) | 44439 |\\n|[LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant) | 35906 |\\n|[hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI) | 35528 |\\n|[moymix/TaskMatrix](https://github.com/moymix/TaskMatrix) | 34342 |\\n|[geekan/MetaGPT](https://github.com/geekan/MetaGPT) | 31126 |\\n|[streamlit/streamlit](https://github.com/streamlit/streamlit) | 28911 |\\n|[reworkd/AgentGPT](https://github.com/reworkd/AgentGPT) | 27833 |\\n|[StanGirard/quivr](https://github.com/StanGirard/quivr) | 26032 |\\n|[OpenBB-finance/OpenBBTerminal](https://github.com/OpenBB-finance/OpenBBTerminal) | 24946 |\\n|[run-llama/llama_index](https://github.com/run-llama/llama_index) | 24859 |\\n|[jmorganca/ollama](https://github.com/jmorganca/ollama) | 20849 |\\n|[openai/chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin) | 20249 |\\n|[chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat) | 19305 |\\n|[mindsdb/mindsdb](https://github.com/mindsdb/mindsdb) | 19172 |\\n|[PromtEngineer/localGPT](https://github.com/PromtEngineer/localGPT) | 17528 |\\n|[cube-js/cube](https://github.com/cube-js/cube) | 16575 |\\n|[mlflow/mlflow](https://github.com/mlflow/mlflow) | 16000 |\\n|[mudler/LocalAI](https://github.com/mudler/LocalAI) | 14067 |\\n|[logspace-ai/langflow](https://github.com/logspace-ai/langflow) | 13679 |\\n|[GaiZhenbiao/ChuanhuChatGPT](https://github.com/GaiZhenbiao/ChuanhuChatGPT) | 13648 |\\n|[arc53/DocsGPT](https://github.com/arc53/DocsGPT) | 13423 |\\n|[openai/evals](https://github.com/openai/evals) | 12649 |\\n|[airbytehq/airbyte](https://github.com/airbytehq/airbyte) | 12460 |\\n|[langgenius/dify](https://github.com/langgenius/dify) | 11859 |\\n|[databrickslabs/dolly](https://github.com/databrickslabs/dolly) | 10672 |\\n|[AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT) | 9437 |\\n|[langchain-ai/langchainjs](https://github.com/langchain-ai/langchainjs) | 9227 |\\n|[gventuri/pandas-ai](https://github.com/gventuri/pandas-ai) | 9203 |\\n|[aws/amazon-sagemaker-examples](https://github.com/aws/amazon-sagemaker-examples) | 9079 |\\n|[h2oai/h2ogpt](https://github.com/h2oai/h2ogpt) | 8945 |\\n|[PipedreamHQ/pipedream](https://github.com/PipedreamHQ/pipedream) | 7550 |\\n|[bentoml/OpenLLM](https://github.com/bentoml/OpenLLM) | 6957 |\\n|[THUDM/ChatGLM3](https://github.com/THUDM/ChatGLM3) | 6801 |\\n|[microsoft/promptflow](https://github.com/microsoft/promptflow) | 6776 |\\n|[cpacker/MemGPT](https://github.com/cpacker/MemGPT) | 6642 |\\n|[joshpxyne/gpt-migrate](https://github.com/joshpxyne/gpt-migrate) | 6482 |\\n|[zauberzeug/nicegui](https://github.com/zauberzeug/nicegui) | 6037 |\\n|[embedchain/embedchain](https://github.com/embedchain/embedchain) | 6023 |\\n|[mage-ai/mage-ai](https://github.com/mage-ai/mage-ai) | 6019 |\\n|[assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher) | 5936 |\\n|[sweepai/sweep](https://github.com/sweepai/sweep) | 5855 |\\n|[wenda-LLM/wenda](https://github.com/wenda-LLM/wenda) | 5766 |\\n|[zilliztech/GPTCache](https://github.com/zilliztech/GPTCache) | 5710 |\\n|[pdm-project/pdm](https://github.com/pdm-project/pdm) | 5665 |\\n|[GreyDGL/PentestGPT](https://github.com/GreyDGL/PentestGPT) | 5568 |\\n|[gkamradt/langchain-tutorials](https://github.com/gkamradt/langchain-tutorials) | 5507 |\\n|[Shaunwei/RealChar](https://github.com/Shaunwei/RealChar) | 5501 |\\n|[facebookresearch/llama-recipes](https://github.com/facebookresearch/llama-recipes) | 5477 |\\n|[serge-chat/serge](https://github.com/serge-chat/serge) | 5221 |\\n|[run-llama/rags](https://github.com/run-llama/rags) | 4916 |\\n|[openchatai/OpenChat](https://github.com/openchatai/OpenChat) | 4870 |\\n|[danswer-ai/danswer](https://github.com/danswer-ai/danswer) | 4774 |\\n|[langchain-ai/opengpts](https://github.com/langchain-ai/opengpts) | 4709 |\\n|[postgresml/postgresml](https://github.com/postgresml/postgresml) | 4639 |\\n|[MineDojo/Voyager](https://github.com/MineDojo/Voyager) | 4582 |\\n|[intel-analytics/BigDL](https://github.com/intel-analytics/BigDL) | 4581 |\\n|[yihong0618/xiaogpt](https://github.com/yihong0618/xiaogpt) | 4359 |\\n|[RayVentura/ShortGPT](https://github.com/RayVentura/ShortGPT) | 4357 |\\n|[Azure-Samples/azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | 4317 |\\n|[madawei2699/myGPTReader](https://github.com/madawei2699/myGPTReader) | 4289 |\\n|[apache/nifi](https://github.com/apache/nifi) | 4098 |\\n|[langchain-ai/chat-langchain](https://github.com/langchain-ai/chat-langchain) | 4091 |\\n|[aiwaves-cn/agents](https://github.com/aiwaves-cn/agents) | 4073 |\\n|[krishnaik06/The-Grand-Complete-Data-Science-Materials](https://github.com/krishnaik06/The-Grand-Complete-Data-Science-Materials) | 4065 |\\n|[khoj-ai/khoj](https://github.com/khoj-ai/khoj) | 4016 |\\n|[Azure/azure-sdk-for-python](https://github.com/Azure/azure-sdk-for-python) | 3941 |\\n|[PrefectHQ/marvin](https://github.com/PrefectHQ/marvin) | 3915 |\\n|[OpenBMB/ToolBench](https://github.com/OpenBMB/ToolBench) | 3799 |\\n|[marqo-ai/marqo](https://github.com/marqo-ai/marqo) | 3771 |\\n|[kyegomez/tree-of-thoughts](https://github.com/kyegomez/tree-of-thoughts) | 3688 |\\n|[Unstructured-IO/unstructured](https://github.com/Unstructured-IO/unstructured) | 3543 |\\n|[llm-workflow-engine/llm-workflow-engine](https://github.com/llm-workflow-engine/llm-workflow-engine) | 3515 |\\n|[shroominic/codeinterpreter-api](https://github.com/shroominic/codeinterpreter-api) | 3425 |\\n|[openchatai/OpenCopilot](https://github.com/openchatai/OpenCopilot) | 3418 |\\n|[josStorer/RWKV-Runner](https://github.com/josStorer/RWKV-Runner) | 3297 |\\n|[whitead/paper-qa](https://github.com/whitead/paper-qa) | 3280 |\\n|[homanp/superagent](https://github.com/homanp/superagent) | 3258 |\\n|[ParisNeo/lollms-webui](https://github.com/ParisNeo/lollms-webui) | 3199 |\\n|[OpenBMB/AgentVerse](https://github.com/OpenBMB/AgentVerse) | 3099 |\\n|[project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot) | 3090 |\\n|[OpenGVLab/InternGPT](https://github.com/OpenGVLab/InternGPT) | 2989 |\\n|[xlang-ai/OpenAgents](https://github.com/xlang-ai/OpenAgents) | 2825 |\\n|[dataelement/bisheng](https://github.com/dataelement/bisheng) | 2797 |\\n|[Mintplex-Labs/anything-llm](https://github.com/Mintplex-Labs/anything-llm) | 2784 |\\n|[OpenBMB/BMTools](https://github.com/OpenBMB/BMTools) | 2734 |\\n|[run-llama/llama-hub](https://github.com/run-llama/llama-hub) | 2721 |\\n|[SamurAIGPT/EmbedAI](https://github.com/SamurAIGPT/EmbedAI) | 2647 |\\n|[NVIDIA/NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) | 2637 |\\n|[X-D-Lab/LangChain-ChatGLM-Webui](https://github.com/X-D-Lab/LangChain-ChatGLM-Webui) | 2532 |\\n|[GerevAI/gerev](https://github.com/GerevAI/gerev) | 2517 |\\n|[keephq/keep](https://github.com/keephq/keep) | 2448 |\\n|[yanqiangmiffy/Chinese-LangChain](https://github.com/yanqiangmiffy/Chinese-LangChain) | 2397 |\\n|[OpenGVLab/Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) | 2324 |\\n|[IntelligenzaArtificiale/Free-Auto-GPT](https://github.com/IntelligenzaArtificiale/Free-Auto-GPT) | 2241 |\\n|[YiVal/YiVal](https://github.com/YiVal/YiVal) | 2232 |\\n|[jupyterlab/jupyter-ai](https://github.com/jupyterlab/jupyter-ai) | 2189 |\\n|[Farama-Foundation/PettingZoo](https://github.com/Farama-Foundation/PettingZoo) | 2136 |\\n|[microsoft/TaskWeaver](https://github.com/microsoft/TaskWeaver) | 2126 |\\n|[hwchase17/notion-qa](https://github.com/hwchase17/notion-qa) | 2083 |\\n|[FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding) | 2053 |\\n|[paulpierre/RasaGPT](https://github.com/paulpierre/RasaGPT) | 1999 |\\n|[hegelai/prompttools](https://github.com/hegelai/prompttools) | 1984 |\\n|[mckinsey/vizro](https://github.com/mckinsey/vizro) | 1951 |\\n|[vocodedev/vocode-python](https://github.com/vocodedev/vocode-python) | 1868 |\\n|[dot-agent/openAMS](https://github.com/dot-agent/openAMS) | 1796 |\\n|[explodinggradients/ragas](https://github.com/explodinggradients/ragas) | 1766 |\\n|[AI-Citizen/SolidGPT](https://github.com/AI-Citizen/SolidGPT) | 1761 |\\n|[Kav-K/GPTDiscord](https://github.com/Kav-K/GPTDiscord) | 1696 |\\n|[run-llama/sec-insights](https://github.com/run-llama/sec-insights) | 1654 |\\n|[avinashkranjan/Amazing-Python-Scripts](https://github.com/avinashkranjan/Amazing-Python-Scripts) | 1635 |\\n|[microsoft/WhatTheHack](https://github.com/microsoft/WhatTheHack) | 1629 |\\n|[noahshinn/reflexion](https://github.com/noahshinn/reflexion) | 1625 |\\n|[psychic-api/psychic](https://github.com/psychic-api/psychic) | 1618 |\\n|[Forethought-Technologies/AutoChain](https://github.com/Forethought-Technologies/AutoChain) | 1611 |\\n|[pinterest/querybook](https://github.com/pinterest/querybook) | 1586 |\\n|[refuel-ai/autolabel](https://github.com/refuel-ai/autolabel) | 1553 |\\n|[jina-ai/langchain-serve](https://github.com/jina-ai/langchain-serve) | 1537 |\\n|[jina-ai/dev-gpt](https://github.com/jina-ai/dev-gpt) | 1522 |\\n|[agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI) | 1493 |\\n|[ttengwang/Caption-Anything](https://github.com/ttengwang/Caption-Anything) | 1484 |\\n|[greshake/llm-security](https://github.com/greshake/llm-security) | 1483 |\\n|[promptfoo/promptfoo](https://github.com/promptfoo/promptfoo) | 1480 |\\n|[milvus-io/bootcamp](https://github.com/milvus-io/bootcamp) | 1477 |\\n|[richardyc/Chrome-GPT](https://github.com/richardyc/Chrome-GPT) | 1475 |\\n|[melih-unsal/DemoGPT](https://github.com/melih-unsal/DemoGPT) | 1428 |\\n|[YORG-AI/Open-Assistant](https://github.com/YORG-AI/Open-Assistant) | 1419 |\\n|[101dotxyz/GPTeam](https://github.com/101dotxyz/GPTeam) | 1416 |\\n|[jina-ai/thinkgpt](https://github.com/jina-ai/thinkgpt) | 1408 |\\n|[mmz-001/knowledge_gpt](https://github.com/mmz-001/knowledge_gpt) | 1398 |\\n|[intel/intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers) | 1387 |\\n|[Azure/azureml-examples](https://github.com/Azure/azureml-examples) | 1385 |\\n|[lunasec-io/lunasec](https://github.com/lunasec-io/lunasec) | 1367 |\\n|[eyurtsev/kor](https://github.com/eyurtsev/kor) | 1355 |\\n|[xusenlinzy/api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm) | 1325 |\\n|[griptape-ai/griptape](https://github.com/griptape-ai/griptape) | 1323 |\\n|[SuperDuperDB/superduperdb](https://github.com/SuperDuperDB/superduperdb) | 1290 |\\n|[cofactoryai/textbase](https://github.com/cofactoryai/textbase) | 1284 |\\n|[psychic-api/rag-stack](https://github.com/psychic-api/rag-stack) | 1260 |\\n|[filip-michalsky/SalesGPT](https://github.com/filip-michalsky/SalesGPT) | 1250 |\\n|[nod-ai/SHARK](https://github.com/nod-ai/SHARK) | 1237 |\\n|[pluralsh/plural](https://github.com/pluralsh/plural) | 1234 |\\n|[cheshire-cat-ai/core](https://github.com/cheshire-cat-ai/core) | 1194 |\\n|[LC1332/Chat-Haruhi-Suzumiya](https://github.com/LC1332/Chat-Haruhi-Suzumiya) | 1184 |\\n|[poe-platform/server-bot-quick-start](https://github.com/poe-platform/server-bot-quick-start) | 1182 |\\n|[microsoft/X-Decoder](https://github.com/microsoft/X-Decoder) | 1180 |\\n|[juncongmoo/chatllama](https://github.com/juncongmoo/chatllama) | 1171 |\\n|[visual-openllm/visual-openllm](https://github.com/visual-openllm/visual-openllm) | 1156 |\\n|[alejandro-ao/ask-multiple-pdfs](https://github.com/alejandro-ao/ask-multiple-pdfs) | 1153 |\\n|[ThousandBirdsInc/chidori](https://github.com/ThousandBirdsInc/chidori) | 1152 |\\n|[irgolic/AutoPR](https://github.com/irgolic/AutoPR) | 1137 |\\n|[SamurAIGPT/Camel-AutoGPT](https://github.com/SamurAIGPT/Camel-AutoGPT) | 1083 |\\n|[ray-project/llm-applications](https://github.com/ray-project/llm-applications) | 1080 |\\n|[run-llama/llama-lab](https://github.com/run-llama/llama-lab) | 1072 |\\n|[jiran214/GPT-vup](https://github.com/jiran214/GPT-vup) | 1041 |\\n|[MetaGLM/FinGLM](https://github.com/MetaGLM/FinGLM) | 1035 |\\n|[peterw/Chat-with-Github-Repo](https://github.com/peterw/Chat-with-Github-Repo) | 1020 |\\n|[Anil-matcha/ChatPDF](https://github.com/Anil-matcha/ChatPDF) | 991 |\\n|[langchain-ai/langserve](https://github.com/langchain-ai/langserve) | 983 |\\n|[THUDM/AgentTuning](https://github.com/THUDM/AgentTuning) | 976 |\\n|[rlancemartin/auto-evaluator](https://github.com/rlancemartin/auto-evaluator) | 975 |\\n|[codeacme17/examor](https://github.com/codeacme17/examor) | 964 |\\n|[all-in-aigc/gpts-works](https://github.com/all-in-aigc/gpts-works) | 946 |\\n|[Ikaros-521/AI-Vtuber](https://github.com/Ikaros-521/AI-Vtuber) | 946 |\\n|[microsoft/Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx) | 898 |\\n|[cirediatpl/FigmaChain](https://github.com/cirediatpl/FigmaChain) | 895 |\\n|[ricklamers/shell-ai](https://github.com/ricklamers/shell-ai) | 893 |\\n|[modelscope/modelscope-agent](https://github.com/modelscope/modelscope-agent) | 893 |\\n|[seanpixel/Teenage-AGI](https://github.com/seanpixel/Teenage-AGI) | 886 |\\n|[ajndkr/lanarky](https://github.com/ajndkr/lanarky) | 880 |\\n|[kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference) | 872 |\\n|[corca-ai/EVAL](https://github.com/corca-ai/EVAL) | 846 |\\n|[hwchase17/chat-your-data](https://github.com/hwchase17/chat-your-data) | 841 |\\n|[kreneskyp/ix](https://github.com/kreneskyp/ix) | 821 |\\n|[Link-AGI/AutoAgents](https://github.com/Link-AGI/AutoAgents) | 820 |\\n|[truera/trulens](https://github.com/truera/trulens) | 794 |\\n|[Dataherald/dataherald](https://github.com/Dataherald/dataherald) | 788 |\\n|[sunlabuiuc/PyHealth](https://github.com/sunlabuiuc/PyHealth) | 783 |\\n|[jondurbin/airoboros](https://github.com/jondurbin/airoboros) | 783 |\\n|[pyspark-ai/pyspark-ai](https://github.com/pyspark-ai/pyspark-ai) | 782 |\\n|[confident-ai/deepeval](https://github.com/confident-ai/deepeval) | 780 |\\n|[billxbf/ReWOO](https://github.com/billxbf/ReWOO) | 777 |\\n|[langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent) | 776 |\\n|[akshata29/entaoai](https://github.com/akshata29/entaoai) | 771 |\\n|[LambdaLabsML/examples](https://github.com/LambdaLabsML/examples) | 770 |\\n|[getmetal/motorhead](https://github.com/getmetal/motorhead) | 768 |\\n|[Dicklesworthstone/swiss_army_llama](https://github.com/Dicklesworthstone/swiss_army_llama) | 757 |\\n|[ruoccofabrizio/azure-open-ai-embeddings-qna](https://github.com/ruoccofabrizio/azure-open-ai-embeddings-qna) | 757 |\\n|[msoedov/langcorn](https://github.com/msoedov/langcorn) | 754 |\\n|[e-johnstonn/BriefGPT](https://github.com/e-johnstonn/BriefGPT) | 753 |\\n|[microsoft/sample-app-aoai-chatGPT](https://github.com/microsoft/sample-app-aoai-chatGPT) | 749 |\\n|[explosion/spacy-llm](https://github.com/explosion/spacy-llm) | 731 |\\n|[MiuLab/Taiwan-LLM](https://github.com/MiuLab/Taiwan-LLM) | 716 |\\n|[whyiyhw/chatgpt-wechat](https://github.com/whyiyhw/chatgpt-wechat) | 702 |\\n|[Azure-Samples/openai](https://github.com/Azure-Samples/openai) | 692 |\\n|[iusztinpaul/hands-on-llms](https://github.com/iusztinpaul/hands-on-llms) | 687 |\\n|[safevideo/autollm](https://github.com/safevideo/autollm) | 682 |\\n|[OpenGenerativeAI/GenossGPT](https://github.com/OpenGenerativeAI/GenossGPT) | 669 |\\n|[NoDataFound/hackGPT](https://github.com/NoDataFound/hackGPT) | 663 |\\n|[AILab-CVC/GPT4Tools](https://github.com/AILab-CVC/GPT4Tools) | 662 |\\n|[langchain-ai/auto-evaluator](https://github.com/langchain-ai/auto-evaluator) | 657 |\\n|[yvann-ba/Robby-chatbot](https://github.com/yvann-ba/Robby-chatbot) | 639 |\\n|[alexanderatallah/window.ai](https://github.com/alexanderatallah/window.ai) | 635 |\\n|[amosjyng/langchain-visualizer](https://github.com/amosjyng/langchain-visualizer) | 630 |\\n|[microsoft/PodcastCopilot](https://github.com/microsoft/PodcastCopilot) | 621 |\\n|[aws-samples/aws-genai-llm-chatbot](https://github.com/aws-samples/aws-genai-llm-chatbot) | 616 |\\n|[NeumTry/NeumAI](https://github.com/NeumTry/NeumAI) | 605 |\\n|[namuan/dr-doc-search](https://github.com/namuan/dr-doc-search) | 599 |\\n|[plastic-labs/tutor-gpt](https://github.com/plastic-labs/tutor-gpt) | 595 |\\n|[marimo-team/marimo](https://github.com/marimo-team/marimo) | 591 |\\n|[yakami129/VirtualWife](https://github.com/yakami129/VirtualWife) | 586 |\\n|[xuwenhao/geektime-ai-course](https://github.com/xuwenhao/geektime-ai-course) | 584 |\\n|[jonra1993/fastapi-alembic-sqlmodel-async](https://github.com/jonra1993/fastapi-alembic-sqlmodel-async) | 573 |\\n|[dgarnitz/vectorflow](https://github.com/dgarnitz/vectorflow) | 568 |\\n|[yeagerai/yeagerai-agent](https://github.com/yeagerai/yeagerai-agent) | 564 |\\n|[daveebbelaar/langchain-experiments](https://github.com/daveebbelaar/langchain-experiments) | 563 |\\n|[traceloop/openllmetry](https://github.com/traceloop/openllmetry) | 559 |\\n|[Agenta-AI/agenta](https://github.com/Agenta-AI/agenta) | 546 |\\n|[michaelthwan/searchGPT](https://github.com/michaelthwan/searchGPT) | 545 |\\n|[jina-ai/agentchain](https://github.com/jina-ai/agentchain) | 544 |\\n|[mckaywrigley/repo-chat](https://github.com/mckaywrigley/repo-chat) | 533 |\\n|[marella/chatdocs](https://github.com/marella/chatdocs) | 532 |\\n|[opentensor/bittensor](https://github.com/opentensor/bittensor) | 532 |\\n|[DjangoPeng/openai-quickstart](https://github.com/DjangoPeng/openai-quickstart) | 527 |\\n|[freddyaboulton/gradio-tools](https://github.com/freddyaboulton/gradio-tools) | 517 |\\n|[sidhq/Multi-GPT](https://github.com/sidhq/Multi-GPT) | 515 |\\n|[alejandro-ao/langchain-ask-pdf](https://github.com/alejandro-ao/langchain-ask-pdf) | 514 |\\n|[sajjadium/ctf-archives](https://github.com/sajjadium/ctf-archives) | 507 |\\n|[continuum-llms/chatgpt-memory](https://github.com/continuum-llms/chatgpt-memory) | 502 |\\n|[steamship-core/steamship-langchain](https://github.com/steamship-core/steamship-langchain) | 494 |\\n|[mpaepper/content-chatbot](https://github.com/mpaepper/content-chatbot) | 493 |\\n|[langchain-ai/langchain-aiplugin](https://github.com/langchain-ai/langchain-aiplugin) | 492 |\\n|[logan-markewich/llama_index_starter_pack](https://github.com/logan-markewich/llama_index_starter_pack) | 483 |\\n|[datawhalechina/llm-universe](https://github.com/datawhalechina/llm-universe) | 475 |\\n|[leondz/garak](https://github.com/leondz/garak) | 464 |\\n|[RedisVentures/ArXivChatGuru](https://github.com/RedisVentures/ArXivChatGuru) | 461 |\\n|[Anil-matcha/Chatbase](https://github.com/Anil-matcha/Chatbase) | 455 |\\n|[Aiyu-awa/luna-ai](https://github.com/Aiyu-awa/luna-ai) | 450 |\\n|[DataDog/dd-trace-py](https://github.com/DataDog/dd-trace-py) | 450 |\\n|[Azure-Samples/miyagi](https://github.com/Azure-Samples/miyagi) | 449 |\\n|[poe-platform/poe-protocol](https://github.com/poe-platform/poe-protocol) | 447 |\\n|[onlyphantom/llm-python](https://github.com/onlyphantom/llm-python) | 446 |\\n|[junruxiong/IncarnaMind](https://github.com/junruxiong/IncarnaMind) | 441 |\\n|[CarperAI/OpenELM](https://github.com/CarperAI/OpenELM) | 441 |\\n|[daodao97/chatdoc](https://github.com/daodao97/chatdoc) | 437 |\\n|[showlab/VLog](https://github.com/showlab/VLog) | 436 |\\n|[wandb/weave](https://github.com/wandb/weave) | 420 |\\n|[QwenLM/Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) | 419 |\\n|[huchenxucs/ChatDB](https://github.com/huchenxucs/ChatDB) | 416 |\\n|[jerlendds/osintbuddy](https://github.com/jerlendds/osintbuddy) | 411 |\\n|[monarch-initiative/ontogpt](https://github.com/monarch-initiative/ontogpt) | 408 |\\n|[mallorbc/Finetune_LLMs](https://github.com/mallorbc/Finetune_LLMs) | 406 |\\n|[JayZeeDesign/researcher-gpt](https://github.com/JayZeeDesign/researcher-gpt) | 405 |\\n|[rsaryev/talk-codebase](https://github.com/rsaryev/talk-codebase) | 401 |\\n|[langchain-ai/langsmith-cookbook](https://github.com/langchain-ai/langsmith-cookbook) | 398 |\\n|[mtenenholtz/chat-twitter](https://github.com/mtenenholtz/chat-twitter) | 398 |\\n|[morpheuslord/GPT_Vuln-analyzer](https://github.com/morpheuslord/GPT_Vuln-analyzer) | 391 |\\n|[MagnivOrg/prompt-layer-library](https://github.com/MagnivOrg/prompt-layer-library) | 387 |\\n|[JohnSnowLabs/langtest](https://github.com/JohnSnowLabs/langtest) | 384 |\\n|[mrwadams/attackgen](https://github.com/mrwadams/attackgen) | 381 |\\n|[codefuse-ai/Test-Agent](https://github.com/codefuse-ai/Test-Agent) | 380 |\\n|[personoids/personoids-lite](https://github.com/personoids/personoids-lite) | 379 |\\n|[mosaicml/examples](https://github.com/mosaicml/examples) | 378 |\\n|[steamship-packages/langchain-production-starter](https://github.com/steamship-packages/langchain-production-starter) | 370 |\\n|[FlagAI-Open/Aquila2](https://github.com/FlagAI-Open/Aquila2) | 365 |\\n|[Mintplex-Labs/vector-admin](https://github.com/Mintplex-Labs/vector-admin) | 365 |\\n|[NimbleBoxAI/ChainFury](https://github.com/NimbleBoxAI/ChainFury) | 357 |\\n|[BlackHC/llm-strategy](https://github.com/BlackHC/llm-strategy) | 354 |\\n|[lilacai/lilac](https://github.com/lilacai/lilac) | 352 |\\n|[preset-io/promptimize](https://github.com/preset-io/promptimize) | 351 |\\n|[yuanjie-ai/ChatLLM](https://github.com/yuanjie-ai/ChatLLM) | 347 |\\n|[andylokandy/gpt-4-search](https://github.com/andylokandy/gpt-4-search) | 346 |\\n|[zhoudaquan/ChatAnything](https://github.com/zhoudaquan/ChatAnything) | 343 |\\n|[rgomezcasas/dotfiles](https://github.com/rgomezcasas/dotfiles) | 343 |\\n|[tigerlab-ai/tiger](https://github.com/tigerlab-ai/tiger) | 342 |\\n|[HumanSignal/label-studio-ml-backend](https://github.com/HumanSignal/label-studio-ml-backend) | 334 |\\n|[nasa-petal/bidara](https://github.com/nasa-petal/bidara) | 334 |\\n|[momegas/megabots](https://github.com/momegas/megabots) | 334 |\\n|[Cheems-Seminar/grounded-segment-any-parts](https://github.com/Cheems-Seminar/grounded-segment-any-parts) | 330 |\\n|[CambioML/pykoi](https://github.com/CambioML/pykoi) | 326 |\\n|[Nuggt-dev/Nuggt](https://github.com/Nuggt-dev/Nuggt) | 326 |\\n|[wandb/edu](https://github.com/wandb/edu) | 326 |\\n|[Haste171/langchain-chatbot](https://github.com/Haste171/langchain-chatbot) | 324 |\\n|[sugarforever/LangChain-Tutorials](https://github.com/sugarforever/LangChain-Tutorials) | 322 |\\n|[liangwq/Chatglm_lora_multi-gpu](https://github.com/liangwq/Chatglm_lora_multi-gpu) | 321 |\\n|[ur-whitelab/chemcrow-public](https://github.com/ur-whitelab/chemcrow-public) | 320 |\\n|[itamargol/openai](https://github.com/itamargol/openai) | 318 |\\n|[gia-guar/JARVIS-ChatGPT](https://github.com/gia-guar/JARVIS-ChatGPT) | 304 |\\n|[SpecterOps/Nemesis](https://github.com/SpecterOps/Nemesis) | 302 |\\n|[facebookresearch/personal-timeline](https://github.com/facebookresearch/personal-timeline) | 302 |\\n|[hnawaz007/pythondataanalysis](https://github.com/hnawaz007/pythondataanalysis) | 301 |\\n|[Chainlit/cookbook](https://github.com/Chainlit/cookbook) | 300 |\\n|[airobotlab/KoChatGPT](https://github.com/airobotlab/KoChatGPT) | 300 |\\n|[GPT-Fathom/GPT-Fathom](https://github.com/GPT-Fathom/GPT-Fathom) | 299 |\\n|[kaarthik108/snowChat](https://github.com/kaarthik108/snowChat) | 299 |\\n|[kyegomez/swarms](https://github.com/kyegomez/swarms) | 296 |\\n|[LangStream/langstream](https://github.com/LangStream/langstream) | 295 |\\n|[genia-dev/GeniA](https://github.com/genia-dev/GeniA) | 294 |\\n|[shamspias/customizable-gpt-chatbot](https://github.com/shamspias/customizable-gpt-chatbot) | 291 |\\n|[TsinghuaDatabaseGroup/DB-GPT](https://github.com/TsinghuaDatabaseGroup/DB-GPT) | 290 |\\n|[conceptofmind/toolformer](https://github.com/conceptofmind/toolformer) | 283 |\\n|[sullivan-sean/chat-langchainjs](https://github.com/sullivan-sean/chat-langchainjs) | 283 |\\n|[AutoPackAI/beebot](https://github.com/AutoPackAI/beebot) | 282 |\\n|[pablomarin/GPT-Azure-Search-Engine](https://github.com/pablomarin/GPT-Azure-Search-Engine) | 282 |\\n|[gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) | 280 |\\n|[gustavz/DataChad](https://github.com/gustavz/DataChad) | 280 |\\n|[Safiullah-Rahu/CSV-AI](https://github.com/Safiullah-Rahu/CSV-AI) | 278 |\\n|[hwchase17/chroma-langchain](https://github.com/hwchase17/chroma-langchain) | 275 |\\n|[AkshitIreddy/Interactive-LLM-Powered-NPCs](https://github.com/AkshitIreddy/Interactive-LLM-Powered-NPCs) | 268 |\\n|[ennucore/clippinator](https://github.com/ennucore/clippinator) | 267 |\\n|[artitw/text2text](https://github.com/artitw/text2text) | 264 |\\n|[anarchy-ai/LLM-VM](https://github.com/anarchy-ai/LLM-VM) | 263 |\\n|[wpydcr/LLM-Kit](https://github.com/wpydcr/LLM-Kit) | 262 |\\n|[streamlit/llm-examples](https://github.com/streamlit/llm-examples) | 262 |\\n|[paolorechia/learn-langchain](https://github.com/paolorechia/learn-langchain) | 262 |\\n|[yym68686/ChatGPT-Telegram-Bot](https://github.com/yym68686/ChatGPT-Telegram-Bot) | 261 |\\n|[PradipNichite/Youtube-Tutorials](https://github.com/PradipNichite/Youtube-Tutorials) | 259 |\\n|[radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT) | 259 |\\n|[ur-whitelab/exmol](https://github.com/ur-whitelab/exmol) | 259 |\\n|[ml6team/fondant](https://github.com/ml6team/fondant) | 254 |\\n|[bborn/howdoi.ai](https://github.com/bborn/howdoi.ai) | 254 |\\n|[rahulnyk/knowledge_graph](https://github.com/rahulnyk/knowledge_graph) | 253 |\\n|[recalign/RecAlign](https://github.com/recalign/RecAlign) | 248 |\\n|[hwchase17/langchain-streamlit-template](https://github.com/hwchase17/langchain-streamlit-template) | 248 |\\n|[fetchai/uAgents](https://github.com/fetchai/uAgents) | 247 |\\n|[arthur-ai/bench](https://github.com/arthur-ai/bench) | 247 |\\n|[miaoshouai/miaoshouai-assistant](https://github.com/miaoshouai/miaoshouai-assistant) | 246 |\\n|[RoboCoachTechnologies/GPT-Synthesizer](https://github.com/RoboCoachTechnologies/GPT-Synthesizer) | 244 |\\n|[langchain-ai/web-explorer](https://github.com/langchain-ai/web-explorer) | 242 |\\n|[kaleido-lab/dolphin](https://github.com/kaleido-lab/dolphin) | 242 |\\n|[PJLab-ADG/DriveLikeAHuman](https://github.com/PJLab-ADG/DriveLikeAHuman) | 241 |\\n|[stepanogil/autonomous-hr-chatbot](https://github.com/stepanogil/autonomous-hr-chatbot) | 238 |\\n|[WongSaang/chatgpt-ui-server](https://github.com/WongSaang/chatgpt-ui-server) | 236 |\\n|[nexus-stc/stc](https://github.com/nexus-stc/stc) | 235 |\\n|[yeagerai/genworlds](https://github.com/yeagerai/genworlds) | 235 |\\n|[Gentopia-AI/Gentopia](https://github.com/Gentopia-AI/Gentopia) | 235 |\\n|[alphasecio/langchain-examples](https://github.com/alphasecio/langchain-examples) | 235 |\\n|[grumpyp/aixplora](https://github.com/grumpyp/aixplora) | 232 |\\n|[shaman-ai/agent-actors](https://github.com/shaman-ai/agent-actors) | 232 |\\n|[darrenburns/elia](https://github.com/darrenburns/elia) | 231 |\\n|[orgexyz/BlockAGI](https://github.com/orgexyz/BlockAGI) | 231 |\\n|[handrew/browserpilot](https://github.com/handrew/browserpilot) | 226 |\\n|[su77ungr/CASALIOY](https://github.com/su77ungr/CASALIOY) | 225 |\\n|[nicknochnack/LangchainDocuments](https://github.com/nicknochnack/LangchainDocuments) | 225 |\\n|[dbpunk-labs/octogen](https://github.com/dbpunk-labs/octogen) | 224 |\\n|[langchain-ai/weblangchain](https://github.com/langchain-ai/weblangchain) | 222 |\\n|[CL-lau/SQL-GPT](https://github.com/CL-lau/SQL-GPT) | 222 |\\n|[alvarosevilla95/autolang](https://github.com/alvarosevilla95/autolang) | 221 |\\n|[showlab/UniVTG](https://github.com/showlab/UniVTG) | 220 |\\n|[edreisMD/plugnplai](https://github.com/edreisMD/plugnplai) | 219 |\\n|[hardbyte/qabot](https://github.com/hardbyte/qabot) | 216 |\\n|[microsoft/azure-openai-in-a-day-workshop](https://github.com/microsoft/azure-openai-in-a-day-workshop) | 215 |\\n|[Azure-Samples/chat-with-your-data-solution-accelerator](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator) | 214 |\\n|[amadad/agentcy](https://github.com/amadad/agentcy) | 213 |\\n|[snexus/llm-search](https://github.com/snexus/llm-search) | 212 |\\n|[afaqueumer/DocQA](https://github.com/afaqueumer/DocQA) | 206 |\\n|[plchld/InsightFlow](https://github.com/plchld/InsightFlow) | 205 |\\n|[yasyf/compress-gpt](https://github.com/yasyf/compress-gpt) | 205 |\\n|[benthecoder/ClassGPT](https://github.com/benthecoder/ClassGPT) | 205 |\\n|[voxel51/voxelgpt](https://github.com/voxel51/voxelgpt) | 204 |\\n|[jbrukh/gpt-jargon](https://github.com/jbrukh/gpt-jargon) | 204 |\\n|[emarco177/ice_breaker](https://github.com/emarco177/ice_breaker) | 204 |\\n|[tencentmusic/supersonic](https://github.com/tencentmusic/supersonic) | 202 |\\n|[Azure-Samples/azure-search-power-skills](https://github.com/Azure-Samples/azure-search-power-skills) | 202 |\\n|[blob42/Instrukt](https://github.com/blob42/Instrukt) | 201 |\\n|[langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk) | 200 |\\n|[SamPink/dev-gpt](https://github.com/SamPink/dev-gpt) | 200 |\\n|[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators) | 198 |\\n|[KMnO4-zx/huanhuan-chat](https://github.com/KMnO4-zx/huanhuan-chat) | 196 |\\n|[Azure-Samples/jp-azureopenai-samples](https://github.com/Azure-Samples/jp-azureopenai-samples) | 192 |\\n|[hongbo-miao/hongbomiao.com](https://github.com/hongbo-miao/hongbomiao.com) | 190 |\\n|[CakeCrusher/openplugin](https://github.com/CakeCrusher/openplugin) | 190 |\\n|[PaddlePaddle/ERNIE-Bot-SDK](https://github.com/PaddlePaddle/ERNIE-Bot-SDK) | 189 |\\n|[retr0reg/Ret2GPT](https://github.com/retr0reg/Ret2GPT) | 189 |\\n|[AmineDiro/cria](https://github.com/AmineDiro/cria) | 187 |\\n|[lancedb/vectordb-recipes](https://github.com/lancedb/vectordb-recipes) | 186 |\\n|[vaibkumr/prompt-optimizer](https://github.com/vaibkumr/prompt-optimizer) | 185 |\\n|[aws-ia/ecs-blueprints](https://github.com/aws-ia/ecs-blueprints) | 184 |\\n|[ethanyanjiali/minChatGPT](https://github.com/ethanyanjiali/minChatGPT) | 183 |\\n|[MuhammadMoinFaisal/LargeLanguageModelsProjects](https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects) | 182 |\\n|[shauryr/S2QA](https://github.com/shauryr/S2QA) | 181 |\\n|[summarizepaper/summarizepaper](https://github.com/summarizepaper/summarizepaper) | 180 |\\n|[NomaDamas/RAGchain](https://github.com/NomaDamas/RAGchain) | 179 |\\n|[pnkvalavala/repochat](https://github.com/pnkvalavala/repochat) | 179 |\\n|[ibiscp/LLM-IMDB](https://github.com/ibiscp/LLM-IMDB) | 177 |\\n|[fengyuli-dev/multimedia-gpt](https://github.com/fengyuli-dev/multimedia-gpt) | 177 |\\n|[langchain-ai/text-split-explorer](https://github.com/langchain-ai/text-split-explorer) | 175 |\\n|[iMagist486/ElasticSearch-Langchain-Chatglm2](https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2) | 175 |\\n|[limaoyi1/Auto-PPT](https://github.com/limaoyi1/Auto-PPT) | 175 |\\n|[Open-Swarm-Net/GPT-Swarm](https://github.com/Open-Swarm-Net/GPT-Swarm) | 175 |\\n|[morpheuslord/HackBot](https://github.com/morpheuslord/HackBot) | 174 |\\n|[v7labs/benchllm](https://github.com/v7labs/benchllm) | 174 |\\n|[Coding-Crashkurse/Langchain-Full-Course](https://github.com/Coding-Crashkurse/Langchain-Full-Course) | 174 |\\n|[dongyh20/Octopus](https://github.com/dongyh20/Octopus) | 173 |\\n|[kimtth/azure-openai-llm-vector-langchain](https://github.com/kimtth/azure-openai-llm-vector-langchain) | 173 |\\n|[mayooear/private-chatbot-mpt30b-langchain](https://github.com/mayooear/private-chatbot-mpt30b-langchain) | 173 |\\n|[zilliztech/akcio](https://github.com/zilliztech/akcio) | 172 |\\n|[jmpaz/promptlib](https://github.com/jmpaz/promptlib) | 172 |\\n|[ccurme/yolopandas](https://github.com/ccurme/yolopandas) | 172 |\\n|[joaomdmoura/CrewAI](https://github.com/joaomdmoura/CrewAI) | 170 |\\n|[katanaml/llm-mistral-invoice-cpu](https://github.com/katanaml/llm-mistral-invoice-cpu) | 170 |\\n|[chakkaradeep/pyCodeAGI](https://github.com/chakkaradeep/pyCodeAGI) | 170 |\\n|[mudler/LocalAGI](https://github.com/mudler/LocalAGI) | 167 |\\n|[dssjon/biblos](https://github.com/dssjon/biblos) | 165 |\\n|[kjappelbaum/gptchem](https://github.com/kjappelbaum/gptchem) | 165 |\\n|[xxw1995/chatglm3-finetune](https://github.com/xxw1995/chatglm3-finetune) | 164 |\\n|[ArjanCodes/examples](https://github.com/ArjanCodes/examples) | 163 |\\n|[AIAnytime/Llama2-Medical-Chatbot](https://github.com/AIAnytime/Llama2-Medical-Chatbot) | 163 |\\n|[RCGAI/SimplyRetrieve](https://github.com/RCGAI/SimplyRetrieve) | 162 |\\n|[langchain-ai/langchain-teacher](https://github.com/langchain-ai/langchain-teacher) | 162 |\\n|[menloparklab/falcon-langchain](https://github.com/menloparklab/falcon-langchain) | 162 |\\n|[flurb18/AgentOoba](https://github.com/flurb18/AgentOoba) | 162 |\\n|[homanp/vercel-langchain](https://github.com/homanp/vercel-langchain) | 161 |\\n|[jiran214/langup-ai](https://github.com/jiran214/langup-ai) | 160 |\\n|[JorisdeJong123/7-Days-of-LangChain](https://github.com/JorisdeJong123/7-Days-of-LangChain) | 160 |\\n|[GoogleCloudPlatform/data-analytics-golden-demo](https://github.com/GoogleCloudPlatform/data-analytics-golden-demo) | 159 |\\n|[positive666/Prompt-Can-Anything](https://github.com/positive666/Prompt-Can-Anything) | 159 |\\n|[luisroque/large_laguage_models](https://github.com/luisroque/large_laguage_models) | 159 |\\n|[mlops-for-all/mlops-for-all.github.io](https://github.com/mlops-for-all/mlops-for-all.github.io) | 158 |\\n|[wandb/wandbot](https://github.com/wandb/wandbot) | 158 |\\n|[elastic/elasticsearch-labs](https://github.com/elastic/elasticsearch-labs) | 157 |\\n|[shroominic/funcchain](https://github.com/shroominic/funcchain) | 157 |\\n|[deeppavlov/dream](https://github.com/deeppavlov/dream) | 156 |\\n|[mluogh/eastworld](https://github.com/mluogh/eastworld) | 154 |\\n|[georgesung/llm_qlora](https://github.com/georgesung/llm_qlora) | 154 |\\n|[RUC-GSAI/YuLan-Rec](https://github.com/RUC-GSAI/YuLan-Rec) | 153 |\\n|[KylinC/ChatFinance](https://github.com/KylinC/ChatFinance) | 152 |\\n|[Dicklesworthstone/llama2_aided_tesseract](https://github.com/Dicklesworthstone/llama2_aided_tesseract) | 152 |\\n|[c0sogi/LLMChat](https://github.com/c0sogi/LLMChat) | 152 |\\n|[eunomia-bpf/GPTtrace](https://github.com/eunomia-bpf/GPTtrace) | 152 |\\n|[ErikBjare/gptme](https://github.com/ErikBjare/gptme) | 152 |\\n|[Klingefjord/chatgpt-telegram](https://github.com/Klingefjord/chatgpt-telegram) | 152 |\\n|[RoboCoachTechnologies/ROScribe](https://github.com/RoboCoachTechnologies/ROScribe) | 151 |\\n|[Aggregate-Intellect/sherpa](https://github.com/Aggregate-Intellect/sherpa) | 151 |\\n|[3Alan/DocsMind](https://github.com/3Alan/DocsMind) | 151 |\\n|[tangqiaoyu/ToolAlpaca](https://github.com/tangqiaoyu/ToolAlpaca) | 150 |\\n|[kulltc/chatgpt-sql](https://github.com/kulltc/chatgpt-sql) | 150 |\\n|[mallahyari/drqa](https://github.com/mallahyari/drqa) | 150 |\\n|[MedalCollector/Orator](https://github.com/MedalCollector/Orator) | 149 |\\n|[Teahouse-Studios/akari-bot](https://github.com/Teahouse-Studios/akari-bot) | 149 |\\n|[realminchoi/babyagi-ui](https://github.com/realminchoi/babyagi-ui) | 148 |\\n|[ssheng/BentoChain](https://github.com/ssheng/BentoChain) | 148 |\\n|[solana-labs/chatgpt-plugin](https://github.com/solana-labs/chatgpt-plugin) | 147 |\\n|[aurelio-labs/arxiv-bot](https://github.com/aurelio-labs/arxiv-bot) | 147 |\\n|[Jaseci-Labs/jaseci](https://github.com/Jaseci-Labs/jaseci) | 146 |\\n|[menloparklab/langchain-cohere-qdrant-doc-retrieval](https://github.com/menloparklab/langchain-cohere-qdrant-doc-retrieval) | 146 |\\n|[trancethehuman/entities-extraction-web-scraper](https://github.com/trancethehuman/entities-extraction-web-scraper) | 144 |\\n|[peterw/StoryStorm](https://github.com/peterw/StoryStorm) | 144 |\\n|[grumpyp/chroma-langchain-tutorial](https://github.com/grumpyp/chroma-langchain-tutorial) | 144 |\\n|[gh18l/CrawlGPT](https://github.com/gh18l/CrawlGPT) | 142 |\\n|[langchain-ai/langchain-aws-template](https://github.com/langchain-ai/langchain-aws-template) | 142 |\\n|[yasyf/summ](https://github.com/yasyf/summ) | 141 |\\n|[petehunt/langchain-github-bot](https://github.com/petehunt/langchain-github-bot) | 141 |\\n|[hirokidaichi/wanna](https://github.com/hirokidaichi/wanna) | 140 |\\n|[jina-ai/fastapi-serve](https://github.com/jina-ai/fastapi-serve) | 139 |\\n|[zenml-io/zenml-projects](https://github.com/zenml-io/zenml-projects) | 139 |\\n|[jlonge4/local_llama](https://github.com/jlonge4/local_llama) | 139 |\\n|[smyja/blackmaria](https://github.com/smyja/blackmaria) | 138 |\\n|[ChuloAI/BrainChulo](https://github.com/ChuloAI/BrainChulo) | 137 |\\n|[log1stics/voice-generator-webui](https://github.com/log1stics/voice-generator-webui) | 137 |\\n|[davila7/file-gpt](https://github.com/davila7/file-gpt) | 137 |\\n|[dcaribou/transfermarkt-datasets](https://github.com/dcaribou/transfermarkt-datasets) | 136 |\\n|[ciare-robotics/world-creator](https://github.com/ciare-robotics/world-creator) | 135 |\\n|[Undertone0809/promptulate](https://github.com/Undertone0809/promptulate) | 134 |\\n|[fixie-ai/fixie-examples](https://github.com/fixie-ai/fixie-examples) | 134 |\\n|[run-llama/ai-engineer-workshop](https://github.com/run-llama/ai-engineer-workshop) | 133 |\\n|[definitive-io/code-indexer-loop](https://github.com/definitive-io/code-indexer-loop) | 131 |\\n|[mortium91/langchain-assistant](https://github.com/mortium91/langchain-assistant) | 131 |\\n|[baidubce/bce-qianfan-sdk](https://github.com/baidubce/bce-qianfan-sdk) | 130 |\\n|[Ngonie-x/langchain_csv](https://github.com/Ngonie-x/langchain_csv) | 130 |\\n|[IvanIsCoding/ResuLLMe](https://github.com/IvanIsCoding/ResuLLMe) | 130 |\\n|[AnchoringAI/anchoring-ai](https://github.com/AnchoringAI/anchoring-ai) | 129 |\\n|[Azure/business-process-automation](https://github.com/Azure/business-process-automation) | 128 |\\n|[athina-ai/athina-sdk](https://github.com/athina-ai/athina-sdk) | 126 |\\n|[thunlp/ChatEval](https://github.com/thunlp/ChatEval) | 126 |\\n|[prof-frink-lab/slangchain](https://github.com/prof-frink-lab/slangchain) | 126 |\\n|[vietanhdev/pautobot](https://github.com/vietanhdev/pautobot) | 125 |\\n|[awslabs/generative-ai-cdk-constructs](https://github.com/awslabs/generative-ai-cdk-constructs) | 124 |\\n|[sdaaron/QueryGPT](https://github.com/sdaaron/QueryGPT) | 124 |\\n|[rabbitmetrics/langchain-13-min](https://github.com/rabbitmetrics/langchain-13-min) | 124 |\\n|[AutoLLM/AutoAgents](https://github.com/AutoLLM/AutoAgents) | 122 |\\n|[nicknochnack/Nopenai](https://github.com/nicknochnack/Nopenai) | 122 |\\n|[wombyz/HormoziGPT](https://github.com/wombyz/HormoziGPT) | 122 |\\n|[dotvignesh/PDFChat](https://github.com/dotvignesh/PDFChat) | 122 |\\n|[topoteretes/PromethAI-Backend](https://github.com/topoteretes/PromethAI-Backend) | 121 |\\n|[nftblackmagic/flask-langchain](https://github.com/nftblackmagic/flask-langchain) | 121 |\\n|[vishwasg217/finsight](https://github.com/vishwasg217/finsight) | 120 |\\n|[snap-stanford/MLAgentBench](https://github.com/snap-stanford/MLAgentBench) | 120 |\\n|[Azure/app-service-linux-docs](https://github.com/Azure/app-service-linux-docs) | 120 |\\n|[nyanp/chat2plot](https://github.com/nyanp/chat2plot) | 120 |\\n|[ant4g0nist/polar](https://github.com/ant4g0nist/polar) | 119 |\\n|[aws-samples/cdk-eks-blueprints-patterns](https://github.com/aws-samples/cdk-eks-blueprints-patterns) | 119 |\\n|[aws-samples/amazon-kendra-langchain-extensions](https://github.com/aws-samples/amazon-kendra-langchain-extensions) | 119 |\\n|[Xueheng-Li/SynologyChatbotGPT](https://github.com/Xueheng-Li/SynologyChatbotGPT) | 119 |\\n|[CodeAlchemyAI/ViLT-GPT](https://github.com/CodeAlchemyAI/ViLT-GPT) | 117 |\\n|[Lin-jun-xiang/docGPT-langchain](https://github.com/Lin-jun-xiang/docGPT-langchain) | 117 |\\n|[ademakdogan/ChatSQL](https://github.com/ademakdogan/ChatSQL) | 116 |\\n|[aniketmaurya/llm-inference](https://github.com/aniketmaurya/llm-inference) | 115 |\\n|[xuwenhao/mactalk-ai-course](https://github.com/xuwenhao/mactalk-ai-course) | 115 |\\n|[cmooredev/RepoReader](https://github.com/cmooredev/RepoReader) | 115 |\\n|[abi/autocommit](https://github.com/abi/autocommit) | 115 |\\n|[MIDORIBIN/langchain-gpt4free](https://github.com/MIDORIBIN/langchain-gpt4free) | 114 |\\n|[finaldie/auto-news](https://github.com/finaldie/auto-news) | 114 |\\n|[Anil-matcha/Youtube-to-chatbot](https://github.com/Anil-matcha/Youtube-to-chatbot) | 114 |\\n|[avrabyt/MemoryBot](https://github.com/avrabyt/MemoryBot) | 114 |\\n|[Capsize-Games/airunner](https://github.com/Capsize-Games/airunner) | 113 |\\n|[atisharma/llama_farm](https://github.com/atisharma/llama_farm) | 113 |\\n|[mbchang/data-driven-characters](https://github.com/mbchang/data-driven-characters) | 112 |\\n|[fiddler-labs/fiddler-auditor](https://github.com/fiddler-labs/fiddler-auditor) | 112 |\\n|[dirkjbreeuwer/gpt-automated-web-scraper](https://github.com/dirkjbreeuwer/gpt-automated-web-scraper) | 111 |\\n|[Appointat/Chat-with-Document-s-using-ChatGPT-API-and-Text-Embedding](https://github.com/Appointat/Chat-with-Document-s-using-ChatGPT-API-and-Text-Embedding) | 111 |\\n|[hwchase17/langchain-gradio-template](https://github.com/hwchase17/langchain-gradio-template) | 111 |\\n|[artas728/spelltest](https://github.com/artas728/spelltest) | 110 |\\n|[NVIDIA/GenerativeAIExamples](https://github.com/NVIDIA/GenerativeAIExamples) | 109 |\\n|[Azure/aistudio-copilot-sample](https://github.com/Azure/aistudio-copilot-sample) | 108 |\\n|[codefuse-ai/codefuse-chatbot](https://github.com/codefuse-ai/codefuse-chatbot) | 108 |\\n|[apirrone/Memento](https://github.com/apirrone/Memento) | 108 |\\n|[e-johnstonn/GPT-Doc-Summarizer](https://github.com/e-johnstonn/GPT-Doc-Summarizer) | 108 |\\n|[salesforce/BOLAA](https://github.com/salesforce/BOLAA) | 107 |\\n|[Erol444/gpt4-openai-api](https://github.com/Erol444/gpt4-openai-api) | 106 |\\n|[linjungz/chat-with-your-doc](https://github.com/linjungz/chat-with-your-doc) | 106 |\\n|[crosleythomas/MirrorGPT](https://github.com/crosleythomas/MirrorGPT) | 106 |\\n|[panaverse/learn-generative-ai](https://github.com/panaverse/learn-generative-ai) | 105 |\\n|[Azure/azure-sdk-tools](https://github.com/Azure/azure-sdk-tools) | 105 |\\n|[malywut/gpt_examples](https://github.com/malywut/gpt_examples) | 105 |\\n|[ritun16/chain-of-verification](https://github.com/ritun16/chain-of-verification) | 104 |\\n|[langchain-ai/langchain-benchmarks](https://github.com/langchain-ai/langchain-benchmarks) | 104 |\\n|[lightninglabs/LangChainBitcoin](https://github.com/lightninglabs/LangChainBitcoin) | 104 |\\n|[flepied/second-brain-agent](https://github.com/flepied/second-brain-agent) | 103 |\\n|[llmapp/openai.mini](https://github.com/llmapp/openai.mini) | 102 |\\n|[gimlet-ai/tddGPT](https://github.com/gimlet-ai/tddGPT) | 102 |\\n|[jlonge4/gpt_chatwithPDF](https://github.com/jlonge4/gpt_chatwithPDF) | 102 |\\n|[agentification/RAFA_code](https://github.com/agentification/RAFA_code) | 101 |\\n|[pacman100/DHS-LLM-Workshop](https://github.com/pacman100/DHS-LLM-Workshop) | 101 |\\n|[aws-samples/private-llm-qa-bot](https://github.com/aws-samples/private-llm-qa-bot) | 101 |'), Document(metadata={'source': 'docs/docs/additional_resources/dependents.mdx', 'file_path': 'docs/docs/additional_resources/dependents.mdx', 'file_name': 'dependents.mdx', 'file_type': '.mdx'}, page_content='_Generated by [github-dependents-info](https://github.com/nvuillam/github-dependents-info)_\\n\\n`github-dependents-info --repo \"langchain-ai/langchain\" --markdownfile dependents.md --minstars 100 --sort stars`'), Document(metadata={'source': 'docs/docs/additional_resources/tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_type': '.mdx'}, page_content='# 3rd Party Tutorials\\n\\n##  Tutorials'), Document(metadata={'source': 'docs/docs/additional_resources/tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_type': '.mdx'}, page_content='### [LangChain v 0.1 by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae0gBSJ9T0w7cu7iJZbH3T31)\\n### [Build with Langchain - Advanced by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae06tclDATrMYY0idsTdLg9v)\\n### [LangGraph by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg)\\n### [by Greg Kamradt](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\\n### [by Sam Witteveen](https://www.youtube.com/playlist?list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ)\\n### [by James Briggs](https://www.youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)\\n### [by Prompt Engineering](https://www.youtube.com/playlist?list=PLVEEucA9MYhOu89CX8H3MBZqayTbcCTMr)\\n### [by Mayo Oshin](https://www.youtube.com/@chatwithdata/search?query=langchain)\\n### [by 1 little Coder](https://www.youtube.com/playlist?list=PLpdmBGJ6ELUK-v0MK-t4wZmVEbxM5xk6L)\\n### [by BobLin (Chinese language)](https://www.youtube.com/playlist?list=PLbd7ntv6PxC3QMFQvtWfk55p-Op_syO1C)\\n### [by Total Technology Zonne](https://youtube.com/playlist?list=PLI8raxzYtfGyE02fAxiM1CPhLUuqcTLWg&si=fkAye16rQKBJVHc9)'), Document(metadata={'source': 'docs/docs/additional_resources/tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_type': '.mdx'}, page_content='## Courses\\n\\n### Featured courses on Deeplearning.AI\\n\\n- [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\\n- [LangChain Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)\\n- [Functions, Tools and Agents with LangChain](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/)\\n- [Build LLM Apps with LangChain.js](https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/)\\n\\n### Online courses'), Document(metadata={'source': 'docs/docs/additional_resources/tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_type': '.mdx'}, page_content='- [Udemy](https://www.udemy.com/courses/search/?q=langchain)\\n- [DataCamp](https://www.datacamp.com/courses/developing-llm-applications-with-langchain)\\n- [Pluralsight](https://www.pluralsight.com/search?q=langchain)\\n- [Coursera](https://www.coursera.org/search?query=langchain)\\n- [Maven](https://maven.com/courses?query=langchain)\\n- [Udacity](https://www.udacity.com/catalog/all/any-price/any-school/any-skill/any-difficulty/any-duration/any-type/relevance/page-1?searchValue=langchain)\\n- [LinkedIn Learning](https://www.linkedin.com/search/results/learning/?keywords=langchain)\\n- [edX](https://www.edx.org/search?q=langchain)\\n- [freeCodeCamp](https://www.youtube.com/@freecodecamp/search?query=langchain)\\n\\n## Short Tutorials\\n\\n- [by Nicholas Renotte](https://youtu.be/MlK6SIjcjE8)\\n- [by Patrick Loeber](https://youtu.be/LbT1yp6quS8)\\n- [by Rabbitmetrics](https://youtu.be/aywZrzNaKjs)\\n- [by Ivan Reznikov](https://medium.com/@ivanreznikov/langchain-101-course-updated-668f7b41d6cb)'), Document(metadata={'source': 'docs/docs/additional_resources/tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_type': '.mdx'}, page_content='## Books and Handbooks\\n\\n- [Generative AI with LangChain](https://www.amazon.com/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/ref=sr_1_1?crid=1GMOMH0G7GLR&keywords=generative+ai+with+langchain&qid=1703247181&sprefix=%2Caps%2C298&sr=8-1) by [Ben Auffrath](https://www.amazon.com/stores/Ben-Auffarth/author/B08JQKSZ7D?ref=ap_rdr&store_ref=ap_rdr&isDramIntegrated=true&shoppingPortalEnabled=true), ©️ 2023 Packt Publishing\\n- [LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) By **James Briggs** and **Francisco Ingham**\\n- [LangChain Cheatsheet](https://pub.towardsai.net/langchain-cheatsheet-all-secrets-on-a-single-page-8be26b721cde) by **Ivan Reznikov**\\n- [Dive into Langchain (Chinese language)](https://langchain.boblin.app/)\\n\\n---------------------'), Document(metadata={'source': 'docs/docs/additional_resources/youtube.mdx', 'file_path': 'docs/docs/additional_resources/youtube.mdx', 'file_name': 'youtube.mdx', 'file_type': '.mdx'}, page_content='# YouTube videos\\n\\n[Updated 2024-05-16]\\n\\n### [Official LangChain YouTube channel](https://www.youtube.com/@LangChain)\\n\\n### [Tutorials on YouTube](/docs/additional_resources/tutorials/#tutorials)\\n\\n## Videos (sorted by views)\\n\\nOnly videos with 40K+ views:'), Document(metadata={'source': 'docs/docs/additional_resources/youtube.mdx', 'file_path': 'docs/docs/additional_resources/youtube.mdx', 'file_name': 'youtube.mdx', 'file_type': '.mdx'}, page_content=\"- [Using `ChatGPT` with YOUR OWN Data. This is magical. (LangChain `OpenAI API`)](https://youtu.be/9AXP7tCI9PI)\\n- [Chat with Multiple `PDFs` | LangChain App Tutorial in Python (Free LLMs and Embeddings)](https://youtu.be/dXxQ0LR-3Hg?si=pjXKhsHRzn10vOqX)\\n- [`Hugging Face` + Langchain in 5 mins | Access 200k+ FREE AI models for your AI apps](https://youtu.be/_j7JEDWuqLE?si=psimQscN3qo2dOa9)\\n- [LangChain Crash Course For Beginners | LangChain Tutorial](https://youtu.be/nAmC7SoVLd8?si=qJdvyG5-rnjqfdj1)\\n- [Vector Embeddings Tutorial – Code Your Own AI Assistant with GPT-4 API + LangChain + NLP](https://youtu.be/yfHHvmaMkcA?si=UBP3yw50cLm3a2nj)\\n- [Development with Large Language Models Tutorial – `OpenAI`, Langchain, Agents, `Chroma`](https://youtu.be/xZDB1naRUlk?si=v8J1q6oFHRyTkf7Y)\\n- [Langchain: `PDF` Chat App (GUI) | ChatGPT for Your PDF FILES | Step-by-Step Tutorial](https://youtu.be/RIWbalZ7sTo?si=LbKsCcuyv0BtnrTY)\\n- [Vector Search `RAG` Tutorial – Combine Your Data with LLMs with Advanced Search](https://youtu.be/JEBDfGqrAUA?si=pD7oxpfwWeJCxfBt)\\n- [LangChain Crash Course for Beginners](https://youtu.be/lG7Uxts9SXs?si=Yte4S5afN7KNCw0F)\\n- [Learn `RAG` From Scratch – Python AI Tutorial from a LangChain Engineer](https://youtu.be/sVcwVQRHIc8?si=_LN4g0vOgSdtlB3S)\\n- [`Llama 2` in LangChain — FIRST Open Source Conversational Agent!](https://youtu.be/6iHVJyX2e50?si=rtq1maPrzWKHbwVV)\\n- [LangChain Tutorial for Beginners | Generative AI Series](https://youtu.be/cQUUkZnyoD0?si=KYz-bvcocdqGh9f_)\\n- [Chatbots with `RAG`: LangChain Full Walkthrough](https://youtu.be/LhnCsygAvzY?si=yS7T98VLfcWdkDek)\\n- [LangChain Explained In 15 Minutes - A MUST Learn For Python Programmers](https://youtu.be/mrjq3lFz23s?si=wkQGcSKUJjuiiEPf)\\n- [LLM Project | End to End LLM Project Using Langchain, `OpenAI` in Finance Domain](https://youtu.be/MoqgmWV1fm8?si=oVl-5kJVgd3a07Y_)\\n- [What is LangChain?](https://youtu.be/1bUy-1hGZpI?si=NZ0D51VM5y-DhjGe)\\n- [`RAG` + Langchain Python Project: Easy AI/Chat For Your Doc](https://youtu.be/tcqEUSNCn8I?si=RLcWPBVLIErRqdmU)\\n- [Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application](https://youtu.be/_FpT1cwcSLg?si=X9qVazlXYucN_JBP)\\n- [LangChain GEN AI Tutorial – 6 End-to-End Projects using OpenAI, Google `Gemini Pro`, `LLAMA2`](https://youtu.be/x0AnCE9SE4A?si=_92gJYm7kb-V2bi0)\\n- [Complete Langchain GEN AI Crash Course With 6 End To End LLM Projects With OPENAI, `LLAMA2`, `Gemini Pro`](https://youtu.be/aWKrL4z5H6w?si=NVLi7Yiq0ccE7xXE)\\n- [AI Leader Reveals The Future of AI AGENTS (LangChain CEO)](https://youtu.be/9ZhbA0FHZYc?si=1r4P6kRvKVvEhRgE)\\n- [Learn How To Query Pdf using Langchain Open AI in 5 min](https://youtu.be/5Ghv-F1wF_0?si=ZZRjrWfeiFOVrcvu)\\n- [Reliable, fully local RAG agents with `LLaMA3`](https://youtu.be/-ROS6gfYIts?si=75CXA8W_BbnkIxcV)\\n- [Learn `LangChain.js` - Build LLM apps with JavaScript and `OpenAI`](https://youtu.be/HSZ_uaif57o?si=Icj-RAhwMT-vHaYA)\\n- [LLM Project | End to End LLM Project Using LangChain, Google Palm In Ed-Tech Industry](https://youtu.be/AjQPRomyd-k?si=eC3NT6kn02Lhpz-_)\\n- [Chatbot Answering from Your Own Knowledge Base: Langchain, `ChatGPT`, `Pinecone`, and `Streamlit`: | Code](https://youtu.be/nAKhxQ3hcMA?si=9Zd_Nd_jiYhtml5w)\\n- [LangChain is AMAZING | Quick Python Tutorial](https://youtu.be/I4mFqyqFkxg?si=aJ66qh558OfNAczD)\\n- [`GirlfriendGPT` - AI girlfriend with LangChain](https://youtu.be/LiN3D1QZGQw?si=kZR-lnJwixeVrjmh)\\n- [Using NEW `MPT-7B` in `Hugging Face` and LangChain](https://youtu.be/DXpk9K7DgMo?si=99JDpV_ueimwJhMi)\\n- [LangChain - COMPLETE TUTORIAL - Basics to advanced concept!](https://youtu.be/a89vqgK-Qcs?si=0aVO2EOqsw7GE5e3)\\n- [LangChain Agents: Simply Explained!](https://youtu.be/Xi9Ui-9qcPw?si=DCuG7nGx8dxcfhkx)\\n- [Chat With Multiple `PDF` Documents With Langchain And Google `Gemini Pro`](https://youtu.be/uus5eLz6smA?si=YUwvHtaZsGeIl0WD)\\n- [LLM Project | End to end LLM project Using Langchain, `Google Palm` in Retail Industry](https://youtu.be/4wtrl4hnPT8?si=_eOKPpdLfWu5UXMQ)\\n- [Tutorial | Chat with any Website using Python and Langchain](https://youtu.be/bupx08ZgSFg?si=KRrjYZFnuLsstGwW)\\n- [Prompt Engineering And LLM's With LangChain In One Shot-Generative AI](https://youtu.be/t2bSApmPzU4?si=87vPQQtYEWTyu2Kx)\\n- [Build a Custom Chatbot with `OpenAI`: `GPT-Index` & LangChain | Step-by-Step Tutorial](https://youtu.be/FIDv6nc4CgU?si=gR1u3DUG9lvzBIKK)\\n- [Search Your `PDF` App using Langchain, `ChromaDB`, and Open Source LLM: No OpenAI API (Runs on CPU)](https://youtu.be/rIV1EseKwU4?si=UxZEoXSiPai8fXgl)\\n- [Building a `RAG` application from scratch using Python, LangChain, and the `OpenAI API`](https://youtu.be/BrsocJb-fAo?si=hvkh9iTGzJ-LnsX-)\\n- [Function Calling via `ChatGPT API` - First Look With LangChain](https://youtu.be/0-zlUy7VUjg?si=Vc6LFseckEc6qvuk)\\n- [Private GPT, free deployment! Langchain-Chachat helps you easily play with major mainstream AI models! | Zero Degree Commentary](https://youtu.be/3LLUyaHP-3I?si=AZumEeFXsvqaLl0f)\\n- [Create a ChatGPT clone using `Streamlit` and LangChain](https://youtu.be/IaTiyQ2oYUQ?si=WbgsYmqPDnMidSUK)\\n- [What's next for AI agents ft. LangChain's Harrison Chase](https://youtu.be/pBBe1pk8hf4?si=H4vdBF9nmkNZxiHt)\\n- [`LangFlow`: Build Chatbots without Writing Code - LangChain](https://youtu.be/KJ-ux3hre4s?si=TJuDu4bAlva1myNL)\\n- [Building a LangChain Custom Medical Agent with Memory](https://youtu.be/6UFtRwWnHws?si=wymYad26VgigRkHy)\\n- [`Ollama` meets LangChain](https://youtu.be/k_1pOF1mj8k?si=RlBiCrmaR3s7SnMK)\\n- [End To End LLM Langchain Project using `Pinecone` Vector Database](https://youtu.be/erUfLIi9OFM?si=aHpuHXdIEmAfS4eF)\\n- [`LLaMA2` with LangChain - Basics | LangChain TUTORIAL](https://youtu.be/cIRzwSXB4Rc?si=FUs0OLVJpzKhut0h)\\n- [Understanding `ReACT` with LangChain](https://youtu.be/Eug2clsLtFs?si=imgj534ggxlypS0d)\"), Document(metadata={'source': 'docs/docs/additional_resources/youtube.mdx', 'file_path': 'docs/docs/additional_resources/youtube.mdx', 'file_name': 'youtube.mdx', 'file_type': '.mdx'}, page_content='---------------------\\n[Updated 2024-05-16]'), Document(metadata={'source': 'docs/docs/concepts/agents.mdx', 'file_path': 'docs/docs/concepts/agents.mdx', 'file_name': 'agents.mdx', 'file_type': '.mdx'}, page_content=\"# Agents\\n\\nBy themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\\n\\n[LangGraph](/docs/concepts/architecture#langgraph) is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.\\n\\nPlease see the following resources for more information:\\n\\n* LangGraph docs on [common agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)\\n* [Pre-built agents in LangGraph](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\\n\\n## Legacy agent concept: AgentExecutor\"), Document(metadata={'source': 'docs/docs/concepts/agents.mdx', 'file_path': 'docs/docs/concepts/agents.mdx', 'file_name': 'agents.mdx', 'file_type': '.mdx'}, page_content=\"LangChain previously introduced the `AgentExecutor` as a runtime for agents. \\nWhile it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents. \\nAs a result, we're gradually phasing out `AgentExecutor` in favor of more flexible solutions in LangGraph.\\n\\n### Transitioning from AgentExecutor to langgraph\\n\\nIf you're currently using `AgentExecutor`, don't worry! We've prepared resources to help you:\\n\\n1. For those who still need to use `AgentExecutor`, we offer a comprehensive guide on [how to use AgentExecutor](/docs/how_to/agent_executor).\\n\\n2. However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed [migration guide](/docs/how_to/migrate_agent) to help you move from `AgentExecutor` to LangGraph seamlessly.\"), Document(metadata={'source': 'docs/docs/concepts/architecture.mdx', 'file_path': 'docs/docs/concepts/architecture.mdx', 'file_name': 'architecture.mdx', 'file_type': '.mdx'}, page_content='import ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n# Architecture\\n\\nLangChain is a framework that consists of a number of packages.\\n\\n<ThemedImage\\n    alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n    sources={{\\n        light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n        dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n    }}\\n    title=\"LangChain Framework Overview\"\\n    style={{ width: \"100%\" }}\\n/>\\n\\n\\n## langchain-core\\n\\nThis package contains base abstractions for different components and ways to compose them together.\\nThe interfaces for core components like chat models, vector stores, tools and more are defined here.\\nNo third-party integrations are defined here.\\nThe dependencies are very lightweight.\\n\\n## langchain'), Document(metadata={'source': 'docs/docs/concepts/architecture.mdx', 'file_path': 'docs/docs/concepts/architecture.mdx', 'file_name': 'architecture.mdx', 'file_type': '.mdx'}, page_content=\"The main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture.\\nThese are NOT third-party integrations.\\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\\n\\n## Integration packages\\n\\nPopular integrations have their own packages (e.g. `langchain-openai`, `langchain-anthropic`, etc) so that they can be properly versioned and appropriately lightweight.\\n\\nFor more information see:\\n\\n* A list [integrations packages](/docs/integrations/providers/)\\n* The [API Reference](https://python.langchain.com/api_reference/) where you can find detailed information about each of the integration package.\\n\\n## langchain-community\"), Document(metadata={'source': 'docs/docs/concepts/architecture.mdx', 'file_path': 'docs/docs/concepts/architecture.mdx', 'file_name': 'architecture.mdx', 'file_type': '.mdx'}, page_content='This package contains third-party integrations that are maintained by the LangChain community.\\nKey integration packages are separated out (see above).\\nThis contains integrations for various components (chat models, vector stores, tools, etc).\\nAll dependencies in this package are optional to keep the package as lightweight as possible.\\n\\n## langgraph\\n\\n`langgraph` is an extension of `langchain` aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.\\n\\n:::info[Further reading]\\n\\n* See our LangGraph overview [here](https://langchain-ai.github.io/langgraph/concepts/high_level/#core-principles).\\n* See our LangGraph Academy Course [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n:::\\n\\n## langserve'), Document(metadata={'source': 'docs/docs/concepts/architecture.mdx', 'file_path': 'docs/docs/concepts/architecture.mdx', 'file_name': 'architecture.mdx', 'file_type': '.mdx'}, page_content='A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\\n\\n:::important\\nLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.\\n\\nIf you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications.\\n:::\\n\\nFor more information, see the [LangServe documentation](/docs/langserve).\\n\\n\\n## LangSmith\\n\\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\\n\\nFor more information, see the [LangSmith documentation](https://docs.smith.langchain.com)'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='# Async programming with langchain\\n\\n:::info Prerequisites\\n* [Runnable interface](/docs/concepts/runnables)\\n* [asyncio](https://docs.python.org/3/library/asyncio.html)\\n:::\\n\\nLLM based applications often involve a lot of I/O-bound operations, such as making API calls to language models, databases, or other services. Asynchronous programming (or async programming) is a paradigm that allows a program to perform multiple tasks concurrently without blocking the execution of other tasks, improving efficiency and responsiveness, particularly in I/O-bound operations.\\n\\n:::note\\nYou are expected to be familiar with asynchronous programming in Python before reading this guide. If you are not, please find appropriate resources online to learn how to program asynchronously in Python.\\nThis guide specifically focuses on what you need to know to work with LangChain in an asynchronous context, assuming that you are already familiar with asynch\\n:::\\n\\n## Langchain asynchronous APIs'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.\\n\\nTypically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.\\n\\nIn LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an \"a\" prefix. For example, the synchronous `invoke` method has an asynchronous counterpart called `ainvoke`.\\n\\nMany components of LangChain implement the [Runnable Interface](/docs/concepts/runnables), which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the `await` keyword in Python.\\n\\n```python\\nawait some_runnable.ainvoke(some_input)\\n```'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement the [Runnable Interface](/docs/concepts/runnables) usually still follow the same rule and include the asynchronous version of method in the same class with an \"a\" prefix.\\n\\nFor example,\\n\\n```python\\nawait some_vectorstore.aadd_documents(documents)\\n```\\n\\nRunnables created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel) can also be run asynchronously as they implement\\nthe full [Runnable Interface](/docs/concepts/runnables).\\n\\nFor more information, please review the [API reference](https://python.langchain.com/api_reference/) for the specific component you are using.\\n\\n## Delegation to sync methods'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content=\"Most popular LangChain integrations implement asynchronous support of their APIs. For example, the `ainvoke` method of many ChatModel implementations uses the `httpx.AsyncClient` to make asynchronous HTTP requests to the model provider's API.\\n\\nWhen an asynchronous implementation is not available, LangChain tries to provide a default implementation, even if it incurs\\na **slight** overhead.\"), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='By default, LangChain will delegate the execution of unimplemented asynchronous methods to the synchronous counterparts. LangChain almost always assumes that the synchronous method should be treated as a blocking operation and should be run in a separate thread.\\nThis is done using [asyncio.loop.run_in_executor](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) functionality provided by the `asyncio` library. LangChain uses the default executor provided by the `asyncio` library, which lazily initializes a thread pool executor with a default number of threads that is reused in the given event loop. While this strategy incurs a slight overhead due to context switching between threads, it guarantees that every asynchronous method has a default implementation that works out of the box.\\n\\n## Performance'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='Async code in LangChain should generally perform relatively well with minimal overhead out of the box, and is unlikely\\nto be a bottleneck in most applications.\\n\\nThe two main sources of overhead are:\\n\\n1. Cost of context switching between threads when [delegating to synchronous methods](#delegation-to-sync-methods). This can be addressed by providing a native asynchronous implementation.\\n2. In [LCEL](/docs/concepts/lcel) any \"cheap functions\" that appear as part of the chain will be either scheduled as tasks on the event loop (if they are async) or run in a separate thread (if they are sync), rather than just be run inline.\\n\\nThe latency overhead you should expect from these is between tens of microseconds to a few milliseconds.\\n\\nA more common source of performance issues arises from users accidentally blocking the event loop by calling synchronous code in an async context (e.g., calling `invoke` rather than `ainvoke`).\\n\\n## Compatibility'), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content=\"LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curio`.\\n\\nIn Python 3.9 and 3.10, [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) did not\\naccept a `context` parameter. Due to this limitation, LangChain cannot automatically propagate the `RunnableConfig` down the call chain\\nin certain scenarios.\\n\\nIf you are experiencing issues with streaming, callbacks or tracing in async code and are using Python 3.9 or 3.10, this is a likely cause.\\n\\nPlease read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\\n\\n## How to use in ipython and jupyter notebooks\"), Document(metadata={'source': 'docs/docs/concepts/async.mdx', 'file_path': 'docs/docs/concepts/async.mdx', 'file_name': 'async.mdx', 'file_type': '.mdx'}, page_content='As of IPython 7.0, IPython supports asynchronous REPLs. This means that you can use the `await` keyword in the IPython REPL and Jupyter Notebooks without any additional setup. For more information, see the [IPython blog post](https://blog.jupyter.org/ipython-7-0-async-repl-a35ce050f7f7).'), Document(metadata={'source': 'docs/docs/concepts/callbacks.mdx', 'file_path': 'docs/docs/concepts/callbacks.mdx', 'file_name': 'callbacks.mdx', 'file_type': '.mdx'}, page_content='# Callbacks\\n\\n:::note Prerequisites\\n- [Runnable interface](/docs/concepts/runnables)\\n:::\\n\\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\\n\\nYou can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\\n\\n## Callback events'), Document(metadata={'source': 'docs/docs/concepts/callbacks.mdx', 'file_path': 'docs/docs/concepts/callbacks.mdx', 'file_name': 'callbacks.mdx', 'file_type': '.mdx'}, page_content='| Event            | Event Trigger                               | Associated Method     |\\n|------------------|---------------------------------------------|-----------------------|\\n| Chat model start | When a chat model starts                    | `on_chat_model_start` |\\n| LLM start        | When a llm starts                           | `on_llm_start`        |\\n| LLM new token    | When an llm OR chat model emits a new token | `on_llm_new_token`    |\\n| LLM ends         | When an llm OR chat model ends              | `on_llm_end`          |\\n| LLM errors       | When an llm OR chat model errors            | `on_llm_error`        |\\n| Chain start      | When a chain starts running                 | `on_chain_start`      |\\n| Chain end        | When a chain ends                           | `on_chain_end`        |\\n| Chain error      | When a chain errors                         | `on_chain_error`      |\\n| Tool start       | When a tool starts running                  | `on_tool_start`       |\\n| Tool end         | When a tool ends                            | `on_tool_end`         |\\n| Tool error       | When a tool errors                          | `on_tool_error`       |\\n| Agent action     | When an agent takes an action               | `on_agent_action`     |\\n| Agent finish     | When an agent ends                          | `on_agent_finish`     |\\n| Retriever start  | When a retriever starts                     | `on_retriever_start`  |\\n| Retriever end    | When a retriever ends                       | `on_retriever_end`    |\\n| Retriever error  | When a retriever errors                     | `on_retriever_error`  |\\n| Text             | When arbitrary text is run                  | `on_text`             |\\n| Retry            | When a retry event is run                   | `on_retry`            |'), Document(metadata={'source': 'docs/docs/concepts/callbacks.mdx', 'file_path': 'docs/docs/concepts/callbacks.mdx', 'file_name': 'callbacks.mdx', 'file_type': '.mdx'}, page_content='## Callback handlers\\n\\nCallback handlers can either be `sync` or `async`:\\n\\n* Sync callback handlers implement the [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) interface.\\n* Async callback handlers implement the [AsyncCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) interface.\\n\\nDuring run-time LangChain configures an appropriate callback manager (e.g., [CallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManager.html) or [AsyncCallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.AsyncCallbackManager.html) which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\\n\\n## Passing callbacks'), Document(metadata={'source': 'docs/docs/concepts/callbacks.mdx', 'file_path': 'docs/docs/concepts/callbacks.mdx', 'file_name': 'callbacks.mdx', 'file_type': '.mdx'}, page_content='The `callbacks` property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\\n\\n- **Request time callbacks**: Passed at the time of the request in addition to the input data.\\nAvailable on all standard `Runnable` objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, `chain.invoke({\"number\": 25}, {\"callbacks\": [handler]})`.\\n- **Constructor callbacks**: `chain = TheNameOfSomeChain(callbacks=[handler])`. These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are **not** inherited by any children of the object.\\n\\n:::warning\\nConstructor callbacks are scoped only to the object they are defined on. They are **not** inherited by children\\nof the object.\\n:::\\n\\nIf you\\'re creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.'), Document(metadata={'source': 'docs/docs/concepts/callbacks.mdx', 'file_path': 'docs/docs/concepts/callbacks.mdx', 'file_name': 'callbacks.mdx', 'file_type': '.mdx'}, page_content=':::important Async in Python&lt;=3.10\\n\\nAny `RunnableLambda`, a `RunnableGenerator`, or `Tool` that invokes other runnables\\nand is running `async` in python&lt;=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.\\n\\nThis is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.\\n:::\\n\\nFor specifics on how to use callbacks, see the [relevant how-to guides here](/docs/how_to/#callbacks).'), Document(metadata={'source': 'docs/docs/concepts/chat_history.mdx', 'file_path': 'docs/docs/concepts/chat_history.mdx', 'file_name': 'chat_history.mdx', 'file_type': '.mdx'}, page_content='# Chat history\\n\\n:::info Prerequisites\\n\\n- [Messages](/docs/concepts/messages)\\n- [Chat models](/docs/concepts/chat_models)\\n- [Tool calling](/docs/concepts/tool_calling)\\n:::\\n\\nChat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of [messages](/docs/concepts/messages), each of which is associated with a specific [role](/docs/concepts/messages#role), such as \"user\", \"assistant\", \"system\", or \"tool\".\\n\\n## Conversation patterns\\n\\n![Conversation patterns](/img/conversation_patterns.png)\\n\\nMost conversations start with a **system message** that sets the context for the conversation. This is followed by a **user message** containing the user\\'s input, and then an **assistant message** containing the model\\'s response.'), Document(metadata={'source': 'docs/docs/concepts/chat_history.mdx', 'file_path': 'docs/docs/concepts/chat_history.mdx', 'file_name': 'chat_history.mdx', 'file_type': '.mdx'}, page_content='The **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.\\n\\nA full conversation often involves a combination of two patterns of alternating messages:\\n\\n1. The **user** and the **assistant** representing a back-and-forth conversation.\\n2. The **assistant** and **tool messages** representing an [\"agentic\" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.\\n\\n## Managing chat history\\n\\nSince chat models have a maximum limit on input size, it\\'s important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).\\n\\nWhile processing chat history, it\\'s essential to preserve a correct conversation structure. \\n\\nKey guidelines for managing chat history:'), Document(metadata={'source': 'docs/docs/concepts/chat_history.mdx', 'file_path': 'docs/docs/concepts/chat_history.mdx', 'file_name': 'chat_history.mdx', 'file_type': '.mdx'}, page_content='- The conversation should follow one of these structures:\\n    - The first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\\n    - The last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\\n- When using [tool calling](/docs/concepts/tool_calling), a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\\n\\n:::tip\\nUnderstanding correct conversation structure is essential for being able to properly implement\\n[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.\\n:::\\n\\n## Related resources\\n\\n- [How to trim messages](/docs/how_to/trim_messages/)\\n- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='# Chat models\\n\\n## Overview\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.\\n\\nThe newest generation of chat models offer additional capabilities:'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\n* [Structured output](/docs/concepts/structured_outputs): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n* [Multimodality](/docs/concepts/multimodality): The ability to work with data other than text; for example, images, audio, and video.\\n\\n## Features\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=\"* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.\\n* Use either LangChain's [messages](/docs/concepts/messages) format or OpenAI format.\\n* Standard [tool calling API](/docs/concepts/tool_calling): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\n* Standard API for [structuring outputs](/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\\n* Provides support for [async programming](/docs/concepts/async), [efficient batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](/docs/concepts/streaming).\\n* Integration with [LangSmith](https://docs.smith.langchain.com) for monitoring and debugging production-grade applications based on LLMs.\\n* Additional features like standardized [token usage](/docs/concepts/messages/#aimessage), [rate limiting](#rate-limiting), [caching](#caching) and more.\"), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='## Integrations\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\n\\nThese integrations are one of two types:\\n\\n1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.\\n2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).\\n\\nPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=':::note\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\n:::\\n\\n\\n## Interface\\n\\nLangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](/docs/concepts/runnables), chat models support a [standard streaming interface](/docs/concepts/streaming), [async programming](/docs/concepts/async), optimized [batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](/docs/concepts/runnables) for more details.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='Many of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.\\n\\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](#standard-parameters) section for more details.\\n\\n:::note\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.).\\nThese models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the \"LLM\" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.\\n:::\\n\\n### Key methods\\n\\nThe key methods of a chat model are:'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=\"1. **invoke**: The primary method for interacting with a chat model. It takes a list of [messages](/docs/concepts/messages) as input and returns a list of messages as output.\\n2. **stream**: A method that allows you to stream the output of a chat model as it is generated.\\n3. **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\\n4. **bind_tools**: A method that allows you to bind a tool to a chat model for use in the model's execution context.\\n5. **with_structured_output**: A wrapper around the `invoke` method for models that natively support [structured output](/docs/concepts/structured_outputs).\\n\\nOther important methods can be found in the [BaseChatModel API Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html).\\n\\n### Inputs and outputs\"), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\\n\\nLangChain supports two message formats to interact with chat models:\\n\\n1. **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used internally by LangChain.\\n2. **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard parameters\\n\\nMany chat models have standardized parameters that can be used to configure the model:'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='| Parameter      | Description                                                                                                                                                                                                                                                                                                    |\\n|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `model`        | The name or identifier of the specific AI model you want to use (e.g., `\"gpt-3.5-turbo\"` or `\"gpt-4\"`).                                                                                                                                                                                                        |\\n| `temperature`  | Controls the randomness of the model\\'s output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.                                                                                                                            |\\n| `timeout`      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn’t hang indefinitely.                                                                                                                                                               |\\n| `max_tokens`   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.                                                                                                                                                                                           |\\n| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                                                                                                                                              |\\n| `max_retries`  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.                                                                                                                                                                        |\\n| `api_key`      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.                                                                                                                                                                              |\\n| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model\\'s provider and is necessary for directing your requests.                                                                                                                                                          |\\n| `rate_limiter` | An optional [BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits.  See [rate-limiting](#rate-limiting) below for more details. |'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=\"Some important things to note:\\n\\n- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.\\n- Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they're not enforced on models in `langchain-community`.\\n\\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.\\n\\n## Tool calling\\n\\nChat models can call [tools](/docs/concepts/tools) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\\nsee the [tool calling](/docs/concepts/tool_calling) guide for more information.\"), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=\"## Structured outputs\\n\\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\\nuseful for information extraction tasks. Please read more about\\nthe technique in the [structured outputs](/docs/concepts/structured_outputs) guide.\\n\\n## Multimodality\\n\\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).\\n\\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\\n\\n## Context window\\n\\nA chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\"), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\\n\\n## Advanced topics\\n \\n### Rate-limiting\\n\\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\\n\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='You have a few options to deal with rate limits:'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='### Caching\\n\\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\\n\\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\\n\\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.'), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content=\"A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\\n\\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\\n\\nPlease see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.\\n\\n## Related resources\\n\\n* How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).\\n* List of supported chat models: [chat model integrations](/docs/integrations/chat/).\\n\\n### Conceptual guides\"), Document(metadata={'source': 'docs/docs/concepts/chat_models.mdx', 'file_path': 'docs/docs/concepts/chat_models.mdx', 'file_name': 'chat_models.mdx', 'file_type': '.mdx'}, page_content='* [Messages](/docs/concepts/messages)\\n* [Tool calling](/docs/concepts/tool_calling)\\n* [Multimodality](/docs/concepts/multimodality)\\n* [Structured outputs](/docs/concepts/structured_outputs)\\n* [Tokens](/docs/concepts/tokens)'), Document(metadata={'source': 'docs/docs/concepts/document_loaders.mdx', 'file_path': 'docs/docs/concepts/document_loaders.mdx', 'file_name': 'document_loaders.mdx', 'file_type': '.mdx'}, page_content='# Document loaders\\n<span data-heading-keywords=\"document loader,document loaders\"></span>\\n\\n:::info[Prerequisites]\\n\\n* [Document loaders API reference](/docs/how_to/#document-loaders)\\n:::\\n\\nDocument loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\\n\\n## Integrations\\n\\nYou can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).\\n\\n## Interface\\n\\nDocuments loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).\\n\\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.\\n\\nHere\\'s a simple example:\\n\\n```python\\nfrom langchain_community.document_loaders.csv_loader import CSVLoader'), Document(metadata={'source': 'docs/docs/concepts/document_loaders.mdx', 'file_path': 'docs/docs/concepts/document_loaders.mdx', 'file_name': 'document_loaders.mdx', 'file_type': '.mdx'}, page_content='loader = CSVLoader(\\n    ...  # <-- Integration specific parameters here\\n)\\ndata = loader.load()\\n```\\n\\nWhen working with large datasets, you can use the `.lazy_load` method:\\n\\n```python\\nfor document in loader.lazy_load():\\n    print(document)\\n```\\n\\n## Related resources\\n\\nPlease see the following resources for more information:\\n\\n* [How-to guides for document loaders](/docs/how_to/#document-loaders)\\n* [Document API reference](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\\n* [Document loaders integrations](/docs/integrations/document_loaders/)'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content='# Embedding models\\n<span data-heading-keywords=\"embedding,embeddings\"></span>\\n\\n:::info[Prerequisites]\\n\\n* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\\n\\n:::\\n\\n:::info[Note]\\nThis conceptual overview focuses on text-based embedding models.\\n\\nEmbedding models can also be [multimodal](/docs/concepts/multimodality) though such models are not currently supported by LangChain.\\n:::'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content=\"Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.\\nThis is the power of embedding models, which lie at the heart of many retrieval systems.\\nEmbedding models transform human language into a format that machines can understand and compare with speed and accuracy. \\nThese models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning.\\nEmbeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding. \\n\\n## Key concepts\\n\\n![Conceptual Overview](/img/embeddings_concept.png)\\n\\n(1) **Embed text as a vector**: Embeddings transform text into a numerical vector representation.\\n\\n(2) **Measure similarity**: Embedding vectors can be compared using simple mathematical operations.\\n\\n## Embedding \\n\\n### Historical context\"), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content=\"The landscape of embedding models has evolved significantly over the years. \\nA pivotal moment came in 2018 when Google introduced [BERT (Bidirectional Encoder Representations from Transformers)](https://www.nvidia.com/en-us/glossary/bert/). \\nBERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.\\nHowever, BERT wasn't optimized for generating sentence embeddings efficiently. \\nThis limitation spurred the creation of [SBERT (Sentence-BERT)](https://www.sbert.net/examples/training/sts/README.html), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.\\nToday, the embedding model ecosystem is diverse, with numerous providers offering their own implementations. \\nTo navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) [here](https://huggingface.co/blog/mteb) for objective comparisons.\"), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content=\":::info[Further reading]\\n\\n* See the [seminal BERT paper](https://arxiv.org/abs/1810.04805).\\n* See Cameron Wolfe's [excellent review](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) of embedding models.\\n* See the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/blog/mteb) leaderboard for a comprehensive overview of embedding models.\\n\\n:::\\n\\n### Interface\\n\\nLangChain provides a universal interface for working with them, providing standard methods for common operations.\\nThis common interface simplifies interaction with various embedding providers through two central methods:\\n\\n- `embed_documents`: For embedding multiple texts (documents)\\n- `embed_query`: For embedding a single text (query)\"), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content='This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\\nTo illustrate, here\\'s a practical example using LangChain\\'s `.embed_documents` method to embed a list of strings:\\n\\n```python\\nfrom langchain_openai import OpenAIEmbeddings\\nembeddings_model = OpenAIEmbeddings()\\nembeddings = embeddings_model.embed_documents(\\n    [\\n        \"Hi there!\",\\n        \"Oh, hello!\",\\n        \"What\\'s your name?\",\\n        \"My friends call me World\",\\n        \"Hello World!\"\\n    ]\\n)\\nlen(embeddings), len(embeddings[0])\\n(5, 1536)\\n```\\n\\nFor convenience, you can also use the `embed_query` method to embed a single text:\\n\\n```python\\nquery_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")\\n```\\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content='* See the full list of [LangChain embedding model integrations](/docs/integrations/text_embedding/).\\n* See these [how-to guides](/docs/how_to/embed_text) for working with embedding models.\\n\\n:::\\n\\n### Integrations\\n\\nLangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.\\n\\n## Measure similarity'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content='Each embedding is essentially a set of coordinates, often in a high-dimensional space. \\nIn this space, the position of each point (embedding) reflects the meaning of its corresponding text.\\nJust as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. \\nThis allows for intuitive comparisons between different pieces of text.\\nBy reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\\nSome common similarity metrics include:\\n\\n- **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n- **Euclidean Distance**: Measures the straight-line distance between two points.\\n- **Dot Product**: Measures the projection of one vector onto another.'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content='The choice of similarity metric should be chosen based on the model.\\nAs an example, [OpenAI suggests cosine similarity for their embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use), which can be easily implemented:\\n\\n```python\\nimport numpy as np\\n\\ndef cosine_similarity(vec1, vec2):\\n    dot_product = np.dot(vec1, vec2)\\n    norm_vec1 = np.linalg.norm(vec1)\\n    norm_vec2 = np.linalg.norm(vec2)\\n    return dot_product / (norm_vec1 * norm_vec2)\\n\\nsimilarity = cosine_similarity(query_result, document_result)\\nprint(\"Cosine Similarity:\", similarity)\\n```  \\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/embedding_models.mdx', 'file_path': 'docs/docs/concepts/embedding_models.mdx', 'file_name': 'embedding_models.mdx', 'file_type': '.mdx'}, page_content=\"* See Simon Willison’s [nice blog post and video](https://simonwillison.net/2023/Oct/23/embeddings/) on embeddings and similarity metrics.\\n* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.\\n* See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\\n* See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.\\n\\n:::\"), Document(metadata={'source': 'docs/docs/concepts/evaluation.mdx', 'file_path': 'docs/docs/concepts/evaluation.mdx', 'file_name': 'evaluation.mdx', 'file_type': '.mdx'}, page_content='# Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>\\n\\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications.\\n\\n![](/img/langsmith_evaluate.png)\\n\\n[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:\\n\\n- It makes it easier to create and curate datasets via its tracing and annotation features\\n- It provides an evaluation framework that helps you define metrics and run your app against your dataset\\n- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\\n\\nTo learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).'), Document(metadata={'source': 'docs/docs/concepts/example_selectors.mdx', 'file_path': 'docs/docs/concepts/example_selectors.mdx', 'file_name': 'example_selectors.mdx', 'file_type': '.mdx'}, page_content='# Example selectors\\n\\n:::note Prerequisites\\n\\n- [Chat models](/docs/concepts/chat_models/)\\n- [Few-shot prompting](/docs/concepts/few_shot_prompting/)\\n:::\\n\\n## Overview\\n\\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).\\n\\nThis gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.\\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\\n\\n**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.\\n\\n## Related resources\\n\\n* [Example selector how-to guides](/docs/how_to/#example-selectors)'), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content='# Few-shot prompting\\n\\n:::note Prerequisites\\n\\n- [Chat models](/docs/concepts/chat_models/)\\n:::\\n\\n## Overview\\n\\nOne of the most effective ways to improve model performance is to give a model examples of\\nwhat you want it to do. The technique of adding example inputs and expected outputs\\nto a model prompt is known as \"few-shot prompting\". The technique is based on the\\n[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) paper.\\nThere are a few things to think about when doing few-shot prompting:\\n\\n1. How are examples generated?\\n2. How many examples are in each prompt?\\n3. How are examples selected at runtime?\\n4. How are examples formatted in the prompt?\\n\\nHere are the considerations for each.\\n\\n## 1. Generating examples\\n\\nThe first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.'), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content=\"At a high-level, the basic ways to generate examples are:\\n- Manual: a person/people generates examples they think are useful.\\n- Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.\\n- User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).\\n- LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.\"), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content=\"Which approach is best depends on your task. For tasks where a small number of core principles need to be understood really well, it can be valuable hand-craft a few really good examples.\\nFor tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.\\n\\n**Single-turn v.s. multi-turn examples**\\n\\nAnother dimension to think about when generating examples is what the example is actually showing.\\n\\nThe simplest types of examples just have a user input and an expected model output. These are single-turn examples.\"), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content=\"One more complex type of example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.\\nThis is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where it's useful to show common errors and spell out exactly why they're wrong and what should be done instead.\\n\\n## 2. Number of examples\"), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content='Once we have a dataset of examples, we need to think about how many examples should be in each prompt.\\nThe key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.\\nAnd beyond some threshold having too many examples can start to confuse the model.\\nFinding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.\\nAnecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.\\nBut, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.\\n\\n## 3. Selecting examples'), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content=\"Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:\\n- Randomly\\n- By (semantic or keyword-based) similarity of the inputs\\n- Based on some other constraints, like token size\\n\\nLangChain has a number of [`ExampleSelectors`](/docs/concepts/example_selectors) which make it easy to use any of these techniques.\\n\\nGenerally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.\\n\\n## 4. Formatting examples\\n\\nMost state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:\\n- In the system prompt as a string\\n- As their own messages\"), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content='If we insert our examples into the system prompt as a string, we\\'ll need to make sure it\\'s clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language), XML, TypeScript, etc.\\n\\nIf we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign [names](/docs/concepts/messages) to our messages like `\"example_user\"` and `\"example_assistant\"` to make it clear that these messages correspond to different actors than the latest input message.\\n\\n**Formatting tool call examples**'), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content='One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.\\n- Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,\\n- Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,\\n- Some models require that tools are passed into the model if there are any tool calls / ToolMessages in the chat history.'), Document(metadata={'source': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_path': 'docs/docs/concepts/few_shot_prompting.mdx', 'file_name': 'few_shot_prompting.mdx', 'file_type': '.mdx'}, page_content=\"These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.\\nIn these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.\\n\\nYou can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks [here](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).\"), Document(metadata={'source': 'docs/docs/concepts/index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Conceptual guide\\n\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\n\\nWe recommend that you go through at least one of the [Tutorials](/docs/tutorials) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\n\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).\\n\\n## High level\\n\\n- **[Why LangChain?](/docs/concepts/why_langchain)**: Overview of the value that LangChain provides.\\n- **[Architecture](/docs/concepts/architecture)**: How packages are organized in the LangChain ecosystem.\\n\\n## Concepts'), Document(metadata={'source': 'docs/docs/concepts/index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- **[Chat models](/docs/concepts/chat_models)**: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\n- **[Messages](/docs/concepts/messages)**: The unit of communication in chat models, used to represent model input and output.\\n- **[Chat history](/docs/concepts/chat_history)**: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\n- **[Tools](/docs/concepts/tools)**: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\n- **[Tool calling](/docs/concepts/tool_calling)**: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\n- **[Structured output](/docs/concepts/structured_outputs)**: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n- **[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/)**: Information about a conversation that is persisted so that it can be used in future conversations.\\n- **[Multimodality](/docs/concepts/multimodality)**: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\n- **[Runnable interface](/docs/concepts/runnables)**: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\n- **[Streaming](/docs/concepts/streaming)**: LangChain streaming APIs for surfacing results as they are generated.\\n- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.\\n- **[Document loaders](/docs/concepts/document_loaders)**: Load a source as a list of documents.\\n- **[Retrieval](/docs/concepts/retrieval)**: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\n- **[Text splitters](/docs/concepts/text_splitters)**: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\n- **[Embedding models](/docs/concepts/embedding_models)**: Models that represent data such as text or images in a vector space.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Storage of and efficient search over vectors and associated metadata.\\n- **[Retriever](/docs/concepts/retrievers)**: A component that returns relevant documents from a knowledge base in response to a query.\\n- **[Retrieval Augmented Generation (RAG)](/docs/concepts/rag)**: A technique that enhances language models by combining them with external knowledge bases.\\n- **[Agents](/docs/concepts/agents)**: Use a [language model](/docs/concepts/chat_models) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools).\\n- **[Prompt templates](/docs/concepts/prompt_templates)**: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\n- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling) and [structured outputs](/docs/concepts/structured_outputs).\\n- **[Few-shot prompting](/docs/concepts/few_shot_prompting)**: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\n- **[Example selectors](/docs/concepts/example_selectors)**: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\n- **[Async programming](/docs/concepts/async)**: The basics that one should know to use LangChain in an asynchronous context.\\n- **[Callbacks](/docs/concepts/callbacks)**: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\n- **[Tracing](/docs/concepts/tracing)**: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\n- **[Evaluation](/docs/concepts/evaluation)**: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n- **[Testing](/docs/concepts/testing)**: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.'), Document(metadata={'source': 'docs/docs/concepts/index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='## Glossary'), Document(metadata={'source': 'docs/docs/concepts/index.mdx', 'file_path': 'docs/docs/concepts/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"- **[AIMessageChunk](/docs/concepts/messages#aimessagechunk)**: A partial response from an AI message. Used when streaming responses from a chat model.\\n- **[AIMessage](/docs/concepts/messages#aimessage)**: Represents a complete response from an AI model.\\n- **[astream_events](/docs/concepts/chat_models#key-methods)**: Stream granular information from [LCEL](/docs/concepts/lcel) chains.\\n- **[BaseTool](/docs/concepts/tools/#tool-interface)**: The base class for all tools in LangChain.\\n- **[batch](/docs/concepts/runnables)**: Use to execute a runnable with batch inputs.\\n- **[bind_tools](/docs/concepts/tool_calling/#tool-binding)**: Allows models to interact with tools.\\n- **[Caching](/docs/concepts/chat_models#caching)**: Storing results to avoid redundant calls to a chat model.\\n- **[Chat models](/docs/concepts/multimodality/#multimodality-in-chat-models)**: Chat models that handle multiple data modalities.\\n- **[Configurable runnables](/docs/concepts/runnables/#configurable-runnables)**: Creating configurable Runnables.\\n- **[Context window](/docs/concepts/chat_models#context-window)**: The maximum size of input a chat model can process.\\n- **[Conversation patterns](/docs/concepts/chat_history#conversation-patterns)**: Common patterns in chat interactions.\\n- **[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)**: LangChain's representation of a document.\\n- **[Embedding models](/docs/concepts/multimodality/#multimodality-in-embedding-models)**: Models that generate vector embeddings for various data types.\\n- **[HumanMessage](/docs/concepts/messages#humanmessage)**: Represents a message from a human user.\\n- **[InjectedState](/docs/concepts/tools#injectedstate)**: A state injected into a tool function.\\n- **[InjectedStore](/docs/concepts/tools#injectedstore)**: A store that can be injected into a tool for data persistence.\\n- **[InjectedToolArg](/docs/concepts/tools#injectedtoolarg)**: Mechanism to inject arguments into tool functions.\\n- **[input and output types](/docs/concepts/runnables#input-and-output-types)**: Types used for input and output in Runnables.\\n- **[Integration packages](/docs/concepts/architecture/#integration-packages)**: Third-party packages that integrate with LangChain.\\n- **[Integration tests](/docs/concepts/testing#integration-tests)**: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.\\n- **[invoke](/docs/concepts/runnables)**: A standard method to invoke a Runnable.\\n- **[JSON mode](/docs/concepts/structured_outputs#json-mode)**: Returning responses in JSON format.\\n- **[langchain-community](/docs/concepts/architecture#langchain-community)**: Community-driven components for LangChain.\\n- **[langchain-core](/docs/concepts/architecture#langchain-core)**: Core langchain package. Includes base interfaces and in-memory implementations.\\n- **[langchain](/docs/concepts/architecture#langchain)**: A package for higher level components (e.g., some pre-built chains).\\n- **[langgraph](/docs/concepts/architecture#langgraph)**: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\n- **[langserve](/docs/concepts/architecture#langserve)**: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\n- **[LLMs (legacy)](/docs/concepts/text_llms)**: Older language models that take a string as input and return a string as output.\\n- **[Managing chat history](/docs/concepts/chat_history#managing-chat-history)**: Techniques to maintain and manage the chat history.\\n- **[OpenAI format](/docs/concepts/messages#openai-format)**: OpenAI's message format for chat models.\\n- **[Propagation of RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig)**: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\n- **[rate-limiting](/docs/concepts/chat_models#rate-limiting)**: Client side rate limiting for chat models.\\n- **[RemoveMessage](/docs/concepts/messages/#removemessage)**: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\n- **[role](/docs/concepts/messages#role)**: Represents the role (e.g., user, assistant) of a chat message.\\n- **[RunnableConfig](/docs/concepts/runnables/#runnableconfig)**: Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).\\n- **[Standard parameters for chat models](/docs/concepts/chat_models#standard-parameters)**: Parameters such as API key, `temperature`, and `max_tokens`.\\n- **[Standard tests](/docs/concepts/testing#standard-tests)**: A defined set of unit and integration tests that all integrations must pass.\\n- **[stream](/docs/concepts/streaming)**: Use to stream output from a Runnable or a graph.\\n- **[Tokenization](/docs/concepts/tokens)**: The process of converting data into tokens and vice versa.\\n- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.\\n- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\n- **[Tool binding](/docs/concepts/tool_calling#tool-binding)**: Binding tools to models.\\n- **[@tool](/docs/concepts/tools/#create-tools-using-the-tool-decorator)**: Decorator for creating tools in LangChain.\\n- **[Toolkits](/docs/concepts/tools#toolkits)**: A collection of tools that can be used together.\\n- **[ToolMessage](/docs/concepts/messages#toolmessage)**: Represents a message that contains the results of a tool execution.\\n- **[Unit tests](/docs/concepts/testing#unit-tests)**: Tests that verify the correctness of individual components, run in isolation without access to the Internet.\\n- **[Vector stores](/docs/concepts/vectorstores)**: Datastores specialized for storing and efficiently searching vector embeddings.\\n- **[with_structured_output](/docs/concepts/structured_outputs/#structured-output-method)**: A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\n- **[with_types](/docs/concepts/runnables#with_types)**: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\"), Document(metadata={'source': 'docs/docs/concepts/key_value_stores.mdx', 'file_path': 'docs/docs/concepts/key_value_stores.mdx', 'file_name': 'key_value_stores.mdx', 'file_type': '.mdx'}, page_content='# Key-value stores\\n\\n## Overview\\n\\nLangChain provides a key-value store interface for storing and retrieving data.\\n\\nLangChain includes a [`BaseStore`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\\nencoding and decoding data for their specific needs.\\n\\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\\n\\n## Usage\\n\\nThe key-value store interface in LangChain is used primarily for:'), Document(metadata={'source': 'docs/docs/concepts/key_value_stores.mdx', 'file_path': 'docs/docs/concepts/key_value_stores.mdx', 'file_name': 'key_value_stores.mdx', 'file_type': '.mdx'}, page_content='1. Caching [embeddings](/docs/concepts/embedding_models) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\\n\\n2. As a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\\n\\nPlease see these how-to guides for more information:\\n\\n* [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\\n* [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\\n\\n## Interface\\n\\nAll [`BaseStores`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:'), Document(metadata={'source': 'docs/docs/concepts/key_value_stores.mdx', 'file_path': 'docs/docs/concepts/key_value_stores.mdx', 'file_name': 'key_value_stores.mdx', 'file_type': '.mdx'}, page_content='- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\\n\\n## Integrations\\n\\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content=':::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='Other benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content=\"While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\"), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content=\"* If you are making a single LLM call, you don't need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.\\n* If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\"), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='The usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content='#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```'), Document(metadata={'source': 'docs/docs/concepts/lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_name': 'lcel.mdx', 'file_type': '.mdx'}, page_content=':::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='# Messages\\n\\n:::info Prerequisites\\n- [Chat Models](/docs/concepts/chat_models)\\n:::\\n\\n## Overview\\n\\nMessages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\\n\\nEach message has a **role** (e.g., \"user\", \"assistant\") and **content** (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\\n\\nLangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\n\\n## What is inside a message?\\n\\nA message typically consists of the following pieces of information:'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='- **Role**: The role of the message (e.g., \"user\", \"assistant\").\\n- **Content**: The content of the message (e.g., text, multimodal data).\\n- Additional metadata: id, name, [token usage](/docs/concepts/tokens) and other model-specific metadata.\\n\\n### Role\\n\\nRoles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content=\"| **Role**              | **Description**                                                                                                                                                                                                 |\\n|-----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| **system**            | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                                                                            |\\n| **user**              | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                                                                                |\\n| **assistant**         | Represents a response from the model, which can include text or a request to invoke tools.                                                                                                                      |\\n| **tool**              | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support [tool calling](/docs/concepts/tool_calling). |\\n| **function** (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. **tool** role should be used instead.                                                                                             |\"), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='### Content\\n\\nThe content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\\n\\nCurrently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\\n\\nFor more information see:\\n* [SystemMessage](#systemmessage) -- for content which should be passed to direct the conversation\\n* [HumanMessage](#humanmessage) -- for content in the input from the user.\\n* [AIMessage](#aimessage) -- for content in the response from the model.\\n* [Multimodality](/docs/concepts/multimodality) -- for more information on multimodal content.\\n\\n### Other Message Data\\n\\nDepending on the chat model provider, messages can include other data such as:'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='- **ID**: An optional unique identifier for the message.\\n- **Name**: An optional `name` property which allows differentiate between different entities/speakers with the same role. Not all models support this!\\n- **Metadata**: Additional information about the message, such as timestamps, token usage, etc.\\n- **Tool Calls**: A request made by the model to call one or more tools> See [tool calling](/docs/concepts/tool_calling) for more information.\\n\\n## Conversation Structure\\n\\nThe sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\\n\\nFor example, a typical conversation structure might look like this:\\n\\n1. **User Message**: \"Hello, how are you?\"\\n2. **Assistant Message**: \"I\\'m doing well, thank you for asking.\"\\n3. **User Message**: \"Can you tell me a joke?\"\\n4. **Assistant Message**: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='Please read the [chat history](/docs/concepts/chat_history) guide for more information on managing chat history and ensuring that the conversation structure is correct.\\n\\n## LangChain Messages\\n\\nLangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\n\\nLangChain messages are Python objects that subclass from a [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html).\\n\\nThe five main message types are:'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content=\"- [SystemMessage](#systemmessage): corresponds to **system** role\\n- [HumanMessage](#humanmessage): corresponds to **user** role\\n- [AIMessage](#aimessage): corresponds to **assistant** role\\n- [AIMessageChunk](#aimessagechunk): corresponds to **assistant** role, used for [streaming](/docs/concepts/streaming) responses\\n- [ToolMessage](#toolmessage): corresponds to **tool** role\\n\\nOther important messages include:\\n\\n- [RemoveMessage](#removemessage) -- does not correspond to any role. This is an abstraction, mostly used in [LangGraph](/docs/concepts/architecture#langgraph) to manage chat history.\\n- **Legacy** [FunctionMessage](#legacy-functionmessage): corresponds to the **function** role in OpenAI's **legacy** function-calling API.\\n\\nYou can find more information about **messages** in the [API Reference](https://python.langchain.com/api_reference/core/messages.html).\\n\\n### SystemMessage\"), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='A `SystemMessage` is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\\n\\nDifferent chat providers may support system message in one of the following ways:\\n\\n* **Through a \"system\" message role**: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\\n* **Through a separate API parameter for system instructions**: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\\n* **No support for system messages**: Some models do not support system messages at all.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\\n\\nIf no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message\\'s content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the `langchain-community` package) it is recommended to check the specific documentation for that model.\\n\\n### HumanMessage\\n\\nThe `HumanMessage` corresponds to the **\"user\"** role. A human message represents input from a user interacting with the model.\\n\\n#### Text Content'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='Most chat models expect the user input to be in the form of text.\\n\\n```python\\nfrom langchain_core.messages import HumanMessage\\n\\nmodel.invoke([HumanMessage(content=\"Hello, how are you?\")])\\n```\\n\\n:::tip\\nWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a `HumanMessage` object. This is mostly useful for quick testing.\\n\\n```python\\nmodel.invoke(\"Hello, how are you?\")\\n```\\n:::\\n\\n#### Multi-modal Content\\n\\nSome chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\\n\\nPlease see the [multimodality](/docs/concepts/multimodality) guide for more information.\\n\\n### AIMessage\\n\\n`AIMessage` is used to represent a message with the role **\"assistant\"**. This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_core.messages import HumanMessage\\nai_message = model.invoke([HumanMessage(\"Tell me a joke\")])\\nai_message # <-- AIMessage\\n```\\n\\nAn `AIMessage` has the following attributes. The attributes which are **standardized** are the ones that LangChain attempts to standardize across different chat model providers. **raw** fields are specific to the model provider and may vary.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='| Attribute            | Standardized/Raw | Description                                                                                                                                                                                                             |\\n|----------------------|:-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `content`            | Raw              | Usually a string, but can be a list of content blocks. See [content](#content) for details.                                                                                                                             |\\n| `tool_calls`         | Standardized     | Tool calls associated with the message. See [tool calling](/docs/concepts/tool_calling) for details.                                                                                                                    |\\n| `invalid_tool_calls` | Standardized     | Tool calls with parsing errors associated with the message. See [tool calling](/docs/concepts/tool_calling) for details.                                                                                                |\\n| `usage_metadata`     | Standardized     | Usage metadata for a message, such as [token counts](/docs/concepts/tokens). See [Usage Metadata API Reference](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.UsageMetadata.html). |\\n| `id`                 | Standardized     | An optional unique identifier for the message, ideally provided by the provider/model that created the message.                                                                                                         |\\n| `response_metadata`  | Raw              | Response metadata, e.g., response headers, logprobs, token counts.                                                                                                                                                      |'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='#### content\\n\\nThe **content** property of an `AIMessage` represents the response generated by the chat model.\\n\\nThe content is either:\\n\\n- **text** -- the norm for virtually all chat models.\\n- A **list of dictionaries** -- Each dictionary represents a content block and is associated with a `type`.\\n    * Used by Anthropic for surfacing agent thought process when doing [tool calling](/docs/concepts/tool_calling).\\n    * Used by OpenAI for audio outputs. Please see [multi-modal content](/docs/concepts/multimodality) for more information.\\n\\n:::important\\nThe **content** property is **not** standardized across different chat model providers, mostly because there are\\nstill few examples to generalize from.\\n:::\\n\\n### AIMessageChunk\\n\\nIt is common to [stream](/docs/concepts/streaming) responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='It is returned from the `stream`, `astream` and `astream_events` methods of the chat model.\\n\\nFor example,\\n\\n```python\\nfor chunk in model.stream([HumanMessage(\"what color is the sky?\")]):\\n    print(chunk)\\n```\\n\\n`AIMessageChunk` follows nearly the same structure as `AIMessage`, but uses a different [ToolCallChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCallChunk.html#langchain_core.messages.tool.ToolCallChunk)\\nto be able to stream tool calling in a standardized manner.\\n\\n\\n#### Aggregating\\n\\n`AIMessageChunks` support the `+` operator to merge them into a single `AIMessage`. This is useful when you want to display the final response to the user.\\n\\n```python\\nai_message = chunk1 + chunk2 + chunk3 + ...\\n```\\n\\n### ToolMessage\\n\\nThis represents a message with role \"tool\", which contains the result of [calling a tool](/docs/concepts/tool_calling). In addition to `role` and `content`, this message has:'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='- a `tool_call_id` field which conveys the id of the call to the tool that was called to produce this result.\\n- an `artifact` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\\n\\nPlease see [tool calling](/docs/concepts/tool_calling) for more information.\\n\\n### RemoveMessage\\n\\nThis is a special message type that does not correspond to any roles. It is used\\nfor managing chat history in [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nPlease see the following for more information on how to use the `RemoveMessage`:\\n\\n* [Memory conceptual guide](https://langchain-ai.github.io/langgraph/concepts/memory/)\\n* [How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/)\\n\\n### (Legacy) FunctionMessage'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='This is a legacy message type, corresponding to OpenAI\\'s legacy function-calling API. `ToolMessage` should be used instead to correspond to the updated tool-calling API.\\n\\n## OpenAI Format\\n\\n### Inputs\\n\\nChat models also accept OpenAI\\'s format as **inputs** to chat models:\\n\\n```python\\nchat_model.invoke([\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"Hello, how are you?\",\\n    },\\n    {\\n        \"role\": \"assistant\",\\n        \"content\": \"I\\'m doing well, thank you for asking.\",\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"Can you tell me a joke?\",\\n    }\\n])\\n```\\n\\n### Outputs\\n\\nAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you\\nneed OpenAI format for the output as well.'), Document(metadata={'source': 'docs/docs/concepts/messages.mdx', 'file_path': 'docs/docs/concepts/messages.mdx', 'file_name': 'messages.mdx', 'file_type': '.mdx'}, page_content='The [convert_to_openai_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html) utility function can be used to convert from LangChain messages to OpenAI format.'), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content='# Multimodality\\n\\n## Overview\\n\\n**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.\\n\\n- **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.\\n- **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of data—such as text, images, and audio—into vector spaces.\\n- **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.\\n\\n## Multimodality in chat models'), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content=':::info Pre-requisites\\n* [Chat models](/docs/concepts/chat_models)\\n* [Messages](/docs/concepts/messages)\\n:::\\n \\nMultimodal support is still relatively new and less common, model providers have not yet standardized on the \"best\" way to define the API. As such, LangChain\\'s multimodal abstractions are lightweight and flexible, designed to accommodate different model providers\\' APIs and interaction patterns, but are **not** standardized across models.\\n\\n### How to use multimodal models\\n\\n* Use the [chat model integration table](/docs/integrations/chat/) to identify which models support multimodality.\\n* Reference the [relevant how-to guides](/docs/how_to/#multimodal) for specific examples of how to use multimodal models.\\n\\n### What kind of multimodality is supported?\\n\\n#### Inputs'), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content=\"Some models can accept multimodal inputs, such as images, audio, video, or files. The types of multimodal inputs supported depend on the model provider. For instance, [Google's Gemini](/docs/integrations/chat/google_generative_ai/) supports documents like PDFs as inputs.\\n\\nMost chat models that support **multimodal inputs** also accept those values in OpenAI's content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\\n\\nThe gist of passing multimodal inputs to a chat model is to use content blocks that specify a type and corresponding data. For example, to pass an image to a chat model:\\n\\n```python\\nfrom langchain_core.messages import HumanMessage\"), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content='message = HumanMessage(\\n    content=[\\n        {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\\n        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\\n    ],\\n)\\nresponse = model.invoke([message])\\n```\\n\\n:::caution\\nThe exact format of the content blocks may vary depending on the model provider. Please refer to the chat model\\'s\\nintegration documentation for the correct format. Find the integration in the [chat model integration table](/docs/integrations/chat/).\\n:::\\n\\n#### Outputs\\n\\nVirtually no popular chat models support multimodal outputs at the time of writing (October 2024). \\n\\nThe only exception is OpenAI\\'s chat model ([gpt-4o-audio-preview](/docs/integrations/chat/openai/)), which can generate audio outputs.\\n\\nMultimodal outputs will appear as part of the [AIMessage](/docs/concepts/messages/#aimessage) response object.\\n\\nPlease see the [ChatOpenAI](/docs/integrations/chat/openai/) for more information on how to use multimodal outputs.\\n\\n#### Tools'), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content='Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.\\n\\nHowever, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of [tool calling](/docs/concepts/tool_calling) can be equipped with tools to download and process images, audio, or video.\\n\\n## Multimodality in embedding models\\n\\n:::info Prerequisites\\n* [Embedding Models](/docs/concepts/embedding_models)\\n:::\\n\\n**Embeddings** are vector representations of data used for tasks like similarity search and retrieval.'), Document(metadata={'source': 'docs/docs/concepts/multimodality.mdx', 'file_path': 'docs/docs/concepts/multimodality.mdx', 'file_name': 'multimodality.mdx', 'file_type': '.mdx'}, page_content='The current [embedding interface](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.embeddings.Embeddings.html#langchain_core.embeddings.embeddings.Embeddings) used in LangChain is optimized entirely for text-based data, and will **not** work with multimodal data.\\n\\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.\\n\\n## Multimodality in vector stores\\n\\n:::info Prerequisites\\n* [Vector stores](/docs/concepts/vectorstores)\\n:::\\n\\nVector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.\\n\\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.'), Document(metadata={'source': 'docs/docs/concepts/output_parsers.mdx', 'file_path': 'docs/docs/concepts/output_parsers.mdx', 'file_name': 'output_parsers.mdx', 'file_type': '.mdx'}, page_content='# Output parsers\\n\\n<span data-heading-keywords=\"output parser\"></span>\\n\\n:::note\\n\\nThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that [here](/docs/concepts/tool_calling).\\n\\n:::\\n\\n`Output parser` is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\\n\\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:'), Document(metadata={'source': 'docs/docs/concepts/output_parsers.mdx', 'file_path': 'docs/docs/concepts/output_parsers.mdx', 'file_name': 'output_parsers.mdx', 'file_type': '.mdx'}, page_content='- **Name**: The name of the output parser\\n- **Supports Streaming**: Whether the output parser supports streaming.\\n- **Has Format Instructions**: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\\n- **Calls LLM**: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\\n- **Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\\n- **Output Type**: The output type of the object returned by the parser.\\n- **Description**: Our commentary on this output parser and when to use it.'), Document(metadata={'source': 'docs/docs/concepts/output_parsers.mdx', 'file_path': 'docs/docs/concepts/output_parsers.mdx', 'file_name': 'output_parsers.mdx', 'file_type': '.mdx'}, page_content=\"| Name                                                                                                                                                                                                                                    | Supports Streaming | Has Format Instructions | Calls LLM | Input Type         | Output Type          | Description                                                                                                                                                                                                                                              |\\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|-------------------------|-----------|--------------------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Str](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)                                                                                                         | ✅                  |                         |           | `str` \\\\| `Message` | String                | Parses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).                                                                                                                |\\n| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | ✅                  | ✅                       |           | `str` \\\\| `Message` | JSON object          | Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                    |\\n| [XML](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser)                                                          | ✅                  | ✅                       |           | `str` \\\\| `Message` | `dict`               | Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).                                                                                                                            |\\n| [CSV](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser)                          | ✅                  | ✅                       |           | `str` \\\\| `Message` | `List[str]`          | Returns a list of comma separated values.                                                                                                                                                                                                                |\\n| [OutputFixing](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser)                                                |                    |                         | ✅         | `str` \\\\| `Message` |                      | Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.                                                                                              |\\n| [RetryWithError](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser)                          |                    |                         | ✅         | `str` \\\\| `Message` |                      | Wraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions. |\\n| [Pydantic](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser)                                 |                    | ✅                       |           | `str` \\\\| `Message` | `pydantic.BaseModel` | Takes a user defined Pydantic model and returns data in that format.                                                                                                                                                                                     |\\n| [YAML](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser)                                                          |                    | ✅                       |           | `str` \\\\| `Message` | `pydantic.BaseModel` | Takes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.                                                                                                                                                             |\\n| [PandasDataFrame](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser) |                    | ✅                       |           | `str` \\\\| `Message` | `dict`               | Useful for doing operations with pandas DataFrames.                                                                                                                                                                                                      |\\n| [Enum](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)                                                          |                    | ✅                       |           | `str` \\\\| `Message` | `Enum`               | Parses response into one of the provided enum values.                                                                                                                                                                                                    |\\n| [Datetime](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)                                      |                    | ✅                       |           | `str` \\\\| `Message` | `datetime.datetime`  | Parses response into a datetime string.                                                                                                                                                                                                                  |\\n| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)                            |                    | ✅                       |           | `str` \\\\| `Message` | `Dict[str, str]`     | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.                                            |\"), Document(metadata={'source': 'docs/docs/concepts/output_parsers.mdx', 'file_path': 'docs/docs/concepts/output_parsers.mdx', 'file_name': 'output_parsers.mdx', 'file_type': '.mdx'}, page_content='For specifics on how to use output parsers, see the [relevant how-to guides here](/docs/how_to/#output-parsers).'), Document(metadata={'source': 'docs/docs/concepts/prompt_templates.mdx', 'file_path': 'docs/docs/concepts/prompt_templates.mdx', 'file_name': 'prompt_templates.mdx', 'file_type': '.mdx'}, page_content=\"# Prompt Templates\\n\\nPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\\n\\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\\n\\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\\n\\nThere are a few different types of prompt templates:\\n\\n## String PromptTemplates\\n\\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:\\n\\n```python\\nfrom langchain_core.prompts import PromptTemplate\"), Document(metadata={'source': 'docs/docs/concepts/prompt_templates.mdx', 'file_path': 'docs/docs/concepts/prompt_templates.mdx', 'file_name': 'prompt_templates.mdx', 'file_type': '.mdx'}, page_content='prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\nprompt_template.invoke({\"topic\": \"cats\"})\\n```\\n\\n## ChatPromptTemplates\\n\\nThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\\n\\n```python\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\nprompt_template = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful assistant\"),\\n    (\"user\", \"Tell me a joke about {topic}\")\\n])\\n\\nprompt_template.invoke({\"topic\": \"cats\"})\\n```\\n\\nIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the `topic` variable the user passes in.\\n\\n## MessagesPlaceholder\\n<span data-heading-keywords=\"messagesplaceholder\"></span>'), Document(metadata={'source': 'docs/docs/concepts/prompt_templates.mdx', 'file_path': 'docs/docs/concepts/prompt_templates.mdx', 'file_name': 'prompt_templates.mdx', 'file_type': '.mdx'}, page_content='This prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.\\n\\n```python\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\nfrom langchain_core.messages import HumanMessage\\n\\nprompt_template = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful assistant\"),\\n    MessagesPlaceholder(\"msgs\")\\n])\\n\\nprompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\\n```\\n\\nThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\nThis is useful for letting a list of messages be slotted into a particular spot.'), Document(metadata={'source': 'docs/docs/concepts/prompt_templates.mdx', 'file_path': 'docs/docs/concepts/prompt_templates.mdx', 'file_name': 'prompt_templates.mdx', 'file_type': '.mdx'}, page_content='An alternative way to accomplish the same thing without using the `MessagesPlaceholder` class explicitly is:\\n\\n```python\\nprompt_template = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful assistant\"),\\n    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\\n])\\n```\\n\\nFor specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).'), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content=\"# Retrieval augmented generation (RAG)\\n\\n:::info[Prerequisites]\\n\\n* [Retrieval](/docs/concepts/retrieval/)\\n\\n:::\\n\\n## Overview\\n\\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases. \\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\\nWhen given a query, RAG systems first search a knowledge base for relevant information.\\nThe system then incorporates this retrieved information into the model's prompt.\\nThe model uses the provided context to generate a response to the query.\\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\\n\\n## Key concepts\\n\\n![Conceptual Overview](/img/rag_concepts.png)\"), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content=\"(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\\n\\n(2) **Adding external knowledge**: Pass retrieved information to a model.\\n\\n## Retrieval system\\n\\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\\nUsing a retrieval system offers several advantages:\"), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content='- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\\n\\n:::info[Further reading]\\n\\nSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\\n\\n:::\\n\\n## Adding external knowledge\\n\\nWith a retrieval system in place, we need to pass knowledge from this system to the model. \\nA RAG pipeline typically achieves this following these steps:\\n\\n- Receive an input query.\\n- Use the retrieval system to search for relevant information based on the query.\\n- Incorporate the retrieved information into the prompt sent to the LLM.\\n- Generate a response that leverages the retrieved context.'), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content='As an example, here\\'s a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.messages import SystemMessage, HumanMessage\\n\\n# Define a system prompt that tells the model how to use the retrieved context\\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don\\'t know the answer, just say that you don\\'t know. \\nUse three sentences maximum and keep the answer concise.\\nContext: {context}:\"\"\"\\n    \\n# Define a question\\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\\n\\n# Retrieve relevant documents\\ndocs = retriever.invoke(question)\\n\\n# Combine the documents into a single string\\ndocs_text = \"\".join(d.page_content for d in docs)'), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content='# Populate the system prompt with the retrieved context\\nsystem_prompt_fmt = system_prompt.format(context=docs_text)\\n\\n# Create a model\\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) \\n\\n# Generate a response\\nquestions = model.invoke([SystemMessage(content=system_prompt_fmt),\\n                          HumanMessage(content=question)])\\n```\\n\\n:::info[Further reading]\\n\\nRAG a deep area with many possible optimization and design choices:'), Document(metadata={'source': 'docs/docs/concepts/rag.mdx', 'file_path': 'docs/docs/concepts/rag.mdx', 'file_name': 'rag.mdx', 'file_type': '.mdx'}, page_content='* See [this excellent blog](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval?utm_source=profile&utm_medium=reader2) from Cameron Wolfe for a comprehensive overview and history of RAG.\\n* See our [RAG how-to guides](/docs/how_to/#qa-with-rag).\\n* See our RAG [tutorials](/docs/tutorials/).\\n* See our RAG from Scratch course, with [code](https://github.com/langchain-ai/rag-from-scratch) and [video playlist](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x).\\n* Also, see our RAG from Scratch course [on Freecodecamp](https://youtu.be/sVcwVQRHIc8?feature=shared).\\n\\n:::'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content=\"# Retrieval\\n\\n:::info[Prerequisites]\\n\\n* [Retrievers](/docs/concepts/retrievers/)\\n* [Vector stores](/docs/concepts/vectorstores/)\\n* [Embeddings](/docs/concepts/embedding_models/)\\n* [Text splitters](/docs/concepts/text_splitters/)\\n\\n:::\\n\\n:::danger[Security]\\n \\nSome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\\nThere are inherent risks in doing this. \\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs. \\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases. \\nFor more on general security best practices, see our [security guide](/docs/security/).\\n\\n:::\\n\\n## Overview \\n\\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets. \\nThese systems accommodate various data formats:\"), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\\n- Structured data is typically housed in relational or graph databases with defined schemas.\\n\\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces. \\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database. \\nThis translation enables more intuitive and flexible interactions with complex data structures.\\n\\n## Key concepts \\n\\n![Retrieval](/img/retrieval_concept.png)\\n\\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\\n\\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\\n\\n## Query analysis'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='While users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords. \\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\\n\\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input. \\n\\n### Query re-writing'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='Retrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions. \\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries. \\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\\n\\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\\n\\nVarious techniques have been developed to leverage models for query re-writing, including:'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='| Name                                                                                                      | When to use                                                                                     | Description                                                                                                                                                                                                                                                                            |\\n|-----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a question. | Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.                                                                                                                                        |\\n| [Decomposition](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | When a question can be broken down into smaller subproblems.                                    | Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).                                                           |\\n| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                       | First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. [Paper](https://arxiv.org/pdf/2310.06117).                                            |\\n| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | If you have challenges retrieving relevant documents using the raw user inputs.                 | Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. [Paper](https://arxiv.org/abs/2212.10496). |'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='As an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.\\nThese can then be run sequentially or in parallel on a downstream retrieval system.\\n\\n```python\\nfrom typing import List\\n\\nfrom pydantic import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.messages import SystemMessage, HumanMessage\\n\\n# Define a pydantic model to enforce the output structure\\nclass Questions(BaseModel):\\n    questions: List[str] = Field(\\n        description=\"A list of sub-questions related to the input query.\"\\n    )\\n\\n# Create an instance of the model and enforce the output structure\\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) \\nstructured_model = model.with_structured_output(Questions)'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='# Define the system prompt\\nsystem = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\\\n\\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\\\n\"\"\"\\n\\n# Pass the question to the model\\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\\nquestions = structured_model.invoke([SystemMessage(content=system)]+[HumanMessage(content=question)])\\n```\\n\\n:::tip\\n\\nSee our RAG from Scratch videos for a few different specific approaches:\\n- [Multi-query](https://youtu.be/JChPi0CRnDY?feature=shared)\\n- [Decomposition](https://youtu.be/h0OPWlEOank?feature=shared)\\n- [Step-back](https://youtu.be/xn1jEjRyJ2U?feature=shared)\\n- [HyDE](https://youtu.be/SaDzIVkYqyY?feature=shared)\\n\\n:::\\n\\n### Query construction'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='Query analysis also can focus on translating natural language queries into specialized query languages or filters. \\nThis translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.\\n\\n1. **Structured Data examples**: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.\\n   - **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\\n   - **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\\n\\n2. **Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\\n   - **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='These approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='| Name                                     | When to Use                                                                                                                          | Description                                                                                                                                                                                                                                          |\\n|------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Self Query](/docs/how_to/self_query/)   | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\\n| [Text to SQL](/docs/tutorials/sql_qa/)   | If users are asking questions that require information housed in a relational database, accessible via SQL.                          | This uses an LLM to transform user input into a SQL query.                                                                                                                                                                                           |\\n| [Text-to-Cypher](/docs/tutorials/graph/) | If users are asking questions that require information housed in a graph database, accessible via Cypher.                            | This uses an LLM to transform user input into a Cypher query.                                                                                                                                                                                        |'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='As an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.  \\n\\n```python\\nmetadata_field_info = schema_for_metadata \\ndocument_content_description = \"Brief summary of a movie\"\\nllm = ChatOpenAI(temperature=0)\\nretriever = SelfQueryRetriever.from_llm(\\n    llm,\\n    vectorstore,\\n    document_content_description,\\n    metadata_field_info,\\n)\\n```\\n\\n:::info[Further reading]\\n\\n* See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\\n* See our [blog post overview](https://blog.langchain.dev/query-construction/).\\n* See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\\n\\n::: \\n\\n## Information retrieval \\n\\n### Common retrieval systems\\n\\n#### Lexical search indexes'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='Many search engines are based upon matching words in a query to the words in each document. \\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\\nThe intution is simple: a word appears frequently both in the user’s query and a particular document, then this document might be a good match.'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='The particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).\\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents. \\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\\n\\n:::info[Further reading]\\n\\n* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\\n* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\\n\\n::: \\n\\n#### Vector indexes'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content=\"Vector indexes are an alternative way to index and store unstructured data.\\nSee our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview.  \\nIn short, rather than using word frequencies, vectorstores use an [embedding model](/docs/concepts/embedding_models/) to compress documents into high-dimensional vector representation. \\nThis allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\\n\\n:::info[Further reading]\\n\\n* See our [how-to guide](/docs/how_to/vectorstore_retriever/) for more details on working with vectorstores.\\n* See our [list of vectorstore integrations](/docs/integrations/vectorstores/).\\n* See Cameron Wolfe's [blog post](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) on the basics of vector search.\\n\\n:::\\n\\n#### Relational databases\"), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='Relational databases are a fundamental type of structured data storage used in many applications. \\nThey organize data into tables with predefined schemas, where each table represents an entity or relationship. \\nData is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language). \\nRelational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.\\n\\n:::info[Further reading]\\n\\n* See our [tutorial](/docs/tutorials/sql_qa/) for working with SQL databases.\\n* See our [SQL database toolkit](/docs/integrations/tools/sql_database/).\\n\\n:::\\n\\n#### Graph databases'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content=\"Graph databases are a specialized type of database designed to store and manage highly interconnected data. \\nUnlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties. \\nThis structure allows for efficient representation and querying of complex, interconnected data.\\nGraph databases store data in a graph structure, with nodes, edges, and properties.\\nThey are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services\\n\\n:::info[Further reading]\\n\\n* See our [tutorial](/docs/tutorials/graph/) for working with graph databases.\\n* See our [list of graph database integrations](/docs/integrations/graphs/). \\n* See Neo4j's [starter kit for LangChain](https://neo4j.com/developer-blog/langchain-neo4j-starter-kit/).\\n\\n:::\\n\\n### Retriever\"), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='LangChain provides a unified interface for interacting with various retrieval systems through the [retriever](/docs/concepts/retrievers/) concept. The interface is straightforward:\\n\\n1. Input: A query (string)\\n2. Output: A list of documents (standardized LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects)\\n\\nYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\\nRegardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple `invoke` method:'), Document(metadata={'source': 'docs/docs/concepts/retrieval.mdx', 'file_path': 'docs/docs/concepts/retrieval.mdx', 'file_name': 'retrieval.mdx', 'file_type': '.mdx'}, page_content='```python\\ndocs = retriever.invoke(query)\\n```\\n\\n:::info[Further reading]\\n\\n* See our [conceptual guide on retrievers](/docs/concepts/retrievers/).\\n* See our [how-to guide](/docs/how_to/#retrievers) on working with retrievers.\\n\\n:::'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='# Retrievers\\n\\n<span data-heading-keywords=\"retriever,retrievers\"></span>\\n\\n:::info[Prerequisites]\\n\\n* [Vector stores](/docs/concepts/vectorstores/)\\n* [Embeddings](/docs/concepts/embedding_models/)\\n* [Text splitters](/docs/concepts/text_splitters/)\\n\\n:::\\n\\n## Overview\\n\\nMany different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.\\nWith the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., [RAG](/docs/concepts/rag/)).\\nBecause of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.\\nThe LangChain [retriever](/docs/concepts/retrievers/) interface is straightforward:\\n\\n1. Input: A query (string)\\n2. Output: A list of documents (standardized LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects)\\n\\n## Key concept'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content=\"![Retriever](/img/retriever_concept.png)\\n \\nAll retrievers implement a simple interface for retrieving documents using natural language queries.\\n\\n## Interface \\n\\nThe only requirement for a retriever is the ability to accepts a query and return documents. \\nIn particular, [LangChain's retriever class](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html#) only requires that the `_get_relevant_documents` method is implemented, which takes a `query: str` and returns a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects that are most relevant to the query.\\nThe underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.\"), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='A LangChain retriever is a [runnable](/docs/how_to/lcel_cheatsheet/), which is a standard interface is for LangChain components. \\nThis means that it has a few common methods, including `invoke`, that are used to interact with it. A retriever can be invoked with a query:\\n\\n```python\\ndocs = retriever.invoke(query)\\n```\\n\\nRetrievers return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, which have two attributes:\\n\\n* `page_content`: The content of this document. Currently is a string.\\n* `metadata`: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc). \\n\\n:::info[Further reading]\\n\\n* See our [how-to guide](/docs/how_to/custom_retriever/) on building your own custom retriever.\\n\\n:::\\n \\n## Common types\\n\\nDespite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\\n\\n### Search apis'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content=\"It's important to note that retrievers don't need to actually *store* documents. \\nFor example, we can be built retrievers on top of search APIs that simply return search results! \\nSee our retriever integrations with [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/) or [Wikipedia Search](/docs/integrations/retrievers/wikipedia/). \\n\\n### Relational or graph database\\n\\nRetrievers can be built on top of relational or graph databases.\\nIn these cases, [query analysis](/docs/concepts/retrieval/) techniques to construct a structured query from natural language is critical.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\\n\\n:::info[Further reading]\"), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.\\n* See our [tutorial](/docs/tutorials/graph/) for context on how to build a retreiver using a graph database and text-to-Cypher.\\n\\n:::\\n\\n### Lexical search\\n\\nAs discussed in our conceptual review of [retrieval](/docs/concepts/retrieval/), many search engines are based upon matching words in a query to the words in each document. \\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\\nLangChain has retrievers for many popular lexical search algorithms / engines.\\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\\n* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.\\n* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\\n\\n::: \\n\\n### Vector store \\n\\n[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. \\nA vectorstore can be used as a retriever by calling the `as_retriever()` method.\\n\\n```python\\nvectorstore = MyVectorStore()\\nretriever = vectorstore.as_retriever()\\n```\\n\\n## Advanced retrieval patterns\\n\\n### Ensemble'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='Because the retriever interface is so simple, returning a list of `Document` objects given a search query, it is possible to combine multiple retrievers using ensembling.\\nThis is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\\nIt is easy to create an [ensemble retriever](/docs/how_to/ensemble_retriever/) that combines multiple retrievers with linear weighted scores:\\n\\n```python\\n# Initialize the ensemble retriever\\nensemble_retriever = EnsembleRetriever(\\n    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]\\n)\\n```\\n\\nWhen ensembling, how do we combine search results from many retrievers? \\nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).\\n\\n### Source document retention'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='Many retrievers utilize some kind of index to make documents easily searchable.\\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting). \\nWhatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.\\n\\n![Retrieval with full docs](/img/retriever_full_docs.png)\\n\\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\\nFor example, you may use small chunk size for indexing documents in a vectorstore. \\nIf you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks.'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='LangChain has two different retrievers that can be used to address this challenge. \\nThe [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. \\nThe [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content='| Name                                                      | Index Type                    | Uses an LLM               | When to Use                                                                                                                             | Description                                                                                                                                                                                                              |\\n|-----------------------------------------------------------|-------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |\\n| [Multi Vector](/docs/how_to/multi_vector/)                | Vector store + Document Store | Sometimes during indexing | If you are able to extract information from documents that you think is more relevant to index than the text itself.                    | This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.                                         |'), Document(metadata={'source': 'docs/docs/concepts/retrievers.mdx', 'file_path': 'docs/docs/concepts/retrievers.mdx', 'file_name': 'retrievers.mdx', 'file_type': '.mdx'}, page_content=':::info[Further reading]\\n\\n* See our [how-to guide](/docs/how_to/parent_document_retriever/) on using the ParentDocument retriever.\\n* See our [how-to guide](/docs/how_to/multi_vector/) on using the MultiVector retriever.\\n* See our RAG from Scratch video on the [multi vector retriever](https://youtu.be/gTCU9I6QqCE?feature=shared).\\n\\n:::'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"# Runnable interface\\n\\nThe Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\\n\\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=':::info Related Resources\\n* The [\"Runnable\" Interface API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) provides a detailed overview of the Runnable interface and its methods.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).\\n:::\\n\\n## Overview of runnable interface\\n\\nThe Runnable way defines a standard interface that allows a Runnable component to be:'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\\n* Inspected: Schematic information about Runnable\\'s input, output, and configuration can be accessed.\\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\\n\\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\\n\\n<a id=\"batch\"></a>\\n### Optimized parallel execution (batch)\\n<span data-heading-keywords=\"batch\"></span>\\n\\nLangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Using these methods can significantly improve performance when needing to process multiple independent inputs, as the\\nprocessing can be done in parallel instead of sequentially.\\n\\nThe two batching options are:\\n\\n* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.\\n* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\\n\\nThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"Some Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,\\nrely on a `batch` API provided by a model provider).\\n\\n:::note\\nThe async versions of `abatch` and `abatch_as_completed` relies on asyncio's [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.\\n:::\\n\\n:::tip\\nWhen processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Chat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.\\n:::\\n\\n### Asynchronous support\\n<span data-heading-keywords=\"async-api\"></span>\\n\\nRunnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).\\n\\nPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.\\n\\n## Streaming APIs\\n<span data-heading-keywords=\"streaming-api\"></span>\\n\\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\\n\\nRunnables expose the following three streaming APIs:'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): yields the output a Runnable as it is generated.\\n2. The async `astream_events`: a more advanced streaming API that allows streaming intermediate steps and final output\\n3. The **legacy** async `astream_log`: a legacy streaming API that streams intermediate steps and final output\\n\\nPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.\\n\\n## Input and output types\\n\\nEvery `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.\\n\\n* invoke: Accepts an input and returns an output.\\n* batch: Accepts a list of inputs and returns a list of outputs.\\n* stream: Accepts an input and returns a generator that yields outputs.\\n\\nThe **input type** and **output type** vary by component:'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='| Component    | Input Type                                       | Output Type           |\\n|--------------|--------------------------------------------------|-----------------------|\\n| Prompt       | dictionary                                       | PromptValue           |\\n| ChatModel    | a string, list of chat messages or a PromptValue | ChatMessage           |\\n| LLM          | a string, list of chat messages or a PromptValue | String                |\\n| OutputParser | the output of an LLM or ChatModel                | Depends on the parser |\\n| Retriever    | a string                                         | List of Documents     |\\n| Tool         | a string or dictionary, depending on the tool    | Depends on the tool   |\\n\\nPlease refer to the individual component documentation for more information on the input and output types and how to use them.\\n\\n### Inspecting schemas'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=':::note\\nThis is an advanced feature that is unnecessary for most users. You should probably\\nskip this section unless you have a specific need to inspect the schema of a Runnable.\\n:::\\n\\nIn more advanced use cases, you may want to programmatically **inspect** the Runnable and determine what input and output types the Runnable expects and produces.\\n\\nThe Runnable interface provides methods to get the [JSON Schema](https://json-schema.org/) of the input and output types of a Runnable, as well as [Pydantic schemas](https://docs.pydantic.dev/latest/) for the input and output types.\\n\\nThese APIs are mostly used internally for unit-testing and by [LangServe](/docs/concepts/architecture#langserve) which uses the APIs for input validation and generation of [OpenAPI documentation](https://www.openapis.org/).'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='In addition, to the input and output types, some Runnables have been set up with additional run time configuration options. \\nThere are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.\\nPlease see the [Configurable Runnables](#configurable-runnables) section for more information.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='| Method                  | Description                                                      |\\n|-------------------------|------------------------------------------------------------------|\\n| `get_input_schema`      | Gives the Pydantic Schema of the input schema for the Runnable.  |\\n| `get_output_schema`      | Gives the Pydantic Schema of the output schema for the Runnable. |\\n| `config_schema`         | Gives the Pydantic Schema of the config schema for the Runnable. |\\n| `get_input_jsonschema`  | Gives the JSONSchema of the input schema for the Runnable.       |\\n| `get_output_jsonschema` | Gives the JSONSchema of the output schema for the Runnable.      |\\n| `get_config_jsonschema` | Gives the JSONSchema of the config schema for the Runnable.      |\\n\\n\\n#### With_types\\n\\nLangChain will automatically try to infer the input and output types of a Runnable based on available information.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Currently, this inference does not work well for more complex Runnables that are built using [LCEL](/docs/concepts/lcel) composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the `with_types` method ([API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_types\\n).\\n\\n## RunnableConfig\\n\\nAny of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called\\n`RunnableConfig` ([API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#RunnableConfig)). This argument is a dictionary that contains configuration for the Runnable that will be used\\nat run time during the execution of the runnable.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='A `RunnableConfig` can have any of the following properties defined:'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='| Attribute       | Description                                                                                |\\n|-----------------|--------------------------------------------------------------------------------------------|\\n| run_name        | Name used for the given Runnable (not inherited).                                          |\\n| run_id          | Unique identifier for this call. sub-calls will get their own unique run ids.              |\\n| tags            | Tags for this call and any sub-calls.                                                      |\\n| metadata        | Metadata for this call and any sub-calls.                                                  |\\n| callbacks       | Callbacks for this call and any sub-calls.                                                 |\\n| max_concurrency | Maximum number of parallel calls to make (e.g., used by batch).                            |\\n| recursion_limit | Maximum number of times a call can recurse (e.g., used by Runnables that return Runnables) |\\n| configurable    | Runtime values for configurable attributes of the Runnable.                                |'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"Passing `config` to the `invoke` method is done like so:\\n\\n```python\\nsome_runnable.invoke(\\n   some_input, \\n   config={\\n      'run_name': 'my_run', \\n      'tags': ['tag1', 'tag2'], \\n      'metadata': {'key': 'value'}\\n      \\n   }\\n)\\n```\\n\\n### Propagation of RunnableConfig\\n\\nMany `Runnables` are composed of other Runnables, and it is important that the `RunnableConfig` is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\\n\\nIf this were not the case, it would be impossible to set and propagate [callbacks](/docs/concepts/callbacks) or other configuration values like `tags` and `metadata` which\\nare expected to be inherited by all sub-calls.\\n\\nThere are two main patterns by which new `Runnables` are created:\\n\\n1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):\\n\\n    ```python\\n    chain = prompt | chat_model | output_parser\\n    ```\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"2. Using a [custom Runnable](#custom-runnables)  (e.g., `RunnableLambda`) or using the `@tool` decorator:\\n\\n    ```python\\n    def foo(input):\\n        # Note that .invoke() is used directly here\\n        return bar_runnable.invoke(input)\\n    foo_runnable = RunnableLambda(foo)\\n    ```\\n\\nLangChain will try to propagate `RunnableConfig` automatically for both of the patterns. \\n\\nFor handling the second pattern, LangChain relies on Python's [contextvars](https://docs.python.org/3/library/contextvars.html).\\n\\nIn Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the `RunnableConfig` to the sub-calls.\\n\\nIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. \\n\\nThis is due to a limitation in [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task)  in Python 3.9 and 3.10 which did\\nnot accept a `context` argument).\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Propagating the `RunnableConfig` manually is done like so:\\n\\n```python\\nasync def foo(input, config): # <-- Note the config argument\\n    return await bar_runnable.ainvoke(input, config=config)\\n    \\nfoo_runnable = RunnableLambda(foo)\\n```\\n\\n:::caution\\nWhen using Python 3.10 or lower and writing async code, `RunnableConfig` cannot be propagated\\nautomatically, and you will need to do it manually! This is a common pitfall when\\nattempting to stream data using `astream_events` and `astream_log` as these methods\\nrely on proper propagation of [callbacks](/docs/concepts/callbacks) defined inside of `RunnableConfig`.\\n:::\\n\\n### Setting custom run name, tags, and metadata\\n\\nThe `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='The `run_name` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\\n\\nThe `tags` and `metadata` attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\\n\\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can\\nfilter and search on.\\n\\nThe attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [astream_events](/docs/concepts/streaming) as part of each event in the stream.\\n\\n:::note Related\\n* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n:::\\n\\n### Setting run id\\n\\n:::note\\nThis is an advanced feature that is unnecessary for most users.\\n:::'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"You may need to set a custom `run_id` for a given run, in case you want \\nto reference it later or correlate it with other systems.\\n\\nThe `run_id` MUST be a valid UUID string and **unique** for each run. It is used to identify\\nthe parent run, sub-class will get their own unique run ids automatically.\\n\\nTo set a custom `run_id`, you can pass it as a key-value pair in the `config` dictionary when invoking the Runnable:\\n\\n```python\\nimport uuid\\n\\nrun_id = uuid.uuid4()\\n\\nsome_runnable.invoke(\\n   some_input, \\n   config={\\n      'run_id': run_id\\n   }\\n)\\n\\n# Do something with the run_id\\n```\\n\\n### Setting recursion limit\\n\\n:::note\\nThis is an advanced feature that is unnecessary for most users.\\n:::\\n\\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` dictionary. This will limit the number of times a Runnable can recurse.\\n\\n### Setting max concurrency\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content=\"If using the `batch` or `batch_as_completed` methods, you can set the `max_concurrency` attribute in the `RunnableConfig` dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\\n\\n\\n:::tip\\nIf you're trying to rate limit the number of requests made by a **Chat Model**, you can use the built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) instead of setting `max_concurrency`, which will be more effective.\\n\\nSee the [How to handle rate limits](/docs/how_to/chat_model_rate_limiting/) guide for more information.\\n:::\\n\\n### Setting configurable\\n\\nThe `configurable` field is used to pass runtime values for configurable attributes of the Runnable.\"), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='It is used frequently in [LangGraph](/docs/concepts/architecture#langgraph) with\\n[LangGraph Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/)\\nand [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\nIt is used for a similar purpose in [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) to specify either\\na `session_id` / `conversation_id` to keep track of conversation history.\\n\\nIn addition, you can use it to specify any custom configuration options to pass to any [Configurable Runnable](#configurable-runnables) that they create.\\n\\n### Setting callbacks\\n\\nUse this option to configure [callbacks](/docs/concepts/callbacks) for the runnable at \\nruntime. The callbacks will be passed to all sub-calls made by the runnable.'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='```python\\nsome_runnable.invoke(\\n   some_input,\\n   {\\n      \"callbacks\": [\\n         SomeCallbackHandler(),\\n         AnotherCallbackHandler(),\\n      ]\\n   }\\n)\\n```\\n\\nPlease read the [Callbacks Conceptual Guide](/docs/concepts/callbacks) for more information on how to use callbacks in LangChain.\\n\\n:::important\\nIf you\\'re using Python 3.9 or 3.10 in an async environment, you must propagate\\nthe `RunnableConfig` manually to sub-calls in some cases. Please see the\\n[Propagating RunnableConfig](#propagation-of-runnableconfig) section for more information.\\n:::\\n\\n## Creating a runnable from a function {#custom-runnables}\\n\\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\\nmultiple Runnables and you need to add custom processing logic in one of the steps.\\n\\nThere are two ways to create a custom Runnable from a function:'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='* `RunnableLambda`: Use this for simple transformations where streaming is not required.\\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\\n\\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\\n\\n:::important\\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\\n:::\\n\\n## Configurable runnables\\n\\n:::note\\nThis is an advanced feature that is unnecessary for most users.\\n\\nIt helps with configuration of large \"chains\" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\\n:::'), Document(metadata={'source': 'docs/docs/concepts/runnables.mdx', 'file_path': 'docs/docs/concepts/runnables.mdx', 'file_name': 'runnables.mdx', 'file_type': '.mdx'}, page_content='Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\\n\\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\\n\\n* `configurable_fields`: This method allows you to configure specific **attributes** in a Runnable. For example, the `temperature` attribute of a chat model.\\n* `configurable_alternatives`: This method enables you to specify **alternative** Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\\n\\nSee the [How to configure runtime chain internals](/docs/how_to/configure) guide for more information on how to configure runtime chain internals.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='# Streaming\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n* [Chat Models](/docs/concepts/chat_models)\\n:::\\n\\n**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\\n\\n## Overview\\n\\nGenerating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=\"In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.\\n\\n## What to stream in LLM applications\\n\\nIn applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:\\n\\n### 1. Streaming LLM outputs\\n\\nThe most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.\\n\\n### 2. Streaming pipeline or workflow progress\\n\\nBeyond just streaming LLM output, it’s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\"), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=\"- **In LangGraph Workflows:**\\nWith [LangGraph](/docs/concepts/architecture#langgraph), workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state** as individual **nodes** request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.\\n\\n- **In LCEL Pipelines:**\\nStreaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.\\n\\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\\n\\n### 3. Streaming custom data\"), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='In some cases, you may need to stream **custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.\\n\\n## Streaming APIs\\n\\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.\\n2. The async only [astream_events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events): Use this API to get access to custom events and intermediate outputs from LLM  applications built entirely with [LCEL](/docs/concepts/lcel). Note that this API is available, but not needed when working with LangGraph.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=':::note\\nIn addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_log) API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.\\n:::\\n\\n### `stream()` and `astream()`\\n\\nThe `stream()` method returns an iterator that yields chunks of output synchronously as they are produced. You can use a `for` loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=\"The type of chunk yielded by the `stream()` and `astream()` methods depends on the component being streamed. For example, when streaming from an [LLM](/docs/concepts/chat_models) each component will be an [AIMessageChunk](/docs/concepts/messages#aimessagechunk); however, for other components, the chunk may be different. \\n\\nThe `stream()` method returns an iterator that yields these chunks as they are produced. For example,\\n\\n```python\\nfor chunk in component.stream(some_input):\\n    # IMPORTANT: Keep the processing of each chunk as efficient as possible.\\n    # While you're processing the current chunk, the upstream component is\\n    # waiting to produce the next one. For example, if working with LangGraph,\\n    # graph execution is paused while the current chunk is being processed.\\n    # In extreme cases, this could even result in timeouts (e.g., when llm outputs are\\n    # streamed from an API that has a timeout).\\n    print(chunk)\\n```\"), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=\"The [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\\n\\n#### Usage with chat models\\n\\nWhen using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.\\n\\n#### Usage with LangGraph\\n\\n[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.\"), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='When using the *stream* and *astream* methods with LangGraph, you can choose **one or more** [streaming mode](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamMode) which allow you to control the type of output that is streamed. The available streaming modes are:\\n\\n- **\"values\"**: Emit all values of the [state](https://langchain-ai.github.io/langgraph/concepts/low_level/) for each step.\\n- **\"updates\"**: Emit only the node name(s) and updates that were returned by the node(s) after each step.\\n- **\"debug\"**: Emit debug events for each step.\\n- **\"messages\"**: Emit LLM [messages](/docs/concepts/messages) [token-by-token](/docs/concepts/tokens).\\n- **\"custom\"**: Emit custom output written using [LangGraph\\'s StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter).'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='For more information, please see:\\n* [LangGraph streaming conceptual guide](https://langchain-ai.github.io/langgraph/concepts/streaming/) for more information on how to stream when working with LangGraph.\\n* [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming) for specific examples of streaming in LangGraph.\\n\\n#### Usage with LCEL\\n\\nIf you compose multiple Runnables using [LangChain’s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\\n\\n### `astream_events`\\n<span data-heading-keywords=\"astream_events,stream_events,stream events\"></span>'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content=':::tip\\nUse the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). \\n\\nWhile this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.\\n:::\\n\\nFor chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='There are ways to do this [using callbacks](/docs/concepts/callbacks), or by constructing your chain in such a way that it passes intermediate\\nvalues to the end with something like chained [`.assign()`](/docs/how_to/passthrough/) calls, but LangChain also includes an\\n`.astream_events()` method that combines the flexibility of callbacks with the ergonomics of `.stream()`. When called, it returns an iterator\\nwhich yields [various types of events](/docs/how_to/streaming/#event-reference) that you can filter and process according\\nto the needs of your project.\\n\\nHere\\'s one small example that prints just events containing streamed chat model output:\\n\\n```python\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_anthropic import ChatAnthropic\\n\\nmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\\nparser = StrOutputParser()\\nchain = prompt | model | parser\\n\\nasync for event in chain.astream_events({\"topic\": \"parrot\"}):\\n    kind = event[\"event\"]\\n    if kind == \"on_chat_model_stream\":\\n        print(event, end=\"|\", flush=True)\\n```\\n\\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\\n\\nSee [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.\\n\\n## Writing custom data to the stream\\n\\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='1. LangGraph\\'s [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astream** APIs when working with LangGraph. **Important** this is a LangGraph feature, so it is not available when working with pure LCEL. See [how to streaming custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/) for more information.\\n2. [dispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.dispatch_custom_event.html#) / [adispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) can be used to write custom data that will be surfaced through the **astream_events** API. See [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api) for more information.\\n\\n## \"Auto-Streaming\" Chat Models'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming `invoke` method but still want to stream the entire application, including intermediate results from the chat model.\\n\\n### How It Works\\n\\nWhen you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='Under the hood, it\\'ll have `invoke` (or `ainvoke`) use the `stream` (or `astream`) method to generate its output. The result of the invocation will be the same as far as the code that was using `invoke` is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain\\'s [callback system](/docs/concepts/callbacks). These callback events\\nallow LangGraph `stream`/`astream` and `astream_events` to surface the chat model\\'s output in real-time.\\n\\nExample:\\n\\n```python\\ndef node(state):\\n    ...\\n    # The code below uses the invoke method, but LangChain will \\n    # automatically switch to streaming mode\\n    # when it detects that the overall \\n    # application is being streamed.\\n    ai_message = model.invoke(state[\"messages\"])\\n    ...\\n\\nfor chunk in compiled_graph.stream(..., mode=\"messages\"): \\n    ...\\n```\\n## Async Programming'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an \"a\" (e.g., `ainvoke`, `astream`). When writing async code, it\\'s crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.\\n\\nIf streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.\\n\\nPlease review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.\\n\\n## Related Resources'), Document(metadata={'source': 'docs/docs/concepts/streaming.mdx', 'file_path': 'docs/docs/concepts/streaming.mdx', 'file_name': 'streaming.mdx', 'file_type': '.mdx'}, page_content='Please see the following how-to guides for specific examples of streaming in LangChain:\\n* [LangGraph conceptual guide on streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/)\\n* [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming)\\n* [How to stream runnables](/docs/how_to/streaming/): This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with [LCEL](/docs/concepts/lcel).\\n* [How to stream chat models](/docs/how_to/chat_streaming/)\\n* [How to stream tool calls](/docs/how_to/tool_streaming/)\\n\\nFor writing custom data to the stream, please see the following resources:\\n\\n* If using LangGraph, see [how to stream custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/).\\n* If using LCEL, see [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api).'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='# Structured outputs\\n\\n## Overview \\n\\nFor many applications, such as chatbots, models need to respond to users directly in natural language. \\nHowever, there are scenarios where we need models to output in a *structured format*. \\nFor example, we might want to store the model output in a database and ensure that the output conforms to the database schema.\\nThis need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\\n\\n![Structured output](/img/structured_output.png)\\n\\n## Key concepts \\n\\n**(1) Schema definition:** The output structure is represented as a schema, which can be defined in several ways. \\n**(2) Returning structured output:** The model is given this schema, and is instructed to return output that conforms to it.\\n\\n## Recommended usage'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='This pseudo-code illustrates the recommended workflow when using structured output.\\nLangChain provides a method, [`with_structured_output()`](/docs/how_to/structured_output/#the-with_structured_output-method), that automates the process of binding the schema to the [model](/docs/concepts/chat_models/) and parsing the output.\\nThis helper function is available for all model providers that support structured output. \\n\\n```python\\n# Define schema\\nschema = {\"foo\": \"bar\"}\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n# Invoke the model to produce structured output that matches the schema\\nstructured_output = model_with_structure.invoke(user_input)\\n```\\n\\n## Schema definition'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='The central concept is that the output structure of model responses needs to be represented in some way. \\nWhile types of objects you can use depend on the model you\\'re working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\\n\\nThe simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\\nJSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\\n\\n```json\\n{\\n  \"answer\": \"The answer to the user\\'s question\",\\n  \"followup_question\": \"A followup question the user could ask\"\\n}\\n```\\n\\nAs a second example, [Pydantic](https://docs.pydantic.dev/latest/) is particularly useful for defining structured output schemas because it offers type hints and validation.\\nHere\\'s an example of a Pydantic schema:'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom pydantic import BaseModel, Field\\nclass ResponseFormatter(BaseModel):\\n    \"\"\"Always use this tool to structure your response to the user.\"\"\"\\n    answer: str = Field(description=\"The answer to the user\\'s question\")\\n    followup_question: str = Field(description=\"A followup question the user could ask\")\\n\\n```\\n\\n## Returning structured output\\n\\nWith a schema defined, we need a way to instruct the model to use it.\\nWhile one approach is to include this schema in the prompt and *ask nicely* for the model to use it, this is not recommended. \\nSeveral more powerful methods that utilizes native features in the model provider\\'s API are available.\\n\\n### Using tool calling'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).\\nIn short, tool calling involves binding a tool to a model and, when appropriate, the model can *decide* to call this tool and ensure its response conforms to the tool\\'s schema.\\nWith this in mind, the central concept is straightforward: *simply bind our schema to a model as a tool!*\\nHere is an example using the `ResponseFormatter` schema defined above:\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\n# Bind responseformatter schema as a tool to the model\\nmodel_with_tools = model.bind_tools([ResponseFormatter])\\n# Invoke the model\\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\\n```'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='The arguments of the tool call are already extracted as a dictionary. \\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\\n\\n```python\\n# Get the tool call arguments\\nai_msg.tool_calls[0][\"args\"]\\n{\\'answer\\': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\\n \\'followup_question\\': \\'What is the function of ATP in the cell?\\'}\\n# Parse the dictionary into a pydantic object\\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\\n```\\n\\n### JSON mode'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='In addition to tool calling, some model providers support a feature called `JSON mode`. \\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\\nHere is an example of how to use JSON mode with OpenAI:\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\\nai_msg = model.invoke(\"Return a JSON object with key \\'random_ints\\' and a value of 10 random ints in [0-99]\")\\nai_msg.content\\n\\'\\\\n{\\\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\\\n}\\'\\n```'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content=\"One important point to flag: the model *still* returns a string, which needs to be parsed into a JSON object.\\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json) for more details.\\n\\n```python\\nimport json\\njson_object = json.loads(ai_msg.content)\\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\\n```\\n\\n## Structured output method \\n\\nThere are a few challenges when producing structured output with the above methods: \\n\\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.  \\n\\n(2) In addition, the model needs to be instructed to *always* use the tool when we want to enforce structured output, which is a provider specific setting. \\n\\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\"), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='With these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\\n\\n![Diagram of with structured output](/img/with_structured_output.png)\\n\\nThis both binds the schema to the model as a tool and parses the output to the specified output schema. \\n\\n```python\\n# Bind the schema to the model\\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\\n# Invoke the model\\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\\n# Get back the pydantic object\\nstructured_output\\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question=\\'What is the function of ATP in the cell?\\')\\n```\\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/structured_outputs.mdx', 'file_path': 'docs/docs/concepts/structured_outputs.mdx', 'file_name': 'structured_outputs.mdx', 'file_type': '.mdx'}, page_content='For more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\\n\\n:::'), Document(metadata={'source': 'docs/docs/concepts/testing.mdx', 'file_path': 'docs/docs/concepts/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='# Testing\\n<span data-heading-keywords=\"tests,testing,unit,integration\"></span>\\n\\nTesting is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.\\n\\nIn the LangChain ecosystem, we have 2 main types of tests: **unit tests** and **integration tests**.\\n\\nFor integrations that implement standard LangChain abstractions, we have a set of **standard tests** (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.\\n\\n## Unit Tests\\n\\n**Definition**: Unit tests are designed to validate the smallest parts of your code—individual functions or methods—ensuring they work as expected in isolation. They do not rely on external systems or integrations.\\n\\n**Example**: Testing the `convert_langchain_aimessage_to_dict` function to confirm it correctly converts an AI message to a dictionary format:'), Document(metadata={'source': 'docs/docs/concepts/testing.mdx', 'file_path': 'docs/docs/concepts/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_core.messages import AIMessage, ToolCall, convert_to_openai_messages\\n\\ndef test_convert_to_openai_messages():\\n    ai_message = AIMessage(\\n        content=\"Let me call that tool for you!\",\\n        tool_calls=[\\n            ToolCall(name=\\'parrot_multiply_tool\\', id=\\'1\\', args={\\'a\\': 2, \\'b\\': 3}),\\n        ]\\n    )\\n    \\n    result = convert_to_openai_messages(ai_message)\\n    \\n    expected = {\\n        \"role\": \"assistant\",\\n        \"tool_calls\": [\\n            {\\n                \"type\": \"function\",\\n                \"id\": \"1\",\\n                \"function\": {\\n                    \"name\": \"parrot_multiply_tool\",\\n                    \"arguments\": \\'{\"a\": 2, \"b\": 3}\\',\\n                },\\n            }\\n        ],\\n        \"content\": \"Let me call that tool for you!\",\\n    }\\n    assert result == expected  # Ensure conversion matches expected output\\n```\\n\\n---\\n\\n## Integration Tests'), Document(metadata={'source': 'docs/docs/concepts/testing.mdx', 'file_path': 'docs/docs/concepts/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='**Definition**: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.\\n\\n**Example**: Testing `ParrotMultiplyTool` with access to an API service that multiplies two numbers and adds 80:\\n\\n```python\\ndef test_integration_with_service():\\n    tool = ParrotMultiplyTool()\\n    result = tool.invoke({\"a\": 2, \"b\": 3})\\n    assert result == 86\\n```\\n\\n---\\n\\n## Standard Tests\\n\\n**Definition**: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.\\n\\n**Example**: Subclassing LangChain\\'s `ToolsUnitTests` or `ToolsIntegrationTests` to automatically run standard tests:\\n\\n```python\\nfrom langchain_tests.unit_tests import ToolsUnitTests'), Document(metadata={'source': 'docs/docs/concepts/testing.mdx', 'file_path': 'docs/docs/concepts/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='class TestParrotMultiplyToolUnit(ToolsUnitTests):\\n    @property\\n    def tool_constructor(self):\\n        return ParrotMultiplyTool\\n\\n    def tool_invoke_params_example(self):\\n        return {\"a\": 2, \"b\": 3}\\n```\\n\\nTo learn more, check out our guide on [how to add standard tests to an integration](../../contributing/how_to/integrations/standard_tests).'), Document(metadata={'source': 'docs/docs/concepts/text_llms.mdx', 'file_path': 'docs/docs/concepts/text_llms.mdx', 'file_name': 'text_llms.mdx', 'file_type': '.mdx'}, page_content='# String-in, string-out llms\\n\\n:::tip\\nYou are probably looking for the [Chat Model Concept Guide](/docs/concepts/chat_models) page for more information.\\n:::\\n\\nLangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the \"Chat\" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.), and may include the \"LLM\" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface.\\n\\nUsers should be using almost exclusively the newer [Chat Models](/docs/concepts/chat_models) as most\\nmodel providers have adopted a chat like interface for interacting with language models.'), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content='# Text splitters\\n<span data-heading-keywords=\"text splitter,text splitting\"></span>\\n\\n:::info[Prerequisites]\\n\\n* [Documents](/docs/concepts/retrievers/#interface)\\n* Tokenization(/docs/concepts/tokens)\\n:::\\n\\n## Overview\\n\\nDocument splitting is often a crucial preprocessing step for many applications.\\nIt involves breaking down large texts into smaller, manageable chunks.\\nThis process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\\nThere are several strategies for splitting documents, each with its own advantages.\\n\\n## Key concepts\\n\\n![Conceptual Overview](/img/text_splitters.png)\\n\\nText splitters split documents into smaller chunks for use in downstream applications.\\n\\n## Why split documents?\\n\\nThere are several reasons to split documents:'), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content='- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\\n- **Overcoming model limitations**: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\\n- **Improving representation quality**: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\\n- **Enhancing retrieval precision**: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\\n- **Optimizing computational resources**: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.'), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content=\"Now, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.\\n\\n:::info[Further reading]\\n* See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.\\n:::\\n\\n## Approaches\\n\\n### Length-based\\n\\nThe most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.\\nKey benefits of length-based splitting:\\n- Straightforward implementation\\n- Consistent chunk sizes\\n- Easily adaptable to different model requirements\\n\\nTypes of length-based splitting:\\n- **Token-based**: Splits text based on the number of tokens, which is useful when working with language models.\\n- **Character-based**: Splits text based on the number of characters, which can be more consistent across different types of text.\"), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content='Example implementation using LangChain\\'s `CharacterTextSplitter` with token-based splitting:\\n\\n```python\\nfrom langchain_text_splitters import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\\n    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\\n)\\ntexts = text_splitter.split_text(document)\\n```\\n\\n:::info[Further reading]\\n\\n* See the how-to guide for [token-based](/docs/how_to/split_by_token/) splitting.\\n* See the how-to guide for [character-based](/docs/how_to/character_text_splitter/) splitting.\\n\\n:::\\n\\n### Text-structured based'), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content=\"Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. \\nWe can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\\nLangChain's [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter/) implements this concept:\\n- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.\\n- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\\n- This process continues down to the word level if necessary.\\n\\nHere is example usage:\\n\\n```python\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\\ntexts = text_splitter.split_text(document)\\n```\\n\\n:::info[Further reading]\"), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content=\"* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).\\n\\n:::\\n\\n### Document-structured based\\n\\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. \\nIn these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.\\nKey benefits of structure-based splitting:\\n- Preserves the logical organization of the document\\n- Maintains context within each chunk\\n- Can be more effective for downstream tasks like retrieval or summarization\\n\\nExamples of structure-based splitting:\\n- **Markdown**: Split based on headers (e.g., #, ##, ###)\\n- **HTML**: Split using tags\\n- **JSON**: Split by object or array elements\\n- **Code**: Split by functions, classes, or logical blocks\\n\\n:::info[Further reading]\"), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content=\"* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).\\n* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).\\n* See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).\\n* See the how-to guide for [HTML splitting](/docs/how_to/split_html/).\\n\\n:::\\n\\n### Semantic meaning based\\n\\nUnlike the previous methods, semantic-based splitting actually considers the *content* of the text. \\nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.\\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*.\\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:\"), Document(metadata={'source': 'docs/docs/concepts/text_splitters.mdx', 'file_path': 'docs/docs/concepts/text_splitters.mdx', 'file_name': 'text_splitters.mdx', 'file_type': '.mdx'}, page_content='- Start with the first few sentences and generate an embedding.\\n- Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\\n- Compare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\\n\\nThis technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.\\n\\n:::info[Further reading]\\n\\n* See the how-to guide for [splitting text based on semantic meaning](/docs/how_to/semantic-chunker/).\\n* See Greg Kamradt\\'s [notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) showcasing semantic splitting.\\n\\n:::'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='# Tokens\\n\\nModern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we\\'ll discuss what tokens are and how they are used by language models.\\n\\n## What is a token?\\n\\nA **token** is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:\\n\\n* A whole word (e.g., \"apple\"),\\n* A part of a word (e.g., \"app\"),\\n* Or other linguistic components such as punctuation or spaces.\\n\\nThe way the model tokenizes the input depends on its **tokenizer algorithm**, which converts the input into tokens. Similarly, the model’s output comes as a stream of tokens, which is then decoded back into human-readable text.\\n\\n## How tokens work in language models'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on **tokens**, which represent meaningful linguistic units. Here\\'s how the process works:\\n\\n1. **Input Tokenization**: When you provide a model with a prompt (e.g., \"LangChain is cool!\"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like `[\"Lang\", \"Chain\", \" is\", \" cool\", \"!\"]`. Note that token boundaries don’t always align with word boundaries.\\n    ![](/img/tokenization.png)'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.\\n3. **Output Generation**: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.\\n\\nUsing tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.\\n\\n## Tokens don’t have to be text\\n\\nAlthough tokens are most commonly used to represent text, they don’t have to be limited to textual data. Tokens can also serve as abstract representations of **multi-modal data**, such as:\\n\\n- **Images**,\\n- **Audio**,\\n- **Video**,\\n- And other types of data.'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='At the time of writing, virtually no models support **multi-modal output**, and only a few models can handle **multi-modal inputs** (e.g., text combined with images or audio). However, as advancements in AI continue, we expect **multi-modality** to become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data.'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content=':::note\\nIn principle, **anything that can be represented as a sequence of tokens** could be modeled in a similar way. For example, **DNA sequences**—which are composed of a series of nucleotides (A, T, C, G)—can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences.\\n:::\\n\\nPlease see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.\\n\\n## Why not use characters?'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.\\n\\nIn contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.\\n\\n## How tokens correspond to text\\n\\nPlease see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and how they correspond to text.'), Document(metadata={'source': 'docs/docs/concepts/tokens.mdx', 'file_path': 'docs/docs/concepts/tokens.mdx', 'file_name': 'tokens.mdx', 'file_type': '.mdx'}, page_content='According to the OpenAI post, the approximate token counts for English text are as follows:\\n\\n* 1 token ~= 4 chars in English\\n* 1 token ~= ¾ words\\n* 100 tokens ~= 75 words'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='# Tool calling\\n\\n:::info[Prerequisites]\\n* [Tools](/docs/concepts/tools)\\n* [Chat Models](/docs/concepts/chat_models)\\n:::\\n\\n\\n## Overview \\n\\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\\nBut what about cases where we want a model to also interact *directly* with systems, such as databases or an API?\\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\\nThis need motivates the concept of *tool calling*. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.\\n\\n:::info\\nYou will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. \\n:::\\n\\n![Conceptual overview of tool calling](/img/tool_calling_concept.png)\\n\\n## Key concepts'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content=\"**(1) Tool Creation:** Use the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator to create a [tool](/docs/concepts/tools). A tool is an association between a function and its schema.\\n**(2) Tool Binding:** The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.\\n**(3) Tool Calling:** When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.\\n**(4) Tool Execution:** The tool can be executed using the arguments provided by the model.\\n\\n![Conceptual parts of tool calling](/img/tool_calling_components.png)\\n\\n## Recommended usage\"), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='This pseudo-code illustrates the recommended workflow for using tool calling. \\nCreated tools are passed to `.bind_tools()` method as a list.\\nThis model can be called, as usual. If a tool call is made, model\\'s response will contain the tool call arguments.\\nThe tool call arguments can be passed directly to the tool.\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n# Tool calling \\nresponse = model_with_tools.invoke(user_input)\\n```\\n\\n## Tool creation\\n\\nThe recommended way to create a tool is using the `@tool` decorator.\\n\\n```python\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\"\"\"\\n    return a * b\\n```\\n\\n:::info[Further reading]\\n\\n* See our conceptual guide on [tools](/docs/concepts/tools/) for more details.\\n* See our [model integrations](/docs/integrations/chat/) that support tool calling.\\n* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.\\n\\n:::\\n\\n## Tool binding'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='[Many](https://platform.openai.com/docs/guides/function-calling) [model providers](https://platform.openai.com/docs/guides/function-calling) support tool calling. \\n\\n:::tip\\nSee our [model integration page](/docs/integrations/chat/) for a list of providers that support tool calling.\\n:::\\n\\nThe central concept to understand is that LangChain provides a standardized interface for connecting tools to models. \\nThe `.bind_tools()` method can be used to specify which tools are available for a model to call. \\n\\n```python\\nmodel_with_tools = model.bind_tools(tools_list)\\n```\\n\\nAs a specific example, let\\'s take a function `multiply` and bind it as a tool to a model that supports tool calling.\\n\\n```python\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\nllm_with_tools = tool_calling_model.bind_tools([multiply])\\n```\\n\\n## Tool calling\\n\\n![Diagram of a tool call by a model](/img/tool_call_example.png)'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='A key principle of tool calling is that the model decides when to use a tool based on the input\\'s relevance. The model doesn\\'t always need to call a tool.\\nFor example, given an unrelated input, the model would not call the tool:\\n\\n```python\\nresult = llm_with_tools.invoke(\"Hello world!\")\\n```\\n\\nThe result would be an `AIMessage` containing the model\\'s response in natural language (e.g., \"Hello!\").\\nHowever, if we pass an input *relevant to the tool*, the model should choose to call it:\\n\\n```python\\nresult = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\\n```\\n\\nAs before, the output `result` will be an `AIMessage`. \\nBut, if the tool was called, `result` will have a `tool_calls` attribute.\\nThis attribute includes everything needed to execute the tool, including the tool name and input arguments:\\n\\n```\\nresult.tool_calls\\n{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 2, \\'b\\': 3}, \\'id\\': \\'xxx\\', \\'type\\': \\'tool_call\\'}\\n```\\n\\nFor more details on usage, see our [how-to guides](/docs/how_to/#tools)!'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='## Tool execution\\n\\n[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)) that will often invoke the tool in behalf of the user.\\n\\n:::info[Further reading]\\n\\n* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.\\n* See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).\\n\\n:::\\n\\n## Best practices\\n\\nWhen designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:'), Document(metadata={'source': 'docs/docs/concepts/tool_calling.mdx', 'file_path': 'docs/docs/concepts/tool_calling.mdx', 'file_name': 'tool_calling.mdx', 'file_type': '.mdx'}, page_content='* Models that have explicit [tool-calling APIs](/docs/concepts/tool_calling) will be better at tool calling than non-fine-tuned models.\\n* Models will perform better if the tools have well-chosen names and descriptions.\\n* Simple, narrowly scoped tools are easier for models to use than complex tools.\\n* Asking the model to select from a large list of tools poses challenges for the model.'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\"# Tools\\n\\n:::info Prerequisites\\n- [Chat models](/docs/concepts/chat_models/)\\n:::\\n\\n## Overview\\n\\nThe **tool** abstraction in LangChain associates a Python **function** with a **schema** that defines the function's **name**, **description** and **expected arguments**. \\n\\n**Tools** can be passed to [chat models](/docs/concepts/chat_models) that support [tool calling](/docs/concepts/tool_calling) allowing the model to request the execution of a specific function with specific inputs.\\n\\n## Key concepts\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\"- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.\\n- Create tools using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator, which simplifies the process of tool creation, supporting the following:\\n   - Automatically infer the tool's **name**, **description** and **expected arguments**, while also supporting customization.\\n   - Defining tools that return **artifacts** (e.g. images, dataframes, etc.)\\n   - Hiding input arguments from the schema (and hence from the model) using **injected tool arguments**.\\n\\n## Tool interface\\n\\nThe tool interface is defined in the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) class which is a subclass of the [Runnable Interface](/docs/concepts/runnables).\\n\\nThe key attributes that correspond to the tool's **schema**:\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\"- **name**: The name of the tool.\\n- **description**: A description of what the tool does.\\n- **args**: Property that returns the JSON schema for the tool's arguments.\\n\\nThe key methods to execute the function associated with the **tool**:\\n\\n- **invoke**: Invokes the tool with the given arguments.\\n- **ainvoke**: Invokes the tool with the given arguments, asynchronously. Used for [async programming with Langchain](/docs/concepts/async).\\n\\n## Create tools using the `@tool` decorator\\n\\nThe recommended way to create tools is using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) to create a tool that implements the [Tool Interface](#tool-interface).\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n   \"\"\"Multiply two numbers.\"\"\"\\n   return a * b\\n```\\n\\nFor more details on how to create tools, see the [how to create custom tools](/docs/how_to/custom_tools/) guide.\\n\\n:::note\\nLangChain has a few other ways to create tools; e.g., by sub-classing the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) class or by using `StructuredTool`. These methods are shown in the [how to create custom tools guide](/docs/how_to/custom_tools/), but\\nwe generally recommend using the `@tool` decorator for most cases.\\n:::\\n\\n## Use the tool directly\\n\\nOnce you have defined a tool, you can use it directly by calling the function. For example, to use the `multiply` tool defined above:\\n\\n```python\\nmultiply.invoke({\"a\": 2, \"b\": 3})\\n```\\n\\n### Inspect\\n\\nYou can also inspect the tool\\'s schema and other properties:'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\"```python\\nprint(multiply.name) # multiply\\nprint(multiply.description) # Multiply two numbers.\\nprint(multiply.args) \\n# {\\n# 'type': 'object', \\n# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, \\n# 'required': ['a', 'b']\\n# }\\n```\\n\\n:::note\\nIf you're using pre-built LangChain or LangGraph components like [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent),you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.\\n:::\\n\\n## Configuring the schema\\n\\nThe `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description\\nor parse the function's doc-string to infer the schema).\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\"Please see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.\\n\\n## Tool artifacts\\n\\n**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='```python\\n@tool(response_format=\"content_and_artifact\")\\ndef some_tool(...) -> Tuple[str, Any]:\\n    \"\"\"Tool that does something.\"\"\"\\n    ...\\n    return \\'Message for chat model\\', some_artifact \\n```\\n\\nSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.\\n\\n## Special type annotations\\n\\nThere are a number of special type annotations that can be used in the tool\\'s function signature to configure the run time behavior of the tool.\\n\\nThe following type annotations will end up **removing** the argument from the tool\\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\\n\\n- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.\\n- **RunnableConfig**: Pass in the RunnableConfig object to the tool.\\n- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.\\n- **InjectedStore**: Pass in the LangGraph store object to the tool.'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\\'s schema.\\n\\n- **Annotated[..., \"string literal\"]** -- Adds a description to the argument that will be exposed in the tool\\'s schema.\\n\\n### InjectedToolArg\\n\\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool\\'s schema.\\n\\nFor example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:\\n\\n```python\\nfrom langchain_core.tools import tool, InjectedToolArg\\n\\n@tool\\ndef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\\n    \"\"\"Tool that processes input data.\"\"\"\\n    return f\"User {user_id} processed {input_data}\"\\n```'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='Annotating the `user_id` argument with `InjectedToolArg` tells LangChain that this argument should not be exposed as part of the\\ntool\\'s schema.\\n\\nSee [how to pass run time values to tools](/docs/how_to/tool_runtime/) for more details on how to use `InjectedToolArg`.  \\n\\n\\n### RunnableConfig\\n\\nYou can use the `RunnableConfig` object to pass custom run time values to tools.\\n\\nIf you need to access the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) object from within a tool. This can be done by using the `RunnableConfig` annotation in the tool\\'s function signature.\\n\\n```python\\nfrom langchain_core.runnables import RunnableConfig\\n\\n@tool\\nasync def some_func(..., config: RunnableConfig) -> ...:\\n    \"\"\"Tool that does something.\"\"\"\\n    # do something with config\\n    ...\\n\\nawait some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})\\n```\\n\\nThe `config` will not be part of the tool\\'s schema and will be injected at runtime with appropriate values.'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content=\":::note\\nYou may need to access the `config` object to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an [async](/docs/concepts/async) environment and need to manually propagate the `config` object to sub-calls.\\n\\nPlease read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\\n:::\\n\\n### InjectedState\\n\\nPlease see the [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState) documentation for more details.\\n\\n### InjectedStore\\n\\nPlease see the [InjectedStore](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedStore) documentation for more details.\\n\\n## Best practices\\n\\nWhen designing tools to be used by models, keep the following in mind:\"), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='- Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\\n- Design simple and narrowly scoped tools, as they are easier for models to use correctly.\\n- Use chat models that support [tool-calling](/docs/concepts/tool_calling) APIs to take advantage of tools.\\n\\n\\n## Toolkits\\n<span data-heading-keywords=\"toolkit,toolkits\"></span>\\n\\nLangChain has a concept of **toolkits**. This a very thin abstraction that groups tools together that\\nare designed to be used together for specific tasks.\\n\\n### Interface\\n\\nAll Toolkits expose a `get_tools` method which returns a list of tools. You can therefore do:\\n\\n```python\\n# Initialize a toolkit\\ntoolkit = ExampleTookit(...)\\n\\n# Get list of tools\\ntools = toolkit.get_tools()\\n```\\n\\n## Related resources\\n\\nSee the following resources for more information:'), Document(metadata={'source': 'docs/docs/concepts/tools.mdx', 'file_path': 'docs/docs/concepts/tools.mdx', 'file_name': 'tools.mdx', 'file_type': '.mdx'}, page_content='- [API Reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\\n- [How to create custom tools](/docs/how_to/custom_tools/)\\n- [How to pass run time values to tools](/docs/how_to/tool_runtime/)\\n- [All LangChain tool how-to guides](https://docs.langchain.com/docs/how_to/#tools)\\n- [Additional how-to guides that show usage with LangGraph](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/)\\n- Tool integrations, see the [tool integration docs](https://docs.langchain.com/docs/integrations/tools/).'), Document(metadata={'source': 'docs/docs/concepts/tracing.mdx', 'file_path': 'docs/docs/concepts/tracing.mdx', 'file_name': 'tracing.mdx', 'file_type': '.mdx'}, page_content='# Tracing\\n\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nA trace is essentially a series of steps that your application takes to go from input to output.\\nTraces contain individual steps called `runs`. These can be individual calls from a model, retriever,\\ntool, or sub-chains.\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\nFor a deeper dive, check out [this LangSmith conceptual guide](https://docs.smith.langchain.com/concepts/tracing).'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='# Vector stores\\n<span data-heading-keywords=\"vector,vectorstore,vectorstores,vector store,vector stores\"></span>\\n\\n:::info[Prerequisites]\\n\\n* [Embeddings](/docs/concepts/embedding_models/)\\n* [Text splitters](/docs/concepts/text_splitters/)\\n\\n:::\\n:::info[Note]\\n\\nThis conceptual overview focuses on text-based indexing and retrieval for simplicity. \\nHowever, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)\\nand vector stores can be used to store and retrieve a variety of data types beyond text.\\n:::\\n\\n## Overview\\n\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\\n\\nThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n\\n![Vector stores](/img/vectorstores.png)\\n\\n## Integrations\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\n\\nPlease see the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).\\n\\n## Interface\\n\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\\n\\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\\n\\nThe key methods are:\\n\\n- `add_documents`: Add a list of texts to the vector store.\\n- `delete`: Delete a list of documents from the vector store.\\n- `similarity_search`: Search for similar documents to a given query.'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content=\"## Initialization\\n\\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\\n\\nWe will use LangChain's [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) implementation to illustrate the API.\\n\\n```python\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\n# Initialize with an embedding model\\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\\n```\\n\\n## Adding documents\\n\\nTo add documents, use the `add_documents` method.\\n\\nThis API works with a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects.\\n`Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store unstructured text and associated metadata.\\n\\n```python\\nfrom langchain_core.documents import Document\"), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='document_1 = Document(\\n    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\\n    metadata={\"source\": \"tweet\"},\\n)\\n\\ndocument_2 = Document(\\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\\n    metadata={\"source\": \"news\"},\\n)\\n\\ndocuments = [document_1, document_2]\\n\\nvector_store.add_documents(documents=documents)\\n```\\n\\nYou should usually provide IDs for the documents you add to the vector store, so\\nthat instead of adding the same document multiple times, you can update the existing document.\\n\\n```python\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\\n```\\n\\n## Delete\\n\\nTo delete documents, use the `delete` method which takes a list of document IDs to delete.\\n\\n```python\\nvector_store.delete(ids=[\"doc1\"])\\n```\\n\\n## Search'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Vector stores embed and store the documents that added.\\nIf we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.\\nThis captures two important concepts: first, there needs to be a way to measure the similarity between the query and *any* [embedded](/docs/concepts/embedding_models/) document.\\nSecond, there needs to be an algorithm to efficiently perform this similarity search across *all* embedded documents.\\n\\n### Similarity metrics\\n\\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\\n\\n- **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n- **Euclidean Distance**: Measures the straight-line distance between two points.\\n- **Dot Product**: Measures the projection of one vector onto another.'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content=\"The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer\\nto the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\\n\\n:::info[Further reading]\\n\\n* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.\\n* See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\\n* See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.\\n\\n:::\\n\\n### Similarity search\"), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over *all* the embedded documents to find the most similar ones.\\nThere are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search.\\nRegardless of the search algorithm used under the hood, the LangChain vectorstore interface has a `similarity_search` method for all integrations. \\nThis will take the search query, create an embedding, find similar documents, and return them as a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).\\n\\n```python\\nquery = \"my query\"\\ndocs = vectorstore.similarity_search(query)\\n```'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Many vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you are using to see what parameters are supported.\\nAs an example [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search) several parameters that are important general concepts:\\nMany vectorstores support [the `k`](/docs/integrations/vectorstores/pinecone/#query-directly), which controls the number of Documents to return, and `filter`, which allows for filtering documents by metadata.\\n\\n- `query (str) – Text to look up documents similar to.`\\n- `k (int) – Number of Documents to return. Defaults to 4.`\\n- `filter (dict | None) – Dictionary of argument(s) to filter on metadata`\\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='* See the [how-to guide](/docs/how_to/vectorstores/) for more details on how to use the `similarity_search` method.\\n* See the [integrations page](/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similarity_search` method for specific vectorstores.\\n\\n:::\\n\\n### Metadata filtering\\n\\nWhile vectorstore implement a search algorithm to efficiently search over *all* the embedded documents to find the most similar ones, many also support filtering on metadata.\\nMetadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:\\n\\n1. **Semantic search**: Query the unstructured data directly, often via embedding or keyword similarity.\\n2. **Metadata search**: Apply structured query to the metadata, filtering specific documents.\\n\\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Here is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\\n\\n```python\\nvectorstore.similarity_search(\\n    \"LangChain provides abstractions to make working with LLMs easy\",\\n    k=2,\\n    filter={\"source\": \"tweet\"},\\n)\\n```  \\n\\n:::info[Further reading]\\n\\n* See Pinecone\\'s [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\\n* See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\\n\\n:::\\n\\n## Advanced search and retrieval techniques'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. \\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.'), Document(metadata={'source': 'docs/docs/concepts/vectorstores.mdx', 'file_path': 'docs/docs/concepts/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |\\n|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\\n| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                        |'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content=\"1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there's [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\"), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='LangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content=\"Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\"), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content=\"Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\"), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='In the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content=\"While standardization for individual components is useful, we've increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\"), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content=\"The recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt's particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\"), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).'), Document(metadata={'source': 'docs/docs/concepts/why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_name': 'why_langchain.mdx', 'file_type': '.mdx'}, page_content='[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.'), Document(metadata={'source': 'docs/docs/contributing/index.mdx', 'file_path': 'docs/docs/contributing/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\n---\\n# Welcome Contributors\\n\\nHi there! Thank you for your interest in contributing to LangChain.\\nAs an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.\\n\\n## Tutorials\\n\\nMore coming soon! We are working on tutorials to help you make your first contribution to the project.\\n\\n- [**Make your first docs PR**](tutorials/docs.mdx)\\n\\n## How-to Guides\\n\\n- [**Documentation**](how_to/documentation/index.mdx): Help improve our docs, including this one!\\n- [**Code**](how_to/code/index.mdx): Help us write code, fix bugs, or improve our infrastructure.\\n- [**Integrations**](how_to/integrations/index.mdx): Help us integrate with your favorite vendors and tools.\\n- [**Standard Tests**](how_to/integrations/standard_tests): Ensure your integration passes an expected set of tests.\\n\\n## Reference'), Document(metadata={'source': 'docs/docs/contributing/index.mdx', 'file_path': 'docs/docs/contributing/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [**Repository Structure**](reference/repo_structure.mdx): Understand the high level structure of the repository.\\n- [**Review Process**](reference/review_process.mdx): Learn about the review process for pull requests.\\n- [**Frequently Asked Questions (FAQ)**](reference/faq.mdx): Get answers to common questions about contributing.\\n\\n## Community\\n\\n### 💭 GitHub Discussions\\n\\nWe have a [discussions](https://github.com/langchain-ai/langchain/discussions) page where users can ask usage questions, discuss design decisions, and propose new features.\\n\\nIf you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.\\n\\n### 🚩 GitHub Issues\\n\\nOur [issues](https://github.com/langchain-ai/langchain/issues) page is kept up to date with bugs, improvements, and feature requests.'), Document(metadata={'source': 'docs/docs/contributing/index.mdx', 'file_path': 'docs/docs/contributing/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='There is a [taxonomy of labels](https://github.com/langchain-ai/langchain/labels?sort=count-desc)\\nto help with sorting and discovery of issues of interest. Please use these to help\\norganize issues. Check out the [Help Wanted](https://github.com/langchain-ai/langchain/labels/help%20wanted)\\nand [Good First Issue](https://github.com/langchain-ai/langchain/labels/good%20first%20issue)\\ntags for recommendations.\\n\\nIf you start working on an issue, please assign it to yourself.\\n\\nIf you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.\\nIf two issues are related, or blocking, please link them rather than combining them.\\n\\nWe will try to keep these issues as up-to-date as possible, though\\nwith the rapid rate of development in this field some may get out of date.\\nIf you notice this happening, please let us know.\\n\\n### 📢 Community Slack'), Document(metadata={'source': 'docs/docs/contributing/index.mdx', 'file_path': 'docs/docs/contributing/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='We have a [community slack](https://www.langchain.com/join-community) where you can ask questions, get help, and discuss the project with other contributors and users.\\n\\n### 🙋 Getting Help\\n\\nOur goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please\\nask in [community slack](https://www.langchain.com/join-community) or open a [discussion on GitHub](https://github.com/langchain-ai/langchain/discussions).\\n\\nIn a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.\\nIf you are finding these difficult (or even just annoying) to work with, feel free to ask in [community slack](https://www.langchain.com/join-community)!'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='# How to load JSON\\n\\n[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\\n\\n[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\\n\\nLangChain implements a [JSONLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) \\nto convert JSON and JSONL data into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) \\nobjects. It uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content \\nand metadata of the LangChain Document.'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"It uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.\\n\\nHere we will demonstrate: \\n\\n- How to load JSON and JSONL data into the content of a LangChain `Document`;\\n- How to load JSON and JSONL data into metadata associated with a `Document`.\\n\\n\\n```python\\n#!pip install jq\\n```\\n\\n\\n```python\\nfrom langchain_community.document_loaders import JSONLoader\\n```\\n\\n\\n```python\\nimport json\\nfrom pathlib import Path\\nfrom pprint import pprint\\n\\n\\nfile_path='./example_data/facebook_chat.json'\\ndata = json.loads(Path(file_path).read_text())\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```python\\npprint(data)\\n```\\n```output\\n    {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\\n     'is_still_participant': True,\\n     'joinable_mode': {'link': '', 'mode': 1},\\n     'magic_words': [],\\n     'messages': [{'content': 'Bye!',\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675597571851},\\n                  {'content': 'Oh no worries! Bye',\\n                   'sender_name': 'User 1',\\n                   'timestamp_ms': 1675597435669},\\n                  {'content': 'No Im sorry it was my mistake, the blue one is not '\\n                              'for sale',\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675596277579},\\n                  {'content': 'I thought you were selling the blue one!',\\n                   'sender_name': 'User 1',\\n                   'timestamp_ms': 1675595140251},\\n                  {'content': 'Im not interested in this bag. Im interested in the '\\n                              'blue one!',\\n                   'sender_name': 'User 1',\\n                   'timestamp_ms': 1675595109305},\\n                  {'content': 'Here is $129',\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675595068468},\\n                  {'photos': [{'creation_timestamp': 1675595059,\\n                               'uri': 'url_of_some_picture.jpg'}],\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675595060730},\\n                  {'content': 'Online is at least $100',\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675595045152},\\n                  {'content': 'How much do you want?',\\n                   'sender_name': 'User 1',\\n                   'timestamp_ms': 1675594799696},\\n                  {'content': 'Goodmorning! $50 is too low.',\\n                   'sender_name': 'User 2',\\n                   'timestamp_ms': 1675577876645},\\n                  {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\\n                              'me know if you are interested. Thanks!',\\n                   'sender_name': 'User 1',\\n                   'timestamp_ms': 1675549022673}],\\n     'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\\n     'thread_path': 'inbox/User 1 and User 2 chat',\\n     'title': 'User 1 and User 2 chat'}\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"## Using `JSONLoader`\\n\\nSuppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data. This can easily be done through the `JSONLoader` as shown below.\\n\\n\\n### JSON file\\n\\n```python\\nloader = JSONLoader(\\n    file_path='./example_data/facebook_chat.json',\\n    jq_schema='.messages[].content',\\n    text_content=False)\\n\\ndata = loader.load()\\n```\\n\\n\\n```python\\npprint(data)\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}),\\n     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}),\\n     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}),\\n     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}),\\n     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}),\\n     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}),\\n     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}),\\n     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}),\\n     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}),\\n     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}),\\n     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})]\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='### JSON Lines file\\n\\nIf you want to load documents from a JSON Lines file, you pass `json_lines=True`\\nand specify `jq_schema` to extract `page_content` from a single JSON object.\\n\\n```python\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'\\npprint(Path(file_path).read_text())\\n```\\n\\n```output\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'\\n     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'\\n     \\'worries! Bye\"}\\\\n\\'\\n     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'\\n     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\n```\\n\\n```python\\nloader = JSONLoader(\\n    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',\\n    jq_schema=\\'.content\\',\\n    text_content=False,\\n    json_lines=True)\\n\\ndata = loader.load()\\n```\\n\\n```python\\npprint(data)\\n```'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),\\n     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),\\n     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\\n```\\n\\nAnother option is to set `jq_schema='.'` and provide `content_key`:\\n\\n```python\\nloader = JSONLoader(\\n    file_path='./example_data/facebook_chat_messages.jsonl',\\n    jq_schema='.',\\n    content_key='sender_name',\\n    json_lines=True)\\n\\ndata = loader.load()\\n```\\n\\n```python\\npprint(data)\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),\\n     Document(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),\\n     Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]\\n```\\n\\n\\n### JSON file with jq schema `content_key`\\n\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True. \\nEnsure that content_key is compatible and can be parsed using the jq schema.\\n\\n```python\\nfile_path = './sample.json'\\npprint(Path(file_path).read_text())\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='```outputjson\\n    {\"data\": [\\n        {\"attributes\": {\\n            \"message\": \"message1\",\\n            \"tags\": [\\n            \"tag1\"]},\\n        \"id\": \"1\"},\\n        {\"attributes\": {\\n            \"message\": \"message2\",\\n            \"tags\": [\\n            \"tag2\"]},\\n        \"id\": \"2\"}]}\\n```\\n\\n```python\\nloader = JSONLoader(\\n    file_path=file_path,\\n    jq_schema=\".data[]\",\\n    content_key=\".attributes.message\",\\n    is_content_key_jq_parsable=True,\\n)\\n\\ndata = loader.load()\\n```\\n\\n```python\\npprint(data)\\n```\\n\\n```output\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),\\n     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\n```\\n\\n\\n## Extracting metadata\\n\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\n\\nThe following demonstrates how metadata can be extracted using the `JSONLoader`.'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"There are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the `page_content` can be extracted from.\\n\\n```\\n.messages[].content\\n```\\n\\nIn the current example, we have to tell the loader to iterate over the records in the `messages` field. The jq_schema then has to be:\\n\\n```\\n.messages[]\\n```\\n\\nThis allows us to pass the records (dict) into the `metadata_func` that has to be implemented. The `metadata_func` is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final `Document` object.\\n\\nAdditionally, we now have to explicitly specify in the loader, via the `content_key` argument, the key from the record where the value for the `page_content` needs to be extracted from.\\n\\n\\n```python\\n# Define the metadata extraction function.\\ndef metadata_func(record: dict, metadata: dict) -> dict:\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='metadata[\"sender_name\"] = record.get(\"sender_name\")\\n    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\\n\\n    return metadata\\n\\n\\nloader = JSONLoader(\\n    file_path=\\'./example_data/facebook_chat.json\\',\\n    jq_schema=\\'.messages[]\\',\\n    content_key=\"content\",\\n    metadata_func=metadata_func\\n)\\n\\ndata = loader.load()\\n```\\n\\n\\n```python\\npprint(data)\\n```'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),\\n     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),\\n     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),\\n     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),\\n     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),\\n     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),\\n     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),\\n     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),\\n     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),\\n     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),\\n     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='Now, you will see that the documents contain the metadata associated with the content we extracted.\\n\\n## The `metadata_func`\\n\\nAs shown above, the `metadata_func` accepts the default metadata generated by the `JSONLoader`. This allows full control to the user with respect to how the metadata is formatted.\\n\\nFor example, the default metadata contains the `source` and the `seq_num` keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the `metadata_func` to rename the default keys and use the ones from the JSON data.\\n\\nThe example below shows how we can modify the `source` to only contain information of the file source relative to the `langchain` directory.\\n\\n\\n```python\\n# Define the metadata extraction function.\\ndef metadata_func(record: dict, metadata: dict) -> dict:\\n\\n    metadata[\"sender_name\"] = record.get(\"sender_name\")\\n    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='if \"source\" in metadata:\\n        source = metadata[\"source\"].split(\"/\")\\n        source = source[source.index(\"langchain\"):]\\n        metadata[\"source\"] = \"/\".join(source)\\n\\n    return metadata\\n\\n\\nloader = JSONLoader(\\n    file_path=\\'./example_data/facebook_chat.json\\',\\n    jq_schema=\\'.messages[]\\',\\n    content_key=\"content\",\\n    metadata_func=metadata_func\\n)\\n\\ndata = loader.load()\\n```\\n\\n\\n```python\\npprint(data)\\n```'), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),\\n     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),\\n     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),\\n     Document(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),\\n     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),\\n     Document(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),\\n     Document(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),\\n     Document(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),\\n     Document(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),\\n     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),\\n     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\\n```\"), Document(metadata={'source': 'docs/docs/how_to/document_loader_json.mdx', 'file_path': 'docs/docs/how_to/document_loader_json.mdx', 'file_name': 'document_loader_json.mdx', 'file_type': '.mdx'}, page_content='## Common JSON structures with jq schema\\n\\nThe list below provides a reference to the possible `jq_schema` the user can use to extract content from the JSON data depending on the structure.\\n\\n```\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]\\njq_schema   -> \".[].text\"\\n\\nJSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}\\njq_schema   -> \".key[].text\"\\n\\nJSON        -> [\"...\", \"...\", \"...\"]\\njq_schema   -> \".[]\"\\n```'), Document(metadata={'source': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_path': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_name': 'document_loader_office_file.mdx', 'file_type': '.mdx'}, page_content='# How to load Microsoft Office files\\n\\nThe [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\\n\\nThis covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a LangChain \\n[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document)\\nobject that we can use downstream.\\n\\n\\n## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader'), Document(metadata={'source': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_path': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_name': 'document_loader_office_file.mdx', 'file_type': '.mdx'}, page_content='[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning \\nbased service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from\\ndigital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\\n\\nThis [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode=\"single\"` or `mode=\"page\"` to return pure texts in a single page or document split by page.\\n\\n### Prerequisite'), Document(metadata={'source': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_path': 'docs/docs/how_to/document_loader_office_file.mdx', 'file_name': 'document_loader_office_file.mdx', 'file_type': '.mdx'}, page_content='An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don\\'t have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.\\n\\n```python\\n%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence\\n\\nfrom langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\\n\\nfile_path = \"<filepath>\"\\nendpoint = \"<endpoint>\"\\nkey = \"<key>\"\\nloader = AzureAIDocumentIntelligenceLoader(\\n    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=\"prebuilt-layout\"\\n)\\n\\ndocuments = loader.load()\\n```'), Document(metadata={'source': 'docs/docs/how_to/embed_text.mdx', 'file_path': 'docs/docs/how_to/embed_text.mdx', 'file_name': 'embed_text.mdx', 'file_type': '.mdx'}, page_content='# Text embedding models\\n\\n:::info\\nHead to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.\\n:::\\n\\nThe Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\\n\\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.'), Document(metadata={'source': 'docs/docs/how_to/embed_text.mdx', 'file_path': 'docs/docs/how_to/embed_text.mdx', 'file_name': 'embed_text.mdx', 'file_type': '.mdx'}, page_content='The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former, `.embed_documents`, takes as input multiple texts, while the latter, `.embed_query`, takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\\n`.embed_query` will return a list of floats, whereas `.embed_documents` returns a list of lists of floats.\\n\\n## Get started\\n\\n### Setup\\n\\nimport EmbeddingTabs from \"@theme/EmbeddingTabs\";\\n\\n<EmbeddingTabs customVarName=\"embeddings_model\" />\\n\\n### `embed_documents`\\n#### Embed list of texts\\n\\nUse `.embed_documents` to embed a list of strings, recovering a list of embeddings:'), Document(metadata={'source': 'docs/docs/how_to/embed_text.mdx', 'file_path': 'docs/docs/how_to/embed_text.mdx', 'file_name': 'embed_text.mdx', 'file_type': '.mdx'}, page_content='```python\\nembeddings = embeddings_model.embed_documents(\\n    [\\n        \"Hi there!\",\\n        \"Oh, hello!\",\\n        \"What\\'s your name?\",\\n        \"My friends call me World\",\\n        \"Hello World!\"\\n    ]\\n)\\nlen(embeddings), len(embeddings[0])\\n```\\n\\n```output\\n(5, 1536)\\n```\\n\\n\\n### `embed_query`\\n#### Embed single query\\nUse `.embed_query` to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).\\n\\n```python\\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\\nembedded_query[:5]\\n```\\n\\n```output\\n[0.0053587136790156364,\\n -0.0004999046213924885,\\n 0.038883671164512634,\\n -0.003001077566295862,\\n -0.00900818221271038]\\n```'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# How-to guides\\n\\nHere you’ll find answers to “How do I….?” types of questions.\\nThese guides are *goal-oriented* and *concrete*; they're meant to help you complete a specific task.\\nFor conceptual explanations see the [Conceptual guide](/docs/concepts/).\\nFor end-to-end walkthroughs see [Tutorials](/docs/tutorials).\\nFor comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).\\n\\n## Installation\\n\\n- [How to: install LangChain packages](/docs/how_to/installation/)\\n- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility)\\n\\n## Key features\\n\\nThis highlights functionality that is core to using LangChain.\"), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\\n\\n## Components\\n\\nThese are the core building blocks you can use when building applications.\\n\\n### Chat models\\n\\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: do function/tool calling](/docs/how_to/tool_calling)\\n- [How to: get models to return structured output](/docs/how_to/structured_output)\\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\\n- [How to: get log probabilities](/docs/how_to/logprobs)\\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: stream a response back](/docs/how_to/chat_streaming)\\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='### Messages\\n\\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\\n\\n- [How to: trim messages](/docs/how_to/trim_messages/)\\n- [How to: filter messages](/docs/how_to/filter_messages/)\\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\\n\\n### Prompt templates\\n\\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\\n\\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\\n\\n### Example selectors'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\\n\\n- [How to: use example selectors](/docs/how_to/example_selectors)\\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\\n\\n### LLMs\\n\\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: cache model responses](/docs/how_to/llm_caching)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: stream a response back](/docs/how_to/streaming_llm)\\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\\n- [How to: work with local models](/docs/how_to/local_llms)\\n\\n### Output parsers\\n\\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: parse text from message objects](/docs/how_to/output_parser_string)\\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n\\n### Document loaders\\n\\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: load PDF files](/docs/how_to/document_loader_pdf)\\n- [How to: load web pages](/docs/how_to/document_loader_web)\\n- [How to: load CSV data](/docs/how_to/document_loader_csv)\\n- [How to: load data from a directory](/docs/how_to/document_loader_directory)\\n- [How to: load HTML data](/docs/how_to/document_loader_html)\\n- [How to: load JSON data](/docs/how_to/document_loader_json)\\n- [How to: load Markdown data](/docs/how_to/document_loader_markdown)\\n- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n\\n### Text splitters\\n\\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\\n- [How to: split HTML](/docs/how_to/split_html)\\n- [How to: split by character](/docs/how_to/character_text_splitter)\\n- [How to: split code](/docs/how_to/code_splitter)\\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\\n- [How to: split by tokens](/docs/how_to/split_by_token)\\n\\n### Embedding models\\n\\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: embed text data](/docs/how_to/embed_text)\\n- [How to: cache embedding results](/docs/how_to/caching_embeddings)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n\\n### Vector stores\\n\\n[Vector stores](/docs/concepts/vectorstores) are databases that can efficiently store and retrieve embeddings.\\nSee [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.\\n\\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores)\\n\\n### Retrievers\\n\\n[Retrievers](/docs/concepts/retrievers) are responsible for taking a query and returning relevant documents.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\\n- [How to: reorder retrieved results to mitigate the \"lost in the middle\" effect](/docs/how_to/long_context_reorder)\\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\\n- [How to: generate metadata filters](/docs/how_to/self_query)\\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='### Indexing\\n\\nIndexing is the process of keeping your vectorstore in-sync with the underlying data source.\\n\\n- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)\\n\\n### Tools\\n\\nLangChain [Tools](/docs/concepts/tools) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-buit tools.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: create tools](/docs/how_to/custom_tools)\\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\\n- [How to: handle tool errors](/docs/how_to/tools_error)\\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"### Multimodal\\n\\n- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)\\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\\n\\n\\n### Agents\\n\\n:::note\\n\\nFor in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.\\n\\n:::\\n\\n- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)\\n- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)\\n\\n### Callbacks\\n\\n[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application's execution.\"), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)\\n- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)\\n- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: use callbacks in async environments](/docs/how_to/callbacks_async)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Custom\\n\\nAll of LangChain components can easily be extended to support your own versions.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\\n- [How to: define a custom tool](/docs/how_to/custom_tools)\\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\\n\\n### Serialization\\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\\n\\n## Use cases\\n\\nThese guides cover use-case specific details.\\n\\n### Q&A with RAG'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\\n\\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\\n- [How to: stream](/docs/how_to/qa_streaming/)\\n- [How to: return sources](/docs/how_to/qa_sources/)\\n- [How to: return citations](/docs/how_to/qa_citations/)\\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\\n\\n\\n### Extraction\\n\\nExtraction is when you use LLMs to extract structured information from unstructured text.\\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\\n\\n- [How to: use reference examples](/docs/how_to/extraction_examples/)\\n- [How to: handle long text](/docs/how_to/extraction_long_text/)\\n- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)\\n\\n### Chatbots'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Chatbots involve using an LLM to have a conversation.\\nFor a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).\\n\\n- [How to: manage memory](/docs/how_to/chatbots_memory)\\n- [How to: do retrieval](/docs/how_to/chatbots_retrieval)\\n- [How to: use tools](/docs/how_to/chatbots_tools)\\n- [How to: manage large chat history](/docs/how_to/trim_messages/)\\n\\n### Query analysis\\n\\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\\n\\n### Q&A over SQL + CSV\\n\\nYou can use LLMs to do question answering over tabular data.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\\n\\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\\n- [How to: do query validation](/docs/how_to/sql_query_checking)\\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\\n\\n### Q&A over graph databases'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='You can use an LLM to do question answering over graph databases.\\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\\n\\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\\n\\n### Summarization\\n\\nLLMs can summarize and otherwise distill desired information from text, including\\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).\\n\\n- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff)\\n- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce)\\n- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine)\\n\\n## LangChain Expression Language (LCEL)\\n\\n:::note Should I use LCEL?\\n\\nLCEL is an orchestration solution. See our\\n[concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to\\nuse LCEL.\\n\\n:::'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='[LangChain Expression Language](/docs/concepts/lcel) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.\\n\\n[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.\\n\\n[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain abstractions to LCEL.'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [How to: chain runnables](/docs/how_to/sequence)\\n- [How to: stream runnables](/docs/how_to/streaming)\\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\\n- [How to: turn any function into a runnable](/docs/how_to/functions)\\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\\n- [How to: route between sub-chains](/docs/how_to/routing)\\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\\n- [How to: inspect runnables](/docs/how_to/inspect)\\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\\n\\n## [LangGraph](https://langchain-ai.github.io/langgraph)'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='LangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n\\nLangGraph documentation is currently hosted on a separate site.\\nYou can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n## [LangSmith](https://docs.smith.langchain.com/)\\n\\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/how_to_guides/), but we\\'ll highlight a few sections that are particularly\\nrelevant to LangChain below:\\n\\n### Evaluation\\n<span data-heading-keywords=\"evaluation,evaluate\"></span>'), Document(metadata={'source': 'docs/docs/how_to/index.mdx', 'file_path': 'docs/docs/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Evaluating performance is a vital part of building LLM-powered applications.\\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\\n\\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\\n\\n### Tracing\\n<span data-heading-keywords=\"trace,tracing\"></span>\\n\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\n\\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\\n\\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).'), Document(metadata={'source': 'docs/docs/how_to/installation.mdx', 'file_path': 'docs/docs/how_to/installation.mdx', 'file_name': 'installation.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 2\\n---\\n\\n# How to install LangChain packages\\n\\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\\nfunctionality to install.\\n\\n## Official release\\n\\nTo install the main `langchain` package, run:\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\nimport CodeBlock from \"@theme/CodeBlock\";\\n\\n<Tabs>\\n  <TabItem value=\"pip\" label=\"Pip\" default>\\n    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\\n  </TabItem>\\n  <TabItem value=\"conda\" label=\"Conda\">\\n    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\\n  </TabItem>\\n</Tabs>\\n\\nWhile this package acts as a sane starting point to using LangChain,\\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.'), Document(metadata={'source': 'docs/docs/how_to/installation.mdx', 'file_path': 'docs/docs/how_to/installation.mdx', 'file_name': 'installation.mdx', 'file_type': '.mdx'}, page_content=\"## Ecosystem packages\\n\\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\\nclasses and abstractions that other packages use. The dependency graph below shows how the different packages are related.\\nA directed arrow indicates that the source package depends on the target package:\\n\\n![](/img/ecosystem_packages.png)\\n\\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\"), Document(metadata={'source': 'docs/docs/how_to/installation.mdx', 'file_path': 'docs/docs/how_to/installation.mdx', 'file_name': 'installation.mdx', 'file_type': '.mdx'}, page_content='### LangChain core\\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\\n\\n```bash\\npip install langchain-core\\n```\\n\\n### Integration packages\\n\\nCertain integrations like OpenAI and Anthropic have their own packages.\\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\\nYou can see a list of all integration packages in the [API reference](https://python.langchain.com/api_reference/) under the \"Partner libs\" dropdown.\\nTo install one of these run:\\n\\n```bash\\npip install langchain-openai\\n```\\n\\nAny integrations that haven\\'t been split out into their own packages will live in the `langchain-community` package. Install with:\\n\\n```bash\\npip install langchain-community\\n```'), Document(metadata={'source': 'docs/docs/how_to/installation.mdx', 'file_path': 'docs/docs/how_to/installation.mdx', 'file_name': 'installation.mdx', 'file_type': '.mdx'}, page_content='### LangChain experimental\\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\\nInstall with:\\n\\n```bash\\npip install langchain-experimental\\n```\\n\\n### LangGraph\\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\\nInstall with:\\n\\n```bash\\npip install langgraph\\n```\\n\\n### LangServe\\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\\nLangServe is automatically installed by LangChain CLI.\\nIf not using LangChain CLI, install with:\\n\\n```bash\\npip install \"langserve[all]\"\\n```\\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\\n\\n### LangChain CLI\\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\\nInstall with:\\n\\n```bash\\npip install langchain-cli\\n```'), Document(metadata={'source': 'docs/docs/how_to/installation.mdx', 'file_path': 'docs/docs/how_to/installation.mdx', 'file_name': 'installation.mdx', 'file_type': '.mdx'}, page_content='### LangSmith SDK\\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\\n`langchain-core`, and can be installed and used independently if desired.\\nIf you are not using LangChain, you can install it with:\\n\\n```bash\\npip install langsmith\\n```\\n\\n### From source\\n\\nIf you want to install a package from source, you can do so by cloning the [main LangChain repo](https://github.com/langchain-ai/langchain), enter the directory of the package you want to install `PATH/TO/REPO/langchain/libs/{package}`, and run:\\n\\n```bash\\npip install -e .\\n```\\n\\nLangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see [all repos here](https://github.com/langchain-ai).'), Document(metadata={'source': 'docs/docs/how_to/toolkits.mdx', 'file_path': 'docs/docs/how_to/toolkits.mdx', 'file_name': 'toolkits.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 3\\n---\\n# How to use toolkits\\n\\n\\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\\n\\nAll Toolkits expose a `get_tools` method which returns a list of tools.\\nYou can therefore do:\\n\\n```python\\n# Initialize a toolkit\\ntoolkit = ExampleTookit(...)\\n\\n# Get list of tools\\ntools = toolkit.get_tools()\\n\\n# Create agent\\nagent = create_agent_method(llm, tools, prompt)\\n```'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content=\"# How to create and query vector stores\\n\\n:::info\\nHead to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores.\\n:::\\n\\nOne of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding\\nvectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are\\n'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search\\nfor you.\\n\\n## Get started\\n\\nThis guide showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them,\\nwhich is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the [text embedding model interfaces](/docs/how_to/embed_text) before diving into this.\\n\\nBefore using the vectorstore at all, we need to load some data and initialize an embedding model.\"), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\n```python\\nimport os\\nimport getpass\\n\\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\\'OpenAI API Key:\\')\\n```\\n\\n```python\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import CharacterTextSplitter\\n\\n# Load the document, split it into chunks, embed each chunk and load it into the vector store.\\nraw_documents = TextLoader(\\'state_of_the_union.txt\\').load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocuments = text_splitter.split_documents(raw_documents)\\n```\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\n\\nThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.\\n\\n\\n<Tabs>\\n  <TabItem value=\"chroma\" label=\"Chroma\" default>'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='This walkthrough uses the `chroma` vector database, which runs on your local machine as a library.\\n\\n```bash\\npip install langchain-chroma\\n```\\n\\n```python\\nfrom langchain_chroma import Chroma\\n\\ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\\n```\\n\\n  </TabItem>\\n  <TabItem value=\"faiss\" label=\"FAISS\">\\n\\nThis walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library.\\n\\n```bash\\npip install faiss-cpu\\n```\\n\\n```python\\nfrom langchain_community.vectorstores import FAISS\\n\\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\\n```\\n\\n  </TabItem>\\n  <TabItem value=\"lance\" label=\"Lance\">\\n\\nThis notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format.\\n\\n```bash\\npip install lancedb\\n```\\n\\n```python\\nfrom langchain_community.vectorstores import LanceDB\\n\\nimport lancedb'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='db = lancedb.connect(\"/tmp/lancedb\")\\ntable = db.create_table(\\n    \"my_table\",\\n    data=[\\n        {\\n            \"vector\": embeddings.embed_query(\"Hello World\"),\\n            \"text\": \"Hello World\",\\n            \"id\": \"1\",\\n        }\\n    ],\\n    mode=\"overwrite\",\\n)\\ndb = LanceDB.from_documents(documents, OpenAIEmbeddings())\\n```\\n\\n  </TabItem>\\n</Tabs>\\n\\n\\n## Similarity search\\n\\nAll vectorstores expose a `similarity_search` method.\\nThis will take incoming documents, create an embedding of them, and then find all documents with the most similar embedding.\\n\\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\ndocs = db.similarity_search(query)\\nprint(docs[0].page_content)\\n```\\n\\n```output\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\n\\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\\n\\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\n```\\n\\n\\n### Similarity search by vector\\n\\nIt is also possible to do a search for documents similar to a given embedding vector using `similarity_search_by_vector` which accepts an embedding vector as a parameter instead of a string.\\n\\n```python\\nembedding_vector = OpenAIEmbeddings().embed_query(query)\\ndocs = db.similarity_search_by_vector(embedding_vector)\\nprint(docs[0].page_content)\\n```'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content='```output\\n    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.\\n\\n    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\\n\\n    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\\n\\n    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\n```\\n\\n\\n## Async Operations'), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content=\"Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as [FastAPI](https://fastapi.tiangolo.com/).\\n\\nLangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix `a`, meaning `async`.\\n\\n```python\\ndocs = await db.asimilarity_search(query)\\ndocs\\n```\"), Document(metadata={'source': 'docs/docs/how_to/vectorstores.mdx', 'file_path': 'docs/docs/how_to/vectorstores.mdx', 'file_name': 'vectorstores.mdx', 'file_type': '.mdx'}, page_content=\"```output\\n[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),\\n Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\n\\\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\\\n\\\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\\\n\\\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\\\n\\\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\\\n\\\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': 'state_of_the_union.txt'}),\\n Document(page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\\\n\\\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\\\n\\\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\\\n\\\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\\\n\\\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\\\n\\\\nFirst, beat the opioid epidemic.', metadata={'source': 'state_of_the_union.txt'}),\\n Document(page_content='Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\\\n\\\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\\\n\\\\nThat ends on my watch. \\\\n\\\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\\\n\\\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\\\n\\\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\\\n\\\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\\\n\\\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.', metadata={'source': 'state_of_the_union.txt'})]\\n```\"), Document(metadata={'source': 'docs/docs/tutorials/index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n# Tutorials\\n\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\n\\n## Get started\\n\\nFamiliarize yourself with LangChain's open-source components by building simple applications.\\n\\nIf you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).\"), Document(metadata={'source': 'docs/docs/tutorials/index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).\\n- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\\n- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).\\n- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).\\n\\nRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.\\n\\n## Orchestration'), Document(metadata={'source': 'docs/docs/tutorials/index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.'), Document(metadata={'source': 'docs/docs/tutorials/index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- [Chatbots](/docs/tutorials/chatbot): Build a chatbot that incorporates memory.\\n- [Agents](/docs/tutorials/agents): Build an agent that interacts with external tools.\\n- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag): Build an application that uses your own documents to inform its responses.\\n- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.\\n- [Question-Answering with SQL](/docs/tutorials/sql_qa): Build a question-answering system that executes SQL queries to inform its responses.\\n- [Summarization](/docs/tutorials/summarization): Generate summaries of (potentially long) texts.\\n- [Question-Answering with Graph Databases](/docs/tutorials/graph): Build a question-answering system that queries a graph database to inform its responses.\\n\\n## LangSmith'), Document(metadata={'source': 'docs/docs/tutorials/index.mdx', 'file_path': 'docs/docs/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='LangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\\n\\nLangSmith documentation is hosted on a separate site.\\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/tutorials/).\\n\\n### Evaluation\\n\\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\\n\\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 2\\nsidebar_label: Release policy\\n---\\n\\n# LangChain release policy\\n\\nThe LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, `langgraph`, `langserve`, partner packages etc.)\\n\\n## Versioning\\n\\n### `langchain`, `langchain-core`, and integration packages\\n\\n`langchain`, `langchain-core`, `langchain-text-splitters`, and integration packages (`langchain-openai`, `langchain-anthropic`, etc.) follow [semantic versioning](https://semver.org/) in the format of 0.**Y**.**Z**. The packages are under rapid development, and so are currently versioning the packages with a major version of 0.\\n\\nMinor version increases will occur for:\\n\\n- Breaking changes for any public interfaces *not* marked as `beta`.\\n\\nPatch version increases will occur for:\\n\\n- Bug fixes,\\n- New features,\\n- Any changes to private interfaces,\\n- Any changes to `beta` features.'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='When upgrading between minor versions, users should review the list of breaking changes and deprecations.\\n\\nFrom time to time, we will version packages as **release candidates**. These are versions that are intended to be released as stable versions, but we want to get feedback from the community before doing so. Release candidates will be versioned as 0.**Y**.**Z**rc**N**. For example, 0.2.0rc1. If no issues are found, the release candidate will be released as a stable version with the same version number. If issues are found, we will release a new release candidate with an incremented `N` value (e.g., 0.2.0rc2).\\n\\n### `langchain-community`\\n\\n`langchain-community` is currently on version `0.2.x`.\\n\\nMinor version increases will occur for:\\n\\n- Updates to the major/minor versions of required `langchain-x` dependencies. E.g., when updating the required version of `langchain-core` from `^0.2.x` to `0.3.0`.\\n\\nPatch version increases will occur for:'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='- Bug fixes,\\n- New features,\\n- Any changes to private interfaces,\\n- Any changes to `beta` features,\\n- Breaking changes to integrations to reflect breaking changes in the third-party service.\\n\\nWhenever possible we will avoid making breaking changes in patch versions.\\nHowever, if an external API makes a breaking change then breaking changes to the corresponding `langchain-community` integration can occur in a patch version.\\n\\n### `langchain-experimental`\\n\\n`langchain-experimental` is currently on version `0.0.x`. All changes will be accompanied with patch version increases.\\n\\n## Release cadence\\n\\nWe expect to space out **minor** releases (e.g., from 0.2.x to 0.3.0) of `langchain` and `langchain-core` by at least 2-3 months, as such releases may contain breaking changes.\\n\\nPatch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.\\n\\n## API stability'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.\\n\\nEven though both `langchain` and `langchain-core` are currently in a pre-1.0 state, we are committed to maintaining API stability in these packages.\\n\\n- Breaking changes to the public API will result in a minor version bump (the second digit)\\n- Any bug fixes or new features will result in a patch version bump (the third digit)\\n\\nWe will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.\\n\\n### Stability of other packages\\n\\nThe stability of other packages in the LangChain ecosystem may vary:'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='- `langchain-community` is a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in `langchain-community`, `langchain-community` is expected to experience more breaking changes than `langchain` and `langchain-core` as it contains many community contributions.\\n- Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.\\n\\n### What is a \"API stability\"?\\n\\nAPI stability means:'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='- All the public APIs (everything in this documentation) will not be moved or renamed without providing backwards-compatible aliases.\\n- If new features are added to these APIs – which is quite possible – they will not break or change the meaning of existing methods. In other words, \"stable\" does not (necessarily) mean \"complete.\"\\n- If, for some reason, an API declared stable must be removed or replaced, it will be declared deprecated but will remain in the API for at least two minor releases. Warnings will be issued when the deprecated method is called.\\n\\n### **APIs marked as internal**\\n\\nCertain APIs are explicitly marked as “internal” in a couple of ways:'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content='- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.\\n- Functions, methods, and other objects prefixed by a leading underscore (**`_`**). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`**, it’s an internal API.\\n    - **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are *meant* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.\\n\\n## Deprecation policy\\n\\nWe will generally avoid deprecating features until a better alternative is available.\\n\\nWhen a feature is deprecated, it will continue to work in the current and next minor version of `langchain` and `langchain-core`. After that, the feature will be removed.'), Document(metadata={'source': 'docs/docs/versions/release_policy.mdx', 'file_path': 'docs/docs/versions/release_policy.mdx', 'file_name': 'release_policy.mdx', 'file_type': '.mdx'}, page_content=\"Since we're expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.\\n\\nIn some situations, we may allow the feature to remain in the code base for longer periods of time, if it's not causing issues in the packages, to reduce the burden on users.\"), Document(metadata={'source': 'docs/docs/changes/changelog/core.mdx', 'file_path': 'docs/docs/changes/changelog/core.mdx', 'file_name': 'core.mdx', 'file_type': '.mdx'}, page_content='# langchain-core\\n\\n## 0.1.x\\n\\n#### Deprecated\\n\\n- `BaseChatModel` methods `__call__`, `call_as_llm`, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.invoke` instead.\\n- `BaseChatModel` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseChatModel.ainvoke` instead.\\n- `BaseLLM` methods `__call__, `predict`, `predict_messages`. Will be removed in 0.2.0. Use `BaseLLM.invoke` instead.\\n- `BaseLLM` methods `apredict`, `apredict_messages`. Will be removed in 0.2.0. Use `BaseLLM.ainvoke` instead.'), Document(metadata={'source': 'docs/docs/changes/changelog/langchain.mdx', 'file_path': 'docs/docs/changes/changelog/langchain.mdx', 'file_name': 'langchain.mdx', 'file_type': '.mdx'}, page_content='# langchain\\n\\n## 0.2.0\\n\\n### Deleted\\n\\nAs of release 0.2.0, `langchain` is required to be integration-agnostic. This means that code in `langchain`  should not by default instantiate any specific chat models, llms, embedding models, vectorstores etc; instead, the user will be required to specify those explicitly.\\n\\nThe following functions and classes require an explicit LLM to be passed as an argument:\\n\\n- `langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit`\\n- `langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit`\\n- `langchain.chains.openai_functions.get_openapi_chain`\\n- `langchain.chains.router.MultiRetrievalQAChain.from_retrievers`\\n- `langchain.indexes.VectorStoreIndexWrapper.query`\\n- `langchain.indexes.VectorStoreIndexWrapper.query_with_sources`\\n- `langchain.indexes.VectorStoreIndexWrapper.aquery_with_sources`\\n- `langchain.chains.flare.FlareChain`\\n\\nThe following classes now require passing an explicit Embedding model as an argument:'), Document(metadata={'source': 'docs/docs/changes/changelog/langchain.mdx', 'file_path': 'docs/docs/changes/changelog/langchain.mdx', 'file_name': 'langchain.mdx', 'file_type': '.mdx'}, page_content='- `langchain.indexes.VectostoreIndexCreator`\\n\\nThe following code has been removed:\\n\\n- `langchain.natbot.NatBotChain.from_default` removed in favor of the `from_llm` class method.\\n\\n### Deprecated\\n\\nWe have two main types of deprecations:\\n\\n1. Code that was moved from `langchain` into another package (e.g, `langchain-community`)\\n\\nIf you try to import it from `langchain`, the import will keep on working, but will raise a deprecation warning. The warning will provide a replacement import statement.\\n\\n```python\\npython -c \"from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\"\\n\\n```\\n\\n```python\\nLangChainDeprecationWarning: Importing UnstructuredMarkdownLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\\n\\n>> from langchain.document_loaders import UnstructuredMarkdownLoader\\n\\nwith new imports of:\\n\\n>> from langchain_community.document_loaders import UnstructuredMarkdownLoader\\n```'), Document(metadata={'source': 'docs/docs/changes/changelog/langchain.mdx', 'file_path': 'docs/docs/changes/changelog/langchain.mdx', 'file_name': 'langchain.mdx', 'file_type': '.mdx'}, page_content='We will continue supporting the imports in `langchain` until release 0.4 as long as the relevant package where the code lives is installed. (e.g., as long as `langchain_community` is installed.)\\n\\nHowever, we advise for users to not rely on these imports and instead migrate to the new imports. To help with this process, we’re releasing a migration script via the LangChain CLI. See further instructions in migration guide.\\n\\n1. Code that has better alternatives available and will eventually be removed, so there’s only a single way to do things. (e.g., `predict_messages` method in ChatModels has been deprecated in favor of `invoke`).\\n\\nMany of these were marked for removal in 0.2. We have bumped the removal to 0.3.\\n\\n\\n## 0.1.0 (Jan 5, 2024)\\n\\n### Deleted\\n\\nNo deletions.\\n\\n### Deprecated\\n\\nDeprecated classes and methods will be removed in 0.2.0'), Document(metadata={'source': 'docs/docs/changes/changelog/langchain.mdx', 'file_path': 'docs/docs/changes/changelog/langchain.mdx', 'file_name': 'langchain.mdx', 'file_type': '.mdx'}, page_content='| Deprecated                      | Alternative                       | Reason                                         |\\n|---------------------------------|-----------------------------------|------------------------------------------------|\\n| ChatVectorDBChain               | ConversationalRetrievalChain      | More general to all retrievers                 |\\n| create_ernie_fn_chain           | create_ernie_fn_runnable          | Use LCEL under the hood                        |\\n| created_structured_output_chain | create_structured_output_runnable | Use LCEL under the hood                        |\\n| NatBotChain                     |                                   | Not used                                       |\\n| create_openai_fn_chain          | create_openai_fn_runnable         | Use LCEL under the hood                        |\\n| create_structured_output_chain  | create_structured_output_runnable | Use LCEL under the hood                        |\\n| load_query_constructor_chain    | load_query_constructor_runnable   | Use LCEL under the hood                        |\\n| VectorDBQA                      | RetrievalQA                       | More general to all retrievers                 |\\n| Sequential Chain                | LCEL                              | Obviated by LCEL                               |\\n| SimpleSequentialChain           | LCEL                              | Obviated by LCEL                               |\\n| TransformChain                  | LCEL/RunnableLambda               | Obviated by LCEL                               |\\n| create_tagging_chain            | create_structured_output_runnable | Use LCEL under the hood                        |\\n| ChatAgent                       | create_react_agent                | Use LCEL builder over a class                  |\\n| ConversationalAgent             | create_react_agent                | Use LCEL builder over a class                  |\\n| ConversationalChatAgent         | create_json_chat_agent            | Use LCEL builder over a class                  |\\n| initialize_agent                | Individual create agent methods   | Individual create agent methods are more clear |\\n| ZeroShotAgent                   | create_react_agent                | Use LCEL builder over a class                  |\\n| OpenAIFunctionsAgent            | create_openai_functions_agent     | Use LCEL builder over a class                  |\\n| OpenAIMultiFunctionsAgent       | create_openai_tools_agent         | Use LCEL builder over a class                  |\\n| SelfAskWithSearchAgent          | create_self_ask_with_search       | Use LCEL builder over a class                  |\\n| StructuredChatAgent             | create_structured_chat_agent      | Use LCEL builder over a class                  |\\n| XMLAgent                        | create_xml_agent                  | Use LCEL builder over a class                  |'), Document(metadata={'source': 'docs/docs/contributing/how_to/index.mdx', 'file_path': 'docs/docs/contributing/how_to/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# How-to Guides\\n\\n- [**Documentation**](documentation/index.mdx): Help improve our docs, including this one!\\n- [**Code**](code/index.mdx): Help us write code, fix bugs, or improve our infrastructure.\\n\\n## Integrations\\n\\n- [**Start Here**](integrations/index.mdx): Help us integrate with your favorite vendors and tools.\\n- [**Package**](integrations/package): Publish an integration package to PyPi\\n- [**Standard Tests**](integrations/standard_tests): Ensure your integration passes an expected set of tests.'), Document(metadata={'source': 'docs/docs/contributing/how_to/testing.mdx', 'file_path': 'docs/docs/contributing/how_to/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 6\\n---\\n\\n# Testing\\n\\nAll of our packages have unit tests and integration tests, and we favor unit tests over integration tests.\\n\\nUnit tests run on every pull request, so they should be fast and reliable.\\n\\nIntegration tests run once a day, and they require more setup, so they should be reserved for confirming interface points with external services.\\n\\n## Unit Tests\\n\\nUnit tests cover modular logic that does not require calls to outside APIs.\\nIf you add new logic, please add a unit test.\\n\\nTo install dependencies for unit tests:\\n\\n```bash\\npoetry install --with test\\n```\\n\\nTo run unit tests:\\n\\n```bash\\nmake test\\n```\\n\\nTo run unit tests in Docker:\\n\\n```bash\\nmake docker_tests\\n```\\n\\nTo run a specific test:\\n\\n```bash\\nTEST_FILE=tests/unit_tests/test_imports.py make test\\n```\\n\\n## Integration Tests\\n\\nIntegration tests cover logic that requires making calls to outside APIs (often integration with other services).\\nIf you add support for a new external API, please add a new integration test.'), Document(metadata={'source': 'docs/docs/contributing/how_to/testing.mdx', 'file_path': 'docs/docs/contributing/how_to/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='**Warning:** Almost no tests should be integration tests.\\n\\n  Tests that require making network connections make it difficult for other\\n  developers to test the code.\\n\\n  Instead favor relying on `responses` library and/or mock.patch to mock\\n  requests using small fixtures.\\n\\nTo install dependencies for integration tests:\\n\\n```bash\\npoetry install --with test,test_integration\\n```\\n\\nTo run integration tests:\\n\\n```bash\\nmake integration_tests\\n```\\n\\n### Prepare\\n\\nThe integration tests use several search engines and databases. The tests\\naim to verify the correct behavior of the engines and databases according to\\ntheir specifications and requirements.\\n\\nTo run some integration tests, such as tests located in\\n`tests/integration_tests/vectorstores/`, you will need to install the following\\nsoftware:\\n\\n- Docker\\n- Python 3.8.1 or later\\n\\nAny new dependencies should be added by running:'), Document(metadata={'source': 'docs/docs/contributing/how_to/testing.mdx', 'file_path': 'docs/docs/contributing/how_to/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='```bash\\n# add package and install it after adding:\\npoetry add tiktoken@latest --group \"test_integration\" && poetry install --with test_integration\\n```\\n\\nBefore running any tests, you should start a specific Docker container that has all the\\nnecessary dependencies installed. For instance, we use the `elasticsearch.yml` container\\nfor `test_elasticsearch.py`:\\n\\n```bash\\ncd tests/integration_tests/vectorstores/docker-compose\\ndocker-compose -f elasticsearch.yml up\\n```\\n\\nFor environments that requires more involving preparation, look for `*.sh`. For instance,\\n`opensearch.sh` builds a required docker image and then launch opensearch.\\n\\n\\n### Prepare environment variables for local testing:\\n\\n- copy `tests/integration_tests/.env.example` to `tests/integration_tests/.env`\\n- set variables in `tests/integration_tests/.env` file, e.g `OPENAI_API_KEY`'), Document(metadata={'source': 'docs/docs/contributing/how_to/testing.mdx', 'file_path': 'docs/docs/contributing/how_to/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content=\"Additionally, it's important to note that some integration tests may require certain\\nenvironment variables to be set, such as `OPENAI_API_KEY`. Be sure to set any required\\nenvironment variables before running the tests to ensure they run correctly.\\n\\n### Recording HTTP interactions with pytest-vcr\\n\\nSome of the integration tests in this repository involve making HTTP requests to\\nexternal services. To prevent these requests from being made every time the tests are\\nrun, we use pytest-vcr to record and replay HTTP interactions.\\n\\nWhen running tests in a CI/CD pipeline, you may not want to modify the existing\\ncassettes. You can use the --vcr-record=none command-line option to disable recording\\nnew cassettes. Here's an example:\\n\\n```bash\\npytest --log-cli-level=10 tests/integration_tests/vectorstores/test_pinecone.py --vcr-record=none\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --vcr-record=none\\n\\n```\\n\\n### Run some tests with coverage:\"), Document(metadata={'source': 'docs/docs/contributing/how_to/testing.mdx', 'file_path': 'docs/docs/contributing/how_to/testing.mdx', 'file_name': 'testing.mdx', 'file_type': '.mdx'}, page_content='```bash\\npytest tests/integration_tests/vectorstores/test_elasticsearch.py --cov=langchain --cov-report=html\\nstart \"\" htmlcov/index.html || open htmlcov/index.html\\n\\n```\\n\\n## Coverage\\n\\nCode coverage (i.e. the amount of code that is covered by unit tests) helps identify areas of the code that are potentially more or less brittle.\\n\\nCoverage requires the dependencies for integration tests:\\n\\n```bash\\npoetry install --with test_integration\\n```\\n\\nTo get a report of current coverage, run the following:\\n\\n```bash\\nmake coverage\\n```'), Document(metadata={'source': 'docs/docs/contributing/reference/faq.mdx', 'file_path': 'docs/docs/contributing/reference/faq.mdx', 'file_name': 'faq.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 6\\nsidebar_label: FAQ\\n---\\n# Frequently Asked Questions\\n\\n## Pull Requests (PRs)\\n\\n### How do I allow maintainers to edit my PR?\\n\\nWhen you submit a pull request, there may be additional changes\\nnecessary before merging it. Oftentimes, it is more efficient for the\\nmaintainers to make these changes themselves before merging, rather than asking you\\nto do so in code review.\\n\\nBy default, most pull requests will have a \\n`✅ Maintainers are allowed to edit this pull request.`\\nbadge in the right-hand sidebar.\\n\\nIf you do not see this badge, you may have this setting off for the fork you are\\npull-requesting from. See [this Github docs page](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork)\\nfor more information.'), Document(metadata={'source': 'docs/docs/contributing/reference/faq.mdx', 'file_path': 'docs/docs/contributing/reference/faq.mdx', 'file_name': 'faq.mdx', 'file_type': '.mdx'}, page_content=\"Notably, Github doesn't allow this setting to be enabled for forks in **organizations** ([issue](https://github.com/orgs/community/discussions/5634)).\\nIf you are working in an organization, we recommend submitting your PR from a personal\\nfork in order to enable this setting.\\n\\n### Why hasn't my PR been reviewed?\\n\\nPlease reference our [Review Process](review_process.mdx).\\n\\n### Why was my PR closed?\\n\\nPlease reference our [Review Process](review_process.mdx).\\n\\n### I think my PR was closed in a way that didn't follow the review process. What should I do?\\n\\nTag `@ccurme` in the PR comments referencing the portion of the review\\nprocess that you believe was not followed. We'll take a look!\"), Document(metadata={'source': 'docs/docs/contributing/reference/index.mdx', 'file_path': 'docs/docs/contributing/reference/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Reference\\n\\n- [**Repository Structure**](repo_structure.mdx): Understand the high level structure of the repository.\\n- [**Review Process**](review_process.mdx): Learn about the review process for pull requests.\\n- [**Frequently Asked Questions (FAQ)**](faq.mdx): Get answers to common questions about contributing.'), Document(metadata={'source': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_path': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_name': 'repo_structure.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 0.5\\n---\\n# Repository Structure\\n\\nIf you plan on contributing to LangChain code or documentation, it can be useful\\nto understand the high level structure of the repository.\\n\\nLangChain is organized as a [monorepo](https://en.wikipedia.org/wiki/Monorepo) that contains multiple packages.\\nYou can check out our [installation guide](/docs/how_to/installation/) for more on how they fit together.\\n\\nHere's the structure visualized as a tree:\"), Document(metadata={'source': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_path': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_name': 'repo_structure.mdx', 'file_type': '.mdx'}, page_content='```text\\n.\\n├── cookbook # Tutorials and examples\\n├── docs # Contains content for the documentation here: https://python.langchain.com/\\n├── libs\\n│   ├── langchain\\n│   │   ├── langchain\\n│   │   ├── tests/unit_tests # Unit tests (present in each package not shown for brevity)\\n│   │   ├── tests/integration_tests # Integration tests (present in each package not shown for brevity)\\n│   ├── community # Third-party integrations\\n│   │   ├── langchain-community\\n│   ├── core # Base interfaces for key abstractions\\n│   │   ├── langchain-core\\n│   ├── experimental # Experimental components and chains\\n│   │   ├── langchain-experimental\\n|   ├── cli # Command line interface\\n│   │   ├── langchain-cli\\n│   ├── text-splitters\\n│   │   ├── langchain-text-splitters\\n│   ├── standard-tests\\n│   │   ├── langchain-standard-tests\\n│   ├── partners\\n│       ├── langchain-partner-1\\n│       ├── langchain-partner-2\\n│       ├── ...\\n│\\n├── templates # A collection of easily deployable reference architectures for a wide variety of tasks.\\n```'), Document(metadata={'source': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_path': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_name': 'repo_structure.mdx', 'file_type': '.mdx'}, page_content='The root directory also contains the following files:\\n\\n* `pyproject.toml`: Dependencies for building docs and linting docs, cookbook.\\n* `Makefile`: A file that contains shortcuts for building, linting and docs and cookbook.\\n\\nThere are other files in the root directory level, but their presence should be self-explanatory. Feel free to browse around!\\n\\n## Documentation\\n\\nThe `/docs` directory contains the content for the documentation that is shown\\nat https://python.langchain.com/ and the associated API Reference https://python.langchain.com/api_reference/langchain/index.html.\\n\\nSee the [documentation](../how_to/documentation/index.mdx) guidelines to learn how to contribute to the documentation.\\n\\n## Code\\n\\nThe `/libs` directory contains the code for the LangChain packages.\\n\\nTo learn more about how to contribute code see the following guidelines:'), Document(metadata={'source': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_path': 'docs/docs/contributing/reference/repo_structure.mdx', 'file_name': 'repo_structure.mdx', 'file_type': '.mdx'}, page_content='- [Code](../how_to/code/index.mdx): Learn how to develop in the LangChain codebase.\\n- [Integrations](../how_to/integrations/index.mdx): Learn how to contribute to third-party integrations to `langchain-community` or to start a new partner package.\\n- [Testing](../how_to/testing.mdx): Guidelines to learn how to write tests for the packages.'), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content='# Review Process\\n\\n## Overview\\n\\nThis document outlines the process used by the LangChain maintainers for reviewing pull requests (PRs). The primary objective of this process is to enhance the LangChain developer experience.\\n\\n## Review Statuses\\n\\nWe categorize PRs using three main statuses, which are marked as project item statuses in the right sidebar and can be viewed in detail [here](https://github.com/orgs/langchain-ai/projects/12/views/1).\\n\\n- **Triage**: \\n  - Initial status for all newly submitted PRs.\\n  - Requires a maintainer to categorize it into one of the other statuses.'), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content='- **Needs Support**:\\n  - PRs that require community feedback or additional input before moving forward.\\n  - Automatically promoted to the backlog if it receives 5 upvotes.\\n  - An auto-comment is generated when this status is applied, explaining the flow and the upvote requirement.\\n  - If the PR remains in this status for 25 days, it will be marked as “stale” via auto-comment.\\n  - PRs will be auto-closed after 30 days if no further action is taken.\\n\\n- **In Review**:\\n  - PRs that are actively under review by our team.\\n  - These are regularly reviewed and monitored.\\n\\n**Note:** A PR may only have one status at a time.\\n\\n**Note:** You may notice 3 additional statuses of Done, Closed, and Internal that\\nare external to this lifecycle. Done and Closed PRs have been merged or closed,\\nrespectively. Internal is for PRs submitted by core maintainers, and these PRs are owned\\nby the submitter.\\n\\n## Review Guidelines'), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content=\"1. **PRs that touch /libs/core**:\\n   - PRs that directly impact core code and are likely to affect end users.\\n   - **Triage Guideline**: most PRs should either go straight to `In Review` or closed.\\n   - These PRs are given top priority and are reviewed the fastest.\\n   - PRs that don't have a **concise** descriptions of their motivation (either in PR summary or in a linked issue) are likely to be closed without an in-depth review. Please do not generate verbose PR descriptions with an LLM.\\n   - PRs that don't have unit tests are likely to be closed.\\n   - Feature requests should first be opened as a GitHub issue and discussed with the LangChain maintainers. Large PRs submitted without prior discussion are likely to be closed.\"), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content=\"2. **PRs that touch /libs/langchain**:\\n   - High-impact PRs that are closely related to core PRs but slightly lower in priority.\\n   - **Triage Guideline**: most PRs should either go straight to `In Review` or closed.\\n   - These are reviewed and closed aggressively, similar to core PRs.\\n   - New feature requests should be discussed with the core maintainer team beforehand in an issue.\\n\\n3. **PRs that touch /libs/partners/**:\\n   - PRs involving integration packages.\\n   - **Triage Guideline**: most PRs should either go straight to `In Review` or closed.\\n   - The review may be conducted by our team or handed off to the partner's development team, depending on the PR's content.\\n   - We maintain communication lines with most partner dev teams to facilitate this process.\"), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content='4. **Community PRs**:\\n   - Most community PRs will get an initial status of \"needs support\".\\n   - **Triage Guideline**: most PRs should go to `Needs support`. Bugfixes on high-traffic integrations should go straight to `In review`.\\n   - **Triage Guideline**: all new features and integrations should go to `Needs support` and will be closed if they do not get enough support (measured by upvotes or comments).\\n   - PRs in the `Needs Support` status for 20 days are marked as “stale” and will be closed after 30 days if no action is taken.'), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content=\"5. **Documentation PRs**:\\n   - PRs that touch the documentation content in docs/docs.\\n   - **Triage Guideline**:\\n      - PRs that fix typos or small errors in a single file and pass CI should go straight to `In Review`.\\n      - PRs that make changes that have been discussed and agreed upon in an issue should go straight to `In Review`.\\n      - PRs that add new pages or change the structure of the documentation should go to `Needs Support`.\\n   - We strive to standardize documentation formats to streamline the review process.\\n   - CI jobs run against documentation to ensure adherence to standards, automating much of the review.\\n\\n6. **PRs must be in English**:\\n   - PRs that are not in English will be closed without review.\\n   - This is to ensure that all maintainers can review the PRs effectively.\\n\\n## How to see a PR's status\\n\\nSee screenshot:\\n\\n![PR Status](/img/review_process_status.png)\"), Document(metadata={'source': 'docs/docs/contributing/reference/review_process.mdx', 'file_path': 'docs/docs/contributing/reference/review_process.mdx', 'file_name': 'review_process.mdx', 'file_type': '.mdx'}, page_content='*To see the status of all open PRs, please visit the [LangChain Project Board](https://github.com/orgs/langchain-ai/projects/12/views/2).*\\n\\n## Review Prioritization\\n\\nOur goal is to provide the best possible development experience by focusing on making software that:\\n\\n- Works: Works as intended (is bug-free).\\n- Is useful: Improves LLM app development with components that work off-the-shelf and runtimes that simplify app building.\\n- Is easy: Is intuitive to use and well-documented.\\n\\nWe believe this process reflects our priorities and are open to feedback if you feel it does not.\\n\\n## Github Discussion\\n\\nWe welcome your feedback on this process. Please feel free to add a comment in \\n[this GitHub Discussion](https://github.com/langchain-ai/langchain/discussions/25920).'), Document(metadata={'source': 'docs/docs/contributing/tutorials/docs.mdx', 'file_path': 'docs/docs/contributing/tutorials/docs.mdx', 'file_name': 'docs.mdx', 'file_type': '.mdx'}, page_content='# Make your first docs PR\\n\\nThis tutorial will guide you through making a simple documentation edit, like correcting a typo.\\n\\n### **Prerequisites**\\n- GitHub account.\\n- Familiarity with GitHub pull requests (basic understanding).\\n\\n---\\n\\n## Editing a Documentation Page on GitHub\\n\\nSometimes you want to make a small change, like fixing a typo, and the easiest way to do this is to use GitHub\\'s editor directly.\\n\\n### **Steps**\\n\\n1. **Navigate to the documentation page in the LangChain docs:**\\n   - On the documentation page, find the green \"Edit this page\" link at the bottom of the page.\\n   - Click the button to be directed to the GitHub editor.\\n   - If the file you\\'re editing is a Jupyter Notebook (.ipynb) instead of a Markdown (.md, .mdx)\\n        file, we recommend following the steps in section 3.'), Document(metadata={'source': 'docs/docs/contributing/tutorials/docs.mdx', 'file_path': 'docs/docs/contributing/tutorials/docs.mdx', 'file_name': 'docs.mdx', 'file_type': '.mdx'}, page_content='2. **Fork the repository:**\\n   - If you haven\\'t already, GitHub will prompt you to fork the repository to your account.\\n   - Make sure to fork the repository into your **personal account and not an organization** ([why?](../reference/faq.mdx#how-do-i-allow-maintainers-to-edit-my-pr)).\\n   - Click the \"Fork this repository\" button to create a copy of the repository under your account.\\n   - After forking, you\\'ll automatically be redirected to the correct editor.\\n\\n3. **Make your changes:**\\n   - Correct the typo directly in the GitHub editor.\\n\\n4. **Commit your changes:**\\n   - Click the \"Commit changes...\" button at the top-right corner of the page.\\n   - Give your commit a title like \"Fix typo in X section.\"\\n   - Optionally, write an extended commit description.\\n   - Click \"Propose changes\"'), Document(metadata={'source': 'docs/docs/contributing/tutorials/docs.mdx', 'file_path': 'docs/docs/contributing/tutorials/docs.mdx', 'file_name': 'docs.mdx', 'file_type': '.mdx'}, page_content=\"5. **Submit a pull request (PR):**\\n   - GitHub will redirect you to a page where you can create a pull request.\\n   - First, review your proposed changes to ensure they are correct.\\n   - Click **Create pull request**.\\n   - Give your PR a title like `docs: Fix typo in X section`.\\n   - Follow the checklist in the PR description template.\\n\\n## Getting a Review\\n\\nOnce you've submitted the pull request, it will be reviewed by the maintainers. You may receive feedback or requests for changes. Keep an eye on the PR to address any comments.\\n\\nDocs PRs are typically reviewed within a few days, but it may take longer depending on the complexity of the change and the availability of maintainers.\\n\\nFor more information on reviews, see the [Review Process](../reference/review_process.mdx).\"), Document(metadata={'source': 'docs/docs/contributing/tutorials/index.mdx', 'file_path': 'docs/docs/contributing/tutorials/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Tutorials\\n\\nMore coming soon! We are working on tutorials to help you make your first contribution to the project.\\n\\n- [**Make your first docs PR**](docs.mdx)'), Document(metadata={'source': 'docs/docs/integrations/chat/index.mdx', 'file_path': 'docs/docs/integrations/chat/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\nkeywords: [compatibility]\\n---\\n\\n# Chat models\\n\\n[Chat models](/docs/concepts/chat_models) are language models that use a sequence of [messages](/docs/concepts/messages) as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.\\n\\n:::info\\n\\nIf you\\'d like to write your own chat model, see [this how-to](/docs/how_to/custom_chat_model/).\\nIf you\\'d like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).\\n\\n:::\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n## Featured Providers\\n\\n:::info\\nWhile all these LangChain classes support the indicated advanced feature, you may have\\nto open the provider-specific documentation to learn which hosted models or backends support\\nthe feature.\\n:::'), Document(metadata={'source': 'docs/docs/integrations/chat/index.mdx', 'file_path': 'docs/docs/integrations/chat/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='import { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\\n\\n<CategoryTable category=\"chat\" />\\n\\n## All chat models\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/integrations/document_loaders/index.mdx', 'file_path': 'docs/docs/integrations/document_loaders/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Document loaders\\n\\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\\n\\nDocumentLoaders load data into the standard LangChain Document format.\\n\\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:\\n\\n```python\\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\\n\\nloader = CSVLoader(\\n    ...  # <-- Integration specific parameters here\\n)\\ndata = loader.load()\\n```\\n\\n## Webpages\\n\\nThe below document loaders allow you to load webpages.\\n\\nSee this guide for a starting point: [How to: load web pages](/docs/how_to/document_loader_web).\\n\\n<CategoryTable category=\"webpage_loaders\" />\\n\\n## PDFs\\n\\nThe below document loaders allow you to load PDF documents.\\n\\nSee this guide for a starting point: [How to: load PDF files](/docs/how_to/document_loader_pdf).\\n\\n<CategoryTable category=\"pdf_loaders\" />\\n\\n## Cloud Providers'), Document(metadata={'source': 'docs/docs/integrations/document_loaders/index.mdx', 'file_path': 'docs/docs/integrations/document_loaders/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='The below document loaders allow you to load documents from your favorite cloud providers.\\n\\n<CategoryTable category=\"cloud_provider_loaders\"/>\\n\\n## Social Platforms\\n\\nThe below document loaders allow you to load documents from different social media platforms.\\n\\n<CategoryTable category=\"social_loaders\"/>\\n\\n## Messaging Services\\n\\nThe below document loaders allow you to load data from different messaging platforms.\\n\\n<CategoryTable category=\"messaging_loaders\"/>\\n\\n## Productivity tools\\n\\nThe below document loaders allow you to load data from commonly used productivity tools.\\n\\n<CategoryTable category=\"productivity_loaders\"/>\\n\\n## Common File Types\\n\\nThe below document loaders allow you to load data from common data formats.\\n\\n<CategoryTable category=\"common_loaders\" />\\n\\n\\n## All document loaders\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/integrations/graphs/tigergraph.mdx', 'file_path': 'docs/docs/integrations/graphs/tigergraph.mdx', 'file_name': 'tigergraph.mdx', 'file_type': '.mdx'}, page_content='# TigerGraph\\n\\n>[TigerGraph](https://www.tigergraph.com/tigergraph-db/) is a natively distributed and high-performance graph database.\\n> The storage of data in a graph format of vertices and edges leads to rich relationships, \\n> ideal for grouding LLM responses.\\n \\nA big example of the `TigerGraph` and `LangChain` integration [presented here](https://github.com/tigergraph/graph-ml-notebooks/blob/main/applications/large_language_models/TigerGraph_LangChain_Demo.ipynb).\\n\\n## Installation and Setup\\n\\nFollow instructions [how to connect to the `TigerGraph` database](https://docs.tigergraph.com/pytigergraph/current/getting-started/connection).\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install pyTigerGraph\\n```\\n\\n## Example\\n\\nTo utilize the `TigerGraph InquiryAI` functionality, you can import `TigerGraph` from `langchain_community.graphs`.\\n\\n```python\\nimport pyTigerGraph as tg'), Document(metadata={'source': 'docs/docs/integrations/graphs/tigergraph.mdx', 'file_path': 'docs/docs/integrations/graphs/tigergraph.mdx', 'file_name': 'tigergraph.mdx', 'file_type': '.mdx'}, page_content='conn = tg.TigerGraphConnection(host=\"DATABASE_HOST_HERE\", graphname=\"GRAPH_NAME_HERE\", username=\"USERNAME_HERE\", password=\"PASSWORD_HERE\")\\n\\n### ==== CONFIGURE INQUIRYAI HOST ====\\nconn.ai.configureInquiryAIHost(\"INQUIRYAI_HOST_HERE\")\\n\\nfrom langchain_community.graphs import TigerGraph\\n\\ngraph = TigerGraph(conn)\\nresult = graph.query(\"How many servers are there?\")\\nprint(result)\\n```'), Document(metadata={'source': 'docs/docs/integrations/llms/index.mdx', 'file_path': 'docs/docs/integrations/llms/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\nkeywords: [compatibility]\\n---\\n\\n# LLMs\\n\\n:::caution\\nYou are currently on a page documenting the use of [text completion models](/docs/concepts/text_llms). Many of the latest and most popular models are [chat completion models](/docs/concepts/chat_models).\\n\\nUnless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/docs/integrations/chat/).\\n:::\\n\\n[LLMs](/docs/concepts/text_llms) are language models that take a string as input and return a string as output.\\n\\n:::info\\n\\nIf you\\'d like to write your own LLM, see [this how-to](/docs/how_to/custom_llm/).\\nIf you\\'d like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).\\n\\n:::\\n\\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\\n\\n<CategoryTable category=\"llms\" />\\n\\n## All LLMs\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_path': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_name': 'layerup_security.mdx', 'file_type': '.mdx'}, page_content=\"# Layerup Security\\n\\nThe [Layerup Security](https://uselayerup.com) integration allows you to secure your calls to any LangChain LLM, LLM chain or LLM agent. The LLM object wraps around any existing LLM object, allowing for a secure layer between your users and your LLMs.\\n\\nWhile the Layerup Security object is designed as an LLM, it is not actually an LLM itself, it simply wraps around an LLM, allowing it to adapt the same functionality as the underlying LLM.\\n\\n## Setup\\nFirst, you'll need a Layerup Security account from the Layerup [website](https://uselayerup.com).\\n\\nNext, create a project via the [dashboard](https://dashboard.uselayerup.com), and copy your API key. We recommend putting your API key in your project's environment.\\n\\nInstall the Layerup Security SDK:\\n```bash\\npip install LayerupSecurity\\n```\\n\\nAnd install LangChain Community:\\n```bash\\npip install langchain-community\\n```\\n\\nAnd now you're ready to start protecting your LLM calls with Layerup Security!\"), Document(metadata={'source': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_path': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_name': 'layerup_security.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.llms.layerup_security import LayerupSecurity\\nfrom langchain_openai import OpenAI\\n\\n# Create an instance of your favorite LLM\\nopenai = OpenAI(\\n    model_name=\"gpt-3.5-turbo\",\\n    openai_api_key=\"OPENAI_API_KEY\",\\n)\\n\\n# Configure Layerup Security\\nlayerup_security = LayerupSecurity(\\n    # Specify a LLM that Layerup Security will wrap around\\n    llm=openai,\\n\\n    # Layerup API key, from the Layerup dashboard\\n    layerup_api_key=\"LAYERUP_API_KEY\",\\n\\n    # Custom base URL, if self hosting\\n    layerup_api_base_url=\"https://api.uselayerup.com/v1\",\\n\\n    # List of guardrails to run on prompts before the LLM is invoked\\n    prompt_guardrails=[],\\n\\n    # List of guardrails to run on responses from the LLM\\n    response_guardrails=[\"layerup.hallucination\"],\\n\\n    # Whether or not to mask the prompt for PII & sensitive data before it is sent to the LLM\\n    mask=False,'), Document(metadata={'source': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_path': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_name': 'layerup_security.mdx', 'file_type': '.mdx'}, page_content='# Metadata for abuse tracking, customer tracking, and scope tracking.\\n    metadata={\"customer\": \"example@uselayerup.com\"},\\n\\n    # Handler for guardrail violations on the prompt guardrails\\n    handle_prompt_guardrail_violation=(\\n        lambda violation: {\\n            \"role\": \"assistant\",\\n            \"content\": (\\n                \"There was sensitive data! I cannot respond. \"\\n                \"Here\\'s a dynamic canned response. Current date: {}\"\\n            ).format(datetime.now())\\n        }\\n        if violation[\"offending_guardrail\"] == \"layerup.sensitive_data\"\\n        else None\\n    ),\\n\\n    # Handler for guardrail violations on the response guardrails\\n    handle_response_guardrail_violation=(\\n        lambda violation: {\\n            \"role\": \"assistant\",\\n            \"content\": (\\n                \"Custom canned response with dynamic data! \"\\n                \"The violation rule was {}.\"\\n            ).format(violation[\"offending_guardrail\"])\\n        }\\n    ),\\n)'), Document(metadata={'source': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_path': 'docs/docs/integrations/llms/layerup_security.mdx', 'file_name': 'layerup_security.mdx', 'file_type': '.mdx'}, page_content='response = layerup_security.invoke(\\n    \"Summarize this message: my name is Bob Dylan. My SSN is 123-45-6789.\"\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/acreom.mdx', 'file_path': 'docs/docs/integrations/providers/acreom.mdx', 'file_name': 'acreom.mdx', 'file_type': '.mdx'}, page_content='# Acreom\\n\\n[acreom](https://acreom.com) is a dev-first knowledge base with tasks running on local `markdown` files.\\n\\n## Installation and Setup\\n\\nNo installation is required. \\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/acreom).\\n\\n```python\\nfrom langchain_community.document_loaders import AcreomLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_path': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_name': 'activeloop_deeplake.mdx', 'file_type': '.mdx'}, page_content=\"# Activeloop Deep Lake\\n\\n>[Activeloop Deep Lake](https://docs.activeloop.ai/) is a data lake for Deep Learning applications, allowing you to use it \\n> as a vector store.\\n\\n## Why Deep Lake?\\n\\n- More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models.\\n- Not only stores embeddings, but also the original data with automatic version control.\\n- Truly serverless. Doesn't require another service and can be used with major cloud providers (`AWS S3`, `GCS`, etc.)\\n\\n`Activeloop Deep Lake` supports `SelfQuery Retrieval`:\\n[Activeloop Deep Lake Self Query Retrieval](/docs/integrations/retrievers/self_query/activeloop_deeplake_self_query)\\n\\n\\n## More Resources\"), Document(metadata={'source': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_path': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_name': 'activeloop_deeplake.mdx', 'file_type': '.mdx'}, page_content='1. [Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/)\\n2. [Twitter the-algorithm codebase analysis with Deep Lake](https://github.com/langchain-ai/langchain/blob/master/cookbook/twitter-the-algorithm-analysis-deeplake.ipynb)\\n3. Here is [whitepaper](https://www.deeplake.ai/whitepaper) and [academic paper](https://arxiv.org/pdf/2209.10785.pdf) for Deep Lake\\n4. Here is a set of additional resources available for review: [Deep Lake](https://github.com/activeloopai/deeplake), [Get started](https://docs.activeloop.ai/getting-started) and\\xa0[Tutorials](https://docs.activeloop.ai/hub-tutorials)\\n\\n## Installation and Setup\\n\\nInstall the Python package:\\n\\n```bash\\npip install deeplake\\n```\\n\\n\\n## VectorStore\\n\\n```python\\nfrom langchain_community.vectorstores import DeepLake\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_path': 'docs/docs/integrations/providers/activeloop_deeplake.mdx', 'file_name': 'activeloop_deeplake.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/vectorstores/activeloop_deeplake).'), Document(metadata={'source': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_path': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_name': 'ads4gpts.mdx', 'file_type': '.mdx'}, page_content=\"# ADS4GPTs\\n\\n> [ADS4GPTs](https://www.ads4gpts.com/) is building the open monetization backbone of the AI-Native internet. It helps AI applications monetize through advertising with a UX and Privacy first approach. \\n\\n## Installation and Setup\\n\\n### Using pip\\nYou can install the package directly from PyPI:\\n\\n```bash\\npip install ads4gpts-langchain\\n```\\n\\n### From Source\\nAlternatively, install from source:\\n\\n```bash\\ngit clone https://github.com/ADS4GPTs/ads4gpts.git\\ncd ads4gpts/libs/python-sdk/ads4gpts-langchain\\npip install .\\n```\\n\\n## Prerequisites\\n\\n- Python 3.11+\\n- ADS4GPTs API Key ([Obtain API Key](https://www.ads4gpts.com))\\n\\n## Environment Variables\\nSet the following environment variables for API authentication:\\n\\n```bash\\nexport ADS4GPTS_API_KEY='your-ads4gpts-api-key'\\n```\\n\\nAlternatively, API keys can be passed directly when initializing classes or stored in a `.env` file.\\n\\n## Tools\\n\\nADS4GPTs provides two main tools for monetization:\"), Document(metadata={'source': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_path': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_name': 'ads4gpts.mdx', 'file_type': '.mdx'}, page_content=\"### Ads4gptsInlineSponsoredResponseTool\\nThis tool fetches native, sponsored responses that can be seamlessly integrated within your AI application's outputs.\\n\\n```python\\nfrom ads4gpts_langchain import Ads4gptsInlineSponsoredResponseTool\\n```\\n\\n### Ads4gptsSuggestedPromptTool\\nGenerates sponsored prompt suggestions to enhance user engagement and provide monetization opportunities.\\n\\n```python\\nfrom ads4gpts_langchain import Ads4gptsSuggestedPromptTool\\n```\\n### Ads4gptsInlineConversationalTool\\nDelivers conversational sponsored content that naturally fits within chat interfaces and dialogs.\\n\\n```python\\nfrom ads4gpts_langchain import Ads4gptsInlineConversationalTool\\n```\\n\\n### Ads4gptsInlineBannerTool\\nProvides inline banner advertisements that can be displayed within your AI application's response.\\n\\n```python\\nfrom ads4gpts_langchain import Ads4gptsInlineBannerTool\\n```\\n\\n### Ads4gptsSuggestedBannerTool\\nGenerates banner advertisement suggestions that can be presented to users as recommended content.\"), Document(metadata={'source': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_path': 'docs/docs/integrations/providers/ads4gpts.mdx', 'file_name': 'ads4gpts.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom ads4gpts_langchain import Ads4gptsSuggestedBannerTool\\n```\\n\\n## Toolkit\\n\\nThe `Ads4gptsToolkit` combines these tools for convenient access in LangChain applications.\\n\\n```python\\nfrom ads4gpts_langchain import Ads4gptsToolkit\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/aerospike.mdx', 'file_path': 'docs/docs/integrations/providers/aerospike.mdx', 'file_name': 'aerospike.mdx', 'file_type': '.mdx'}, page_content='# Aerospike\\n\\n>[Aerospike](https://aerospike.com/docs/vector) is a high-performance, distributed database known for its speed and scalability, now with support for vector storage and search, enabling retrieval and search of embedding vectors for machine learning and AI applications.\\n> See the documentation for Aerospike Vector Search (AVS) [here](https://aerospike.com/docs/vector).\\n\\n## Installation and Setup\\n\\nInstall the AVS Python SDK and AVS langchain vector store:\\n\\n```bash\\npip install aerospike-vector-search langchain-community\\n\\nSee the documentation for the Ptyhon SDK [here](https://aerospike-vector-search-python-client.readthedocs.io/en/latest/index.html).\\nThe documentation for the AVS langchain vector store is [here](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.aerospike.Aerospike.html).\\n\\n## Vector Store\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import Aerospike'), Document(metadata={'source': 'docs/docs/integrations/providers/aerospike.mdx', 'file_path': 'docs/docs/integrations/providers/aerospike.mdx', 'file_name': 'aerospike.mdx', 'file_type': '.mdx'}, page_content='See a usage example [here](https://python.langchain.com/docs/integrations/vectorstores/aerospike/).'), Document(metadata={'source': 'docs/docs/integrations/providers/agentql.mdx', 'file_path': 'docs/docs/integrations/providers/agentql.mdx', 'file_name': 'agentql.mdx', 'file_type': '.mdx'}, page_content='# AgentQL\\n\\n[AgentQL](https://www.agentql.com/) provides web interaction and structured data extraction from any web page using an [AgentQL query](https://docs.agentql.com/agentql-query) or a Natural Language prompt. AgentQL can be used across multiple languages and web pages without breaking over time and change.\\n\\n## Installation and Setup\\n\\nInstall the integration package:\\n\\n```bash\\npip install langchain-agentql\\n```\\n\\n## API Key\\n\\nGet an API Key from our [Dev Portal](https://dev.agentql.com/) and add it to your environment variables:\\n```\\nexport AGENTQL_API_KEY=\"your-api-key-here\"\\n```\\n\\n## DocumentLoader\\nAgentQL\\'s document loader provides structured data extraction from any web page using an AgentQL query.\\n\\n```python\\nfrom langchain_agentql.document_loaders import AgentQLLoader\\n```\\nSee our [document loader documentation and usage example](/docs/integrations/document_loaders/agentql).'), Document(metadata={'source': 'docs/docs/integrations/providers/agentql.mdx', 'file_path': 'docs/docs/integrations/providers/agentql.mdx', 'file_name': 'agentql.mdx', 'file_type': '.mdx'}, page_content='## Tools and Toolkits\\nAgentQL tools provides web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt.\\n\\n```python\\nfrom langchain_agentql.tools import ExtractWebDataTool, ExtractWebDataBrowserTool, GetWebElementBrowserTool\\nfrom langchain_agentql import AgentQLBrowserToolkit\\n```\\nSee our [tools documentation and usage example](/docs/integrations/tools/agentql).'), Document(metadata={'source': 'docs/docs/integrations/providers/ai21.mdx', 'file_path': 'docs/docs/integrations/providers/ai21.mdx', 'file_name': 'ai21.mdx', 'file_type': '.mdx'}, page_content='# AI21 Labs\\n\\n>[AI21 Labs](https://www.ai21.com/about) is a company specializing in Natural \\n> Language Processing (NLP), which develops AI systems \\n> that can understand and generate natural language.\\n\\nThis page covers how to use the `AI21` ecosystem within `LangChain`.\\n\\n## Installation and Setup\\n\\n- Get an AI21 api key and set it as an environment variable (`AI21_API_KEY`)\\n- Install the Python package:\\n\\n```bash\\npip install langchain-ai21\\n```\\n\\n## Chat models\\n\\n### AI21 Chat \\n\\nSee a [usage example](/docs/integrations/chat/ai21).\\n\\n```python\\nfrom langchain_ai21 import ChatAI21\\n```\\n\\n## Deprecated features\\n\\n:::caution The following features are deprecated. \\n:::\\n\\n### AI21 LLM\\n\\n```python\\nfrom langchain_ai21 import AI21LLM\\n```\\n\\n### AI21 Contextual Answer\\n\\n```python\\nfrom langchain_ai21 import AI21ContextualAnswers\\n```\\n\\n### AI21 Embeddings\\n\\n```python\\nfrom langchain_ai21 import AI21Embeddings\\n```\\n## Text splitters\\n\\n### AI21 Semantic Text Splitter'), Document(metadata={'source': 'docs/docs/integrations/providers/ai21.mdx', 'file_path': 'docs/docs/integrations/providers/ai21.mdx', 'file_name': 'ai21.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_ai21 import AI21SemanticTextSplitter\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ainetwork.mdx', 'file_path': 'docs/docs/integrations/providers/ainetwork.mdx', 'file_name': 'ainetwork.mdx', 'file_type': '.mdx'}, page_content='# AINetwork\\n\\n>[AI Network](https://www.ainetwork.ai/build-on-ain) is a layer 1 blockchain designed to accommodate \\n> large-scale AI models, utilizing a decentralized GPU network powered by the \\n> [$AIN token](https://www.ainetwork.ai/token), enriching AI-driven `NFTs` (`AINFTs`).\\n\\n\\n## Installation and Setup\\n\\nYou need to install `ain-py` python package.\\n\\n```bash\\npip install ain-py\\n```\\nYou need to set the `AIN_BLOCKCHAIN_ACCOUNT_PRIVATE_KEY` environmental variable to your AIN Blockchain Account Private Key.\\n## Toolkit\\n\\nSee a [usage example](/docs/integrations/tools/ainetwork).\\n\\n```python\\nfrom langchain_community.agent_toolkits.ainetwork.toolkit import AINetworkToolkit\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/airbyte.mdx', 'file_path': 'docs/docs/integrations/providers/airbyte.mdx', 'file_name': 'airbyte.mdx', 'file_type': '.mdx'}, page_content=\"# Airbyte\\n\\n>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs, \\n> databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.\\n\\n## Installation and Setup\\n\\n```bash\\npip install -U langchain-airbyte\\n```\\n\\n:::note\\n\\nCurrently, the `langchain-airbyte` library does not support Pydantic v2.\\nPlease downgrade to Pydantic v1 to use this package.\\n\\nThis package also currently requires Python 3.10+.\\n\\n:::\\n\\nThe integration package doesn't require any global environment variables that need to be\\nset, but some integrations (e.g. `source-github`) may need credentials passed in.\\n\\n## Document loader\\n\\n### AirbyteLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/airbyte).\\n\\n```python\\nfrom langchain_airbyte import AirbyteLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/alchemy.mdx', 'file_path': 'docs/docs/integrations/providers/alchemy.mdx', 'file_name': 'alchemy.mdx', 'file_type': '.mdx'}, page_content='# Alchemy\\n\\n>[Alchemy](https://www.alchemy.com) is the platform to build blockchain applications.\\n\\n## Installation and Setup\\n\\nCheck out the [installation guide](/docs/integrations/document_loaders/blockchain).\\n\\n## Document loader\\n\\n### BlockchainLoader on the Alchemy platform\\n\\nSee a [usage example](/docs/integrations/document_loaders/blockchain).\\n\\n```python\\nfrom langchain_community.document_loaders.blockchain import (\\n    BlockchainDocumentLoader,\\n    BlockchainType,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/aleph_alpha.mdx', 'file_path': 'docs/docs/integrations/providers/aleph_alpha.mdx', 'file_name': 'aleph_alpha.mdx', 'file_type': '.mdx'}, page_content='# Aleph Alpha\\n\\n>[Aleph Alpha](https://docs.aleph-alpha.com/) was founded in 2019 with the mission to research and build the foundational technology for an era of strong AI. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI like large language and multimodal models and runs the fastest European commercial AI cluster.\\n\\n>[The Luminous series](https://docs.aleph-alpha.com/docs/introduction/luminous/) is a family of large language models.\\n\\n## Installation and Setup\\n\\n```bash\\npip install aleph-alpha-client\\n```\\n\\nYou have to create a new token. Please, see [instructions](https://docs.aleph-alpha.com/docs/account/#create-a-new-token).\\n\\n```python\\nfrom getpass import getpass\\n\\nALEPH_ALPHA_API_KEY = getpass()\\n```\\n\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/aleph_alpha).\\n\\n```python\\nfrom langchain_community.llms import AlephAlpha\\n```\\n\\n## Text Embedding Models'), Document(metadata={'source': 'docs/docs/integrations/providers/aleph_alpha.mdx', 'file_path': 'docs/docs/integrations/providers/aleph_alpha.mdx', 'file_name': 'aleph_alpha.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/text_embedding/aleph_alpha).\\n\\n```python\\nfrom langchain_community.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_path': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_name': 'alibaba_cloud.mdx', 'file_type': '.mdx'}, page_content=\"# Alibaba Cloud\\n\\n>[Alibaba Group Holding Limited (Wikipedia)](https://en.wikipedia.org/wiki/Alibaba_Group), or `Alibaba`\\n> (Chinese: 阿里巴巴), is a Chinese multinational technology company specializing in e-commerce, retail, \\n> Internet, and technology.\\n> \\n> [Alibaba Cloud (Wikipedia)](https://en.wikipedia.org/wiki/Alibaba_Cloud), also known as `Aliyun`\\n> (Chinese: 阿里云; pinyin: Ālǐyún; lit. 'Ali Cloud'), is a cloud computing company, a subsidiary \\n> of `Alibaba Group`. `Alibaba Cloud` provides cloud computing services to online businesses and \\n> Alibaba's own e-commerce ecosystem.\\n \\n \\n## LLMs\\n\\n### Alibaba Cloud PAI EAS\\n\\nSee [installation instructions and a usage example](/docs/integrations/llms/alibabacloud_pai_eas_endpoint).\\n\\n```python\\nfrom langchain_community.llms.pai_eas_endpoint import PaiEasEndpoint\\n```\\n\\n### Tongyi Qwen\\n\\nSee [installation instructions and a usage example](/docs/integrations/llms/tongyi).\\n\\n```python\\nfrom langchain_community.llms import Tongyi\\n```\\n\\n## Chat Models\"), Document(metadata={'source': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_path': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_name': 'alibaba_cloud.mdx', 'file_type': '.mdx'}, page_content='### Alibaba Cloud PAI EAS\\n\\nSee [installation instructions and a usage example](/docs/integrations/chat/alibaba_cloud_pai_eas).\\n\\n```python\\nfrom langchain_community.chat_models import PaiEasChatEndpoint\\n```\\n\\n### Tongyi Qwen Chat\\n\\nSee [installation instructions and a usage example](/docs/integrations/chat/tongyi).\\n\\n```python\\nfrom langchain_community.chat_models.tongyi import ChatTongyi\\n```\\n\\n## Document Loaders\\n\\n### Alibaba Cloud MaxCompute\\n\\nSee [installation instructions and a usage example](/docs/integrations/document_loaders/alibaba_cloud_maxcompute).\\n\\n```python\\nfrom langchain_community.document_loaders import MaxComputeLoader\\n```\\n\\n## Vector stores\\n\\n### Alibaba Cloud OpenSearch\\n\\nSee [installation instructions and a usage example](/docs/integrations/vectorstores/alibabacloud_opensearch).\\n\\n```python\\nfrom langchain_community.vectorstores import AlibabaCloudOpenSearch, AlibabaCloudOpenSearchSettings\\n```\\n\\n### Alibaba Cloud Tair'), Document(metadata={'source': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_path': 'docs/docs/integrations/providers/alibaba_cloud.mdx', 'file_name': 'alibaba_cloud.mdx', 'file_type': '.mdx'}, page_content='See [installation instructions and a usage example](/docs/integrations/vectorstores/tair).\\n\\n```python\\nfrom langchain_community.vectorstores import Tair\\n```\\n\\n### AnalyticDB\\n\\nSee [installation instructions and a usage example](/docs/integrations/vectorstores/analyticdb).\\n\\n```python\\nfrom langchain_community.vectorstores import AnalyticDB\\n```\\n\\n### Hologres\\n\\nSee [installation instructions and a usage example](/docs/integrations/vectorstores/hologres).\\n\\n```python\\nfrom langchain_community.vectorstores import Hologres\\n```\\n\\n### Tablestore\\n\\nSee [installation instructions and a usage example](/docs/integrations/vectorstores/tablestore).\\n\\n```python\\nfrom langchain_community.vectorstores import TablestoreVectorStore\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/analyticdb.mdx', 'file_path': 'docs/docs/integrations/providers/analyticdb.mdx', 'file_name': 'analyticdb.mdx', 'file_type': '.mdx'}, page_content='# AnalyticDB\\n\\n>[AnalyticDB for PostgreSQL](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) \\n> is a massively parallel processing (MPP) data warehousing service \\n> from [Alibaba Cloud](https://www.alibabacloud.com/)\\n>that is designed to analyze large volumes of data online.\\n\\n>`AnalyticDB for PostgreSQL` is developed based on the open-source `Greenplum Database` \\n> project and is enhanced with in-depth extensions by `Alibaba Cloud`. AnalyticDB \\n> for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and \\n> Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and \\n> column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a \\n> high performance level and supports highly concurrent.\\n\\nThis page covers how to use the AnalyticDB ecosystem within LangChain.\\n\\n## Installation and Setup\\n\\nYou need to install the `sqlalchemy` python package.\\n\\n```bash\\npip install sqlalchemy\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/analyticdb.mdx', 'file_path': 'docs/docs/integrations/providers/analyticdb.mdx', 'file_name': 'analyticdb.mdx', 'file_type': '.mdx'}, page_content='## VectorStore\\n\\nSee a [usage example](/docs/integrations/vectorstores/analyticdb).\\n\\n```python\\nfrom langchain_community.vectorstores import AnalyticDB\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/annoy.mdx', 'file_path': 'docs/docs/integrations/providers/annoy.mdx', 'file_name': 'annoy.mdx', 'file_type': '.mdx'}, page_content='# Annoy\\n\\n> [Annoy](https://github.com/spotify/annoy) (`Approximate Nearest Neighbors Oh Yeah`) \\n> is a C++ library with Python bindings to search for points in space that are \\n> close to a given query point. It also creates large read-only file-based data \\n> structures that are mapped into memory so that many processes may share the same data. \\n\\n## Installation and Setup\\n\\n```bash\\npip install annoy\\n```\\n\\n\\n## Vectorstore\\n\\nSee a [usage example](/docs/integrations/vectorstores/annoy).\\n\\n```python\\nfrom langchain_community.vectorstores import Annoy\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/anthropic.mdx', 'file_path': 'docs/docs/integrations/providers/anthropic.mdx', 'file_name': 'anthropic.mdx', 'file_type': '.mdx'}, page_content=\"# Anthropic\\n\\n>[Anthropic](https://www.anthropic.com/) is an AI safety and research company, and is the creator of `Claude`.\\nThis page covers all integrations between `Anthropic` models and `LangChain`.\\n\\n## Installation and Setup\\n\\nTo use `Anthropic` models, you need to install a python package:\\n\\n```bash\\npip install -U langchain-anthropic\\n```\\n\\nYou need to set the `ANTHROPIC_API_KEY` environment variable.\\nYou can get an Anthropic API key [here](https://console.anthropic.com/settings/keys)\\n\\n## Chat Models\\n\\n### ChatAnthropic\\n\\nSee a [usage example](/docs/integrations/chat/anthropic).\\n\\n```python\\nfrom langchain_anthropic import ChatAnthropic\\n\\nmodel = ChatAnthropic(model='claude-3-opus-20240229')\\n```\\n\\n\\n## LLMs\\n\\n### [Legacy] AnthropicLLM\\n\\n**NOTE**: `AnthropicLLM` only supports legacy `Claude 2` models. \\nTo use the newest `Claude 3` models, please use `ChatAnthropic` instead.\\n\\nSee a [usage example](/docs/integrations/llms/anthropic).\\n\\n```python\\nfrom langchain_anthropic import AnthropicLLM\"), Document(metadata={'source': 'docs/docs/integrations/providers/anthropic.mdx', 'file_path': 'docs/docs/integrations/providers/anthropic.mdx', 'file_name': 'anthropic.mdx', 'file_type': '.mdx'}, page_content=\"model = AnthropicLLM(model='claude-2.1')\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/anyscale.mdx', 'file_path': 'docs/docs/integrations/providers/anyscale.mdx', 'file_name': 'anyscale.mdx', 'file_type': '.mdx'}, page_content='# Anyscale\\n\\n>[Anyscale](https://www.anyscale.com) is a platform to run, fine tune and scale LLMs via production-ready APIs.\\n> [Anyscale Endpoints](https://docs.anyscale.com/endpoints/overview) serve many open-source models in a cost-effective way.\\n\\n`Anyscale` also provides [an example](https://docs.anyscale.com/endpoints/model-serving/examples/langchain-integration) \\nhow to setup `LangChain` with `Anyscale` for advanced chat agents.\\n\\n## Installation and Setup\\n\\n- Get an Anyscale Service URL, route and API key and set them as environment variables (`ANYSCALE_SERVICE_URL`,`ANYSCALE_SERVICE_ROUTE`, `ANYSCALE_SERVICE_TOKEN`). \\n- Please see [the Anyscale docs](https://www.anyscale.com/get-started) for more details.\\n\\nWe have to install the `openai` package:\\n\\n```bash\\npip install openai\\n```\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/anyscale).\\n\\n```python\\nfrom langchain_community.llms.anyscale import Anyscale\\n```\\n\\n## Chat Models'), Document(metadata={'source': 'docs/docs/integrations/providers/anyscale.mdx', 'file_path': 'docs/docs/integrations/providers/anyscale.mdx', 'file_name': 'anyscale.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/chat/anyscale).\\n\\n```python\\nfrom langchain_community.chat_models.anyscale import ChatAnyscale\\n```\\n\\n## Embeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/anyscale).\\n\\n```python\\nfrom langchain_community.embeddings import AnyscaleEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/apache.mdx', 'file_path': 'docs/docs/integrations/providers/apache.mdx', 'file_name': 'apache.mdx', 'file_type': '.mdx'}, page_content='# Apache Software Foundation\\n\\n>[The Apache Software Foundation (Wikipedia)](https://en.wikipedia.org/wiki/The_Apache_Software_Foundation) \\n> is a decentralized open source community of developers. The software they \\n> produce is distributed under the terms of the Apache License, a permissive \\n> open-source license for free and open-source software (FOSS). The Apache projects \\n> are characterized by a collaborative, consensus-based development process \\n> and an open and pragmatic software license, which is to say that it \\n> allows developers, who receive the software freely, to redistribute \\n> it under non-free terms. Each project is managed by a self-selected \\n> team of technical experts who are active contributors to the project.\\n\\n## Apache AGE'), Document(metadata={'source': 'docs/docs/integrations/providers/apache.mdx', 'file_path': 'docs/docs/integrations/providers/apache.mdx', 'file_name': 'apache.mdx', 'file_type': '.mdx'}, page_content='>[Apache AGE](https://age.apache.org/) is a `PostgreSQL` extension that provides \\n> graph database functionality. `AGE` is an acronym for `A Graph Extension`, and \\n> is inspired by Bitnine’s fork of `PostgreSQL 10`, `AgensGraph`, which is \\n> a multimodal database. The goal of the project is to create single \\n> storage that can handle both relational and graph model data so that users \\n> can use standard ANSI SQL along with `openCypher`, the Graph query language. \\n> The data elements `Apache AGE` stores are nodes, edges connecting them, and \\n> attributes of nodes and edges.\\n \\nSee more about [integrating with Apache AGE](/docs/integrations/graphs/apache_age).\\n\\n## Apache Cassandra\\n\\n>[Apache Cassandra](https://cassandra.apache.org/) is a NoSQL, row-oriented, \\n> highly scalable and highly available database. Starting with version 5.0, \\n> the database ships with vector search capabilities.\\n \\nSee more about [integrating with Apache Cassandra](/docs/integrations/providers/cassandra/).'), Document(metadata={'source': 'docs/docs/integrations/providers/apache.mdx', 'file_path': 'docs/docs/integrations/providers/apache.mdx', 'file_name': 'apache.mdx', 'file_type': '.mdx'}, page_content='## Apache Doris\\n\\n>[Apache Doris](https://doris.apache.org/) is a modern data warehouse for \\n> real-time analytics. It delivers lightning-fast analytics on real-time data at scale.\\n>\\n>Usually `Apache Doris` is categorized into OLAP, and it has showed excellent \\n> performance in ClickBench — a Benchmark For Analytical DBMS. Since it has \\n> a super-fast vectorized execution engine, it could also be used as a fast vectordb.\\n \\nSee more about [integrating with Apache Doris](/docs/integrations/providers/apache_doris/).\\n\\n## Apache Kafka\\n\\n>[Apache Kafka](https://github.com/apache/kafka) is a distributed messaging system \\n> that is used to publish and subscribe to streams of records.\\n \\nSee more about [integrating with Apache Kafka](/docs/integrations/memory/kafka_chat_message_history).\\n\\n\\n## Apache Spark'), Document(metadata={'source': 'docs/docs/integrations/providers/apache.mdx', 'file_path': 'docs/docs/integrations/providers/apache.mdx', 'file_name': 'apache.mdx', 'file_type': '.mdx'}, page_content='>[Apache Spark](https://spark.apache.org/) is a unified analytics engine for \\n> large-scale data processing. It provides high-level APIs in Scala, Java, \\n> Python, and R, and an optimized engine that supports general computation \\n> graphs for data analysis. It also supports a rich set of higher-level \\n> tools including `Spark SQL` for SQL and DataFrames, `pandas API on Spark` \\n> for pandas workloads, `MLlib` for machine learning, \\n> `GraphX` for graph processing, and `Structured Streaming` for stream processing.\\n\\nSee more about [integrating with Apache Spark](/docs/integrations/providers/spark).'), Document(metadata={'source': 'docs/docs/integrations/providers/apache_doris.mdx', 'file_path': 'docs/docs/integrations/providers/apache_doris.mdx', 'file_name': 'apache_doris.mdx', 'file_type': '.mdx'}, page_content='# Apache Doris\\n\\n>[Apache Doris](https://doris.apache.org/) is a modern data warehouse for real-time analytics.\\nIt delivers lightning-fast analytics on real-time data at scale.\\n\\n>Usually `Apache Doris` is categorized into OLAP, and it has showed excellent performance \\n> in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). \\n> Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\\n\\n## Installation and Setup\\n\\n```bash\\npip install pymysql\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/apache_doris).\\n\\n```python\\nfrom langchain_community.vectorstores import ApacheDoris\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/apify.mdx', 'file_path': 'docs/docs/integrations/providers/apify.mdx', 'file_name': 'apify.mdx', 'file_type': '.mdx'}, page_content='# Apify\\n\\n\\n>[Apify](https://apify.com) is a cloud platform for web scraping and data extraction,\\n>which provides an [ecosystem](https://apify.com/store) of more than a thousand\\n>ready-made apps called *Actors* for various scraping, crawling, and extraction use cases.\\n\\n[![Apify Actors](/img/ApifyActors.png)](https://apify.com/store)\\n\\nThis integration enables you run Actors on the `Apify` platform and load their results into LangChain to feed your vector\\nindexes with documents and data from the web, e.g. to generate answers from websites with documentation,\\nblogs, or knowledge bases.\\n\\n\\n## Installation and Setup\\n\\n- Install the LangChain Apify package for Python with:\\n```bash\\npip install langchain-apify\\n```\\n- Get your [Apify API token](https://console.apify.com/account/integrations) and either set it as\\n  an environment variable (`APIFY_API_TOKEN`) or pass it as `apify_api_token` in the constructor.\\n\\n## Tool\\n\\nYou can use the `ApifyActorsTool` to use Apify Actors with agents.'), Document(metadata={'source': 'docs/docs/integrations/providers/apify.mdx', 'file_path': 'docs/docs/integrations/providers/apify.mdx', 'file_name': 'apify.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_apify import ApifyActorsTool\\n```\\n\\nSee [this notebook](/docs/integrations/tools/apify_actors) for example usage and a full example of a tool-calling agent with LangGraph in the [Apify LangGraph agent Actor template](https://apify.com/templates/python-langgraph).\\n\\nFor more information on how to use this tool, visit [the Apify integration documentation](https://docs.apify.com/platform/integrations/langgraph).\\n\\n## Wrapper\\n\\nYou can use the `ApifyWrapper` to run Actors on the Apify platform.\\n\\n```python\\nfrom langchain_apify import ApifyWrapper\\n```\\n\\nFor more information on how to use this wrapper, see [the Apify integration documentation](https://docs.apify.com/platform/integrations/langchain).\\n\\n\\n## Document loader\\n\\nYou can also use our `ApifyDatasetLoader` to get data from Apify dataset.\\n\\n```python\\nfrom langchain_apify import ApifyDatasetLoader\\n```\\n\\nFor a more detailed walkthrough of this loader, see [this notebook](/docs/integrations/document_loaders/apify_dataset).'), Document(metadata={'source': 'docs/docs/integrations/providers/apify.mdx', 'file_path': 'docs/docs/integrations/providers/apify.mdx', 'file_name': 'apify.mdx', 'file_type': '.mdx'}, page_content='Source code for this integration can be found in the [LangChain Apify repository](https://github.com/apify/langchain-apify).'), Document(metadata={'source': 'docs/docs/integrations/providers/apple.mdx', 'file_path': 'docs/docs/integrations/providers/apple.mdx', 'file_name': 'apple.mdx', 'file_type': '.mdx'}, page_content='# Apple\\n\\n>[Apple Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Apple_Inc.) is an American \\n> multinational corporation and technology company.\\n>\\n> [iMessage (Wikipedia)](https://en.wikipedia.org/wiki/IMessage) is an instant \\n> messaging service developed by Apple Inc. and launched in 2011. \\n> `iMessage` functions exclusively on Apple platforms.\\n\\n## Installation and Setup\\n\\nSee [setup instructions](/docs/integrations/chat_loaders/imessage).\\n\\n## Chat loader\\n\\nIt loads chat sessions from the `iMessage` `chat.db` `SQLite` file.\\n\\nSee a [usage example](/docs/integrations/chat_loaders/imessage).\\n\\n```python\\nfrom langchain_community.chat_loaders.imessage import IMessageChatLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/arangodb.mdx', 'file_path': 'docs/docs/integrations/providers/arangodb.mdx', 'file_name': 'arangodb.mdx', 'file_type': '.mdx'}, page_content='# ArangoDB\\n\\n>[ArangoDB](https://github.com/arangodb/arangodb) is a scalable graph database system to \\n> drive value from connected data, faster. Native graphs, an integrated search engine, and JSON support, via a single query language. ArangoDB runs on-prem, in the cloud – anywhere.\\n\\n## Installation and Setup\\n\\nInstall the [ArangoDB Python Driver](https://github.com/ArangoDB-Community/python-arango) package with\\n\\n```bash\\npip install python-arango\\n```\\n\\n## Graph QA Chain\\n\\nConnect your `ArangoDB` Database with a chat model to get insights on your data. \\n\\nSee the notebook example [here](/docs/integrations/graphs/arangodb).\\n\\n```python\\nfrom arango import ArangoClient\\n\\nfrom langchain_community.graphs import ArangoGraph\\nfrom langchain.chains import ArangoGraphQAChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/arcee.mdx', 'file_path': 'docs/docs/integrations/providers/arcee.mdx', 'file_name': 'arcee.mdx', 'file_type': '.mdx'}, page_content='# Arcee\\n\\n>[Arcee](https://www.arcee.ai/about/about-us) enables the development and advancement \\n> of what we coin as SLMs—small, specialized, secure, and scalable language models. \\n> By offering a SLM Adaptation System and a seamless, secure integration, \\n> `Arcee` empowers enterprises to harness the full potential of \\n> domain-adapted language models, driving the transformative \\n> innovation in operations.\\n\\n\\n## Installation and Setup\\n\\nGet your `Arcee API` key.\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/arcee).\\n\\n```python\\nfrom langchain_community.llms import Arcee\\n```\\n\\n## Retrievers\\n\\nSee a [usage example](/docs/integrations/retrievers/arcee).\\n\\n```python\\nfrom langchain_community.retrievers import ArceeRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/arcgis.mdx', 'file_path': 'docs/docs/integrations/providers/arcgis.mdx', 'file_name': 'arcgis.mdx', 'file_type': '.mdx'}, page_content='# ArcGIS\\n\\n>[ArcGIS](https://www.esri.com/en-us/arcgis/about-arcgis/overview) is a family of client, \\n> server and online geographic information system software developed and maintained by [Esri](https://www.esri.com/).\\n> \\n>`ArcGISLoader` uses the `arcgis` package.\\n> `arcgis` is a Python library for the vector and raster analysis, geocoding, map making, \\n> routing and directions. It administers, organizes and manages users, \\n> groups and information items in your GIS.\\n>It enables access to ready-to-use maps and curated geographic data from `Esri` \\n> and other authoritative sources, and works with your own data as well. \\n\\n## Installation and Setup\\n\\nWe have to install the `arcgis` package.\\n\\n```bash\\npip install -U arcgis\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/arcgis).\\n\\n```python\\nfrom langchain_community.document_loaders import ArcGISLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/argilla.mdx', 'file_path': 'docs/docs/integrations/providers/argilla.mdx', 'file_name': 'argilla.mdx', 'file_type': '.mdx'}, page_content='# Argilla\\n\\n>[Argilla](https://argilla.io/) is an open-source data curation platform for LLMs. \\n> Using `Argilla`, everyone can build robust language models through faster data curation \\n> using both human and machine feedback. `Argilla` provides support for each step in the MLOps cycle, \\n> from data labeling to model monitoring.\\n\\n## Installation and Setup\\n\\nGet your [API key](https://platform.openai.com/account/api-keys).\\n\\nInstall the Python package:\\n\\n```bash\\npip install argilla\\n```\\n\\n## Callbacks\\n\\n\\n```python\\nfrom langchain.callbacks import ArgillaCallbackHandler\\n```\\n\\nSee an [example](/docs/integrations/callbacks/argilla).'), Document(metadata={'source': 'docs/docs/integrations/providers/arize.mdx', 'file_path': 'docs/docs/integrations/providers/arize.mdx', 'file_name': 'arize.mdx', 'file_type': '.mdx'}, page_content='# Arize\\n\\n[Arize](https://arize.com) is an AI observability and LLM evaluation platform that offers\\nsupport for LangChain applications, providing detailed traces of input, embeddings, retrieval,\\nfunctions, and output messages.\\n\\n\\n## Installation and Setup\\n\\nFirst, you need to install `arize` python package.\\n\\n```bash\\npip install arize\\n```\\n\\nSecond, you need to set up your [Arize account](https://app.arize.com/auth/join)\\nand get your  `API_KEY` or `SPACE_KEY`.\\n\\n\\n## Callback handler\\n\\n```python\\nfrom langchain_community.callbacks import ArizeCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/arxiv.mdx', 'file_path': 'docs/docs/integrations/providers/arxiv.mdx', 'file_name': 'arxiv.mdx', 'file_type': '.mdx'}, page_content='# Arxiv\\n\\n>[arXiv](https://arxiv.org/) is an open-access archive for 2 million scholarly articles in the fields of physics, \\n> mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and \\n> systems science, and economics.\\n\\n\\n## Installation and Setup\\n\\nFirst, you need to install `arxiv` python package.\\n\\n```bash\\npip install arxiv\\n```\\n\\nSecond, you need to install `PyMuPDF` python package which transforms PDF files downloaded from the `arxiv.org` site into the text format.\\n\\n```bash\\npip install pymupdf\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/arxiv).\\n\\n```python\\nfrom langchain_community.document_loaders import ArxivLoader\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/arxiv).\\n\\n```python\\nfrom langchain_community.retrievers import ArxivRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ascend.mdx', 'file_path': 'docs/docs/integrations/providers/ascend.mdx', 'file_name': 'ascend.mdx', 'file_type': '.mdx'}, page_content='# Ascend\\n\\n>[Ascend](https://https://www.hiascend.com/) is Natural Process Unit provide by Huawei\\n\\nThis page covers how to use ascend NPU with LangChain.\\n\\n### Installation\\n\\nInstall using torch-npu using:\\n\\n```bash\\npip install torch-npu\\n```\\n\\nPlease follow the installation instructions as specified below:\\n* Install CANN as shown [here](https://www.hiascend.com/document/detail/zh/canncommercial/700/quickstart/quickstart/quickstart_18_0002.html).\\n\\n### Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/ascend).\\n\\n```python\\nfrom langchain_community.embeddings import AscendEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/asknews.mdx', 'file_path': 'docs/docs/integrations/providers/asknews.mdx', 'file_name': 'asknews.mdx', 'file_type': '.mdx'}, page_content='# AskNews\\n\\n[AskNews](https://asknews.app/) enhances language models with up-to-date global or historical news\\nby processing and indexing over 300,000 articles daily, providing prompt-optimized responses\\nthrough a low-latency endpoint, and ensuring transparency and diversity in its news coverage.\\n\\n## Installation and Setup\\n\\nFirst, you need to install `asknews` python package.\\n\\n```bash\\npip install asknews\\n```\\n\\nYou also need to set our AskNews API credentials, which can be generated at \\nthe [AskNews console](https://my.asknews.app/).\\n\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/asknews).\\n\\n```python\\nfrom langchain_community.retrievers import AskNewsRetriever\\n```\\n\\n## Tool\\n\\nSee a [usage example](/docs/integrations/tools/asknews).\\n\\n```python\\nfrom langchain_community.tools.asknews import AskNewsSearch\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/assemblyai.mdx', 'file_path': 'docs/docs/integrations/providers/assemblyai.mdx', 'file_name': 'assemblyai.mdx', 'file_type': '.mdx'}, page_content='# AssemblyAI\\n\\n>[AssemblyAI](https://www.assemblyai.com/) builds `Speech AI` models for tasks like \\nspeech-to-text, speaker diarization, speech summarization, and more.\\n> `AssemblyAI’s` Speech AI models include accurate speech-to-text for voice data \\n> (such as calls, virtual meetings, and podcasts), speaker detection, sentiment analysis, \\n> chapter detection, PII redaction.\\n \\n\\n\\n## Installation and Setup\\n\\nGet your [API key](https://www.assemblyai.com/dashboard/signup).\\n\\nInstall the `assemblyai` package.\\n\\n```bash\\npip install -U assemblyai\\n```\\n\\n## Document Loader\\n\\n###  AssemblyAI Audio Transcript\\n\\nThe `AssemblyAIAudioTranscriptLoader` transcribes audio files with the `AssemblyAI API` \\nand loads the transcribed text into documents.\\n\\nSee a [usage example](/docs/integrations/document_loaders/assemblyai).\\n\\n```python\\nfrom langchain_community.document_loaders import AssemblyAIAudioTranscriptLoader\\n```\\n\\n###  AssemblyAI Audio Loader By Id'), Document(metadata={'source': 'docs/docs/integrations/providers/assemblyai.mdx', 'file_path': 'docs/docs/integrations/providers/assemblyai.mdx', 'file_name': 'assemblyai.mdx', 'file_type': '.mdx'}, page_content='The `AssemblyAIAudioLoaderById` uses the AssemblyAI API to get an existing \\ntranscription and loads the transcribed text into one or more Documents, \\ndepending on the specified format.\\n\\n```python\\nfrom langchain_community.document_loaders import AssemblyAIAudioLoaderById\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/astradb.mdx', 'file_path': 'docs/docs/integrations/providers/astradb.mdx', 'file_name': 'astradb.mdx', 'file_type': '.mdx'}, page_content='# Astra DB\\n\\n> [DataStax Astra DB](https://docs.datastax.com/en/astra/home/astra.html) is a serverless \\n> vector-capable database built on `Apache Cassandra®`and made conveniently available\\n> through an easy-to-use JSON API.\\n\\nSee a [tutorial provided by DataStax](https://docs.datastax.com/en/astra/astra-db-vector/tutorials/chatbot.html).\\n\\n## Installation and Setup\\n\\nInstall the following Python package:\\n```bash\\npip install \"langchain-astradb>=0.1.0\"\\n```\\n\\nGet the [connection secrets](https://docs.datastax.com/en/astra/astra-db-vector/get-started/quickstart.html).\\nSet up the following environment variables:\\n\\n```python\\nASTRA_DB_APPLICATION_TOKEN=\"TOKEN\"\\nASTRA_DB_API_ENDPOINT=\"API_ENDPOINT\"\\n```\\n\\n## Vector Store\\n\\n```python\\nfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=my_embedding,\\n    collection_name=\"my_store\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/astradb.mdx', 'file_path': 'docs/docs/integrations/providers/astradb.mdx', 'file_name': 'astradb.mdx', 'file_type': '.mdx'}, page_content='Learn more in the [example notebook](/docs/integrations/vectorstores/astradb).\\n\\nSee the [example provided by DataStax](https://docs.datastax.com/en/astra/astra-db-vector/integrations/langchain.html).\\n\\n## Chat message history\\n\\n```python\\nfrom langchain_astradb import AstraDBChatMessageHistory\\n\\nmessage_history = AstraDBChatMessageHistory(\\n    session_id=\"test-session\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n```\\n\\nSee the [usage example](/docs/integrations/memory/astradb_chat_message_history#example).\\n\\n## LLM Cache\\n\\n```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_astradb import AstraDBCache\\n\\nset_llm_cache(AstraDBCache(\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n))\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/llm_caching#astra-db-caches) (scroll to the Astra DB section).\\n\\n\\n## Semantic LLM Cache'), Document(metadata={'source': 'docs/docs/integrations/providers/astradb.mdx', 'file_path': 'docs/docs/integrations/providers/astradb.mdx', 'file_name': 'astradb.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_astradb import AstraDBSemanticCache\\n\\nset_llm_cache(AstraDBSemanticCache(\\n    embedding=my_embedding,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n))\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/llm_caching#astra-db-caches) (scroll to the appropriate section).\\n\\nLearn more in the [example notebook](/docs/integrations/memory/astradb_chat_message_history).\\n\\n## Document loader\\n\\n```python\\nfrom langchain_astradb import AstraDBLoader\\n\\nloader = AstraDBLoader(\\n    collection_name=\"my_collection\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/document_loaders/astradb).\\n\\n## Self-querying retriever\\n\\n```python\\nfrom langchain_astradb import AstraDBVectorStore\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever'), Document(metadata={'source': 'docs/docs/integrations/providers/astradb.mdx', 'file_path': 'docs/docs/integrations/providers/astradb.mdx', 'file_name': 'astradb.mdx', 'file_type': '.mdx'}, page_content='vector_store = AstraDBVectorStore(\\n    embedding=my_embedding,\\n    collection_name=\"my_store\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n\\nretriever = SelfQueryRetriever.from_llm(\\n    my_llm,\\n    vector_store,\\n    document_content_description,\\n    metadata_field_info\\n)\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/retrievers/self_query/astradb).\\n\\n## Store\\n\\n```python\\nfrom langchain_astradb import AstraDBStore\\n\\nstore = AstraDBStore(\\n    collection_name=\"my_kv_store\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n```\\n\\nSee the API Reference for the [AstraDBStore](https://python.langchain.com/api_reference/astradb/storage/langchain_astradb.storage.AstraDBStore.html).\\n\\n## Byte Store\\n\\n```python\\nfrom langchain_astradb import AstraDBByteStore\\n\\nstore = AstraDBByteStore(\\n    collection_name=\"my_kv_store\",\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/astradb.mdx', 'file_path': 'docs/docs/integrations/providers/astradb.mdx', 'file_name': 'astradb.mdx', 'file_type': '.mdx'}, page_content='See the API reference for the [AstraDBByteStore](https://python.langchain.com/api_reference/astradb/storage/langchain_astradb.storage.AstraDBByteStore.html).'), Document(metadata={'source': 'docs/docs/integrations/providers/atlas.mdx', 'file_path': 'docs/docs/integrations/providers/atlas.mdx', 'file_name': 'atlas.mdx', 'file_type': '.mdx'}, page_content='# Atlas\\n\\n>[Nomic Atlas](https://docs.nomic.ai/index.html) is a platform for interacting with both \\n> small and internet scale unstructured datasets.\\n\\n\\n## Installation and Setup\\n\\n- Install the Python package with `pip install nomic`\\n- `Nomic` is also included in langchains poetry extras `poetry install -E all`\\n\\n\\n## VectorStore\\n\\nSee a [usage example](/docs/integrations/vectorstores/atlas).\\n\\n```python\\nfrom langchain_community.vectorstores import AtlasDB\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='# AWS\\n\\nThe `LangChain` integrations related to [Amazon AWS](https://aws.amazon.com/) platform.\\n\\nFirst-party AWS integrations are available in the `langchain_aws` package.\\n\\n```bash\\npip install langchain-aws\\n```\\n\\nAnd there are also some community integrations available in the `langchain_community` package with the `boto3` optional dependency.\\n\\n```bash\\npip install langchain-community boto3\\n```\\n\\n## Chat models\\n\\n### Bedrock Chat'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content=\">[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of \\n> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`, \\n> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to \\n> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`, \\n> you can easily experiment with and evaluate top FMs for your use case, privately customize them with \\n> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build \\n> agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is \\n> serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy \\n> generative AI capabilities into your applications using the AWS services you are already familiar with.\"), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/chat/bedrock).\\n\\n```python\\nfrom langchain_aws import ChatBedrock\\n```\\n\\n### Bedrock Converse\\nAWS has recently released the Bedrock Converse API which provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all [models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). To improve reliability the ChatBedrock integration will switch to using the Bedrock Converse API as soon as it has feature parity with the existing Bedrock API. Until then a separate [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html) integration has been released.'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='We recommend using `ChatBedrockConverse` for users who do not need to use custom models. See the [docs](/docs/integrations/chat/bedrock/#bedrock-converse-api) and [API reference](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html) for more detail.\\n\\n```python\\nfrom langchain_aws import ChatBedrockConverse\\n```\\n\\n## LLMs\\n\\n### Bedrock\\n \\nSee a [usage example](/docs/integrations/llms/bedrock).\\n\\n```python\\nfrom langchain_aws import BedrockLLM\\n```\\n\\n### Amazon API Gateway'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='>[Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for \\n> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" \\n> for applications to access data, business logic, or functionality from your backend services. Using \\n> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication \\n> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.\\n> \\n> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of \\n> concurrent API calls, including traffic management, CORS support, authorization and access control, \\n> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs. \\n> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway` \\n> tiered pricing model, you can reduce your cost as your API usage scales.'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/llms/amazon_api_gateway).\\n\\n```python\\nfrom langchain_community.llms import AmazonAPIGateway\\n```\\n\\n### SageMaker Endpoint\\n\\n>[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy \\n> machine learning (ML) models with fully managed infrastructure, tools, and workflows.\\n\\nWe use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.\\n\\nSee a [usage example](/docs/integrations/llms/sagemaker).\\n\\n```python\\nfrom langchain_aws import SagemakerEndpoint\\n```\\n\\n## Embedding Models\\n\\n### Bedrock\\n\\nSee a [usage example](/docs/integrations/text_embedding/bedrock).\\n```python\\nfrom langchain_aws import BedrockEmbeddings\\n```\\n\\n### SageMaker Endpoint\\n\\nSee a [usage example](/docs/integrations/text_embedding/sagemaker-endpoint).\\n```python\\nfrom langchain_community.embeddings import SagemakerEndpointEmbeddings\\nfrom langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\\n```\\n\\n## Document loaders'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='### AWS S3 Directory and File\\n\\n>[Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)\\n> is an object storage service.\\n>[AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)\\n>[AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)\\n\\nSee a [usage example for S3DirectoryLoader](/docs/integrations/document_loaders/aws_s3_directory).\\n\\nSee a [usage example for S3FileLoader](/docs/integrations/document_loaders/aws_s3_file).\\n\\n```python\\nfrom langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\\n```\\n\\n### Amazon Textract\\n\\n>[Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine \\n> learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\\n\\nSee a [usage example](/docs/integrations/document_loaders/amazon_textract).'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import AmazonTextractPDFLoader\\n```\\n\\n### Amazon Athena\\n\\n>[Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built\\n>on open-source frameworks, supporting open-table and file formats.\\n\\nSee a [usage example](/docs/integrations/document_loaders/athena).\\n\\n```python\\nfrom langchain_community.document_loaders.athena import AthenaLoader\\n```\\n\\n### AWS Glue\\n\\n>The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata \\n> repository that allows you to manage, access, and share metadata about \\n> your data stored in AWS. It acts as a metadata store for your data assets, \\n> enabling various AWS services and your applications to query and connect \\n> to the data they need efficiently.\\n\\nSee a [usage example](/docs/integrations/document_loaders/glue_catalog).'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders.glue_catalog import GlueCatalogLoader\\n```\\n\\n## Vector stores\\n\\n### Amazon OpenSearch Service\\n\\n> [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs \\n> interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is \\n> an open source, \\n> distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the \\n> latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as \\n> visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`.\\n\\nWe need to install several python libraries.\\n\\n```bash\\npip install boto3 requests requests-aws4auth\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service).\\n\\n```python\\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\\n```\\n\\n### Amazon DocumentDB Vector Search'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='>[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\\n> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.\\n> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\\n\\n#### Installation and Setup\\n\\nSee [detail configuration instructions](/docs/integrations/vectorstores/documentdb).\\n\\nWe need to install the `pymongo` python package.\\n\\n```bash\\npip install pymongo\\n```\\n\\n#### Deploy DocumentDB on AWS\\n\\n[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/).\\n\\nSee a [usage example](/docs/integrations/vectorstores/documentdb).\\n\\n```python\\nfrom langchain_community.vectorstores import DocumentDBVectorSearch\\n```\\n### Amazon MemoryDB \\n[Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store, \\nenabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today. \\n\\nInMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.\\n\\n```python\\nfrom langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='vds = InMemoryVectorStore.from_documents(\\n            chunks,\\n            embeddings,\\n            redis_url=\"rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none\",\\n            vector_schema=vector_schema,\\n            index_name=INDEX_NAME,\\n        )\\n```\\nSee a [usage example](/docs/integrations/vectorstores/memorydb).\\n\\n## Retrievers\\n\\n### Amazon Kendra\\n\\n> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service \\n> provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine \\n> learning algorithms to enable powerful search capabilities across various data sources within an organization. \\n> `Kendra` is designed to help users find the information they need quickly and accurately, \\n> improving productivity and decision-making.'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='> With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases, \\n> manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and \\n> contextual meanings to provide highly relevant search results.\\n\\nWe need to install the `langchain-aws` library.\\n\\n```bash\\npip install langchain-aws\\n```\\n\\nSee a [usage example](/docs/integrations/retrievers/amazon_kendra_retriever).\\n\\n```python\\nfrom langchain_aws import AmazonKendraRetriever\\n```\\n\\n### Amazon Bedrock (Knowledge Bases)\\n\\n> [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an \\n> `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your \\n> private data to customize foundation model response.\\n\\nWe need to install the `langchain-aws` library.\\n\\n```bash\\npip install langchain-aws\\n```\\n\\nSee a [usage example](/docs/integrations/retrievers/bedrock).'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_aws import AmazonKnowledgeBasesRetriever\\n```\\n\\n## Tools\\n\\n### AWS Lambda\\n\\n>[`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by \\n> `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without \\n> provisioning or managing servers. This serverless architecture enables you to focus on writing and \\n> deploying code, while AWS automatically takes care of scaling, patching, and managing the \\n> infrastructure required to run your applications.\\n\\nWe need to install `boto3` python library.\\n\\n```bash\\npip install boto3\\n```\\n\\nSee a [usage example](/docs/integrations/tools/awslambda).\\n\\n## Memory\\n\\n### AWS DynamoDB'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='>[AWS DynamoDB](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dynamodb/index.html) \\n> is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability.\\n \\nWe have to configure the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). \\n\\nWe need to install the `boto3` library.\\n\\n```bash\\npip install boto3\\n```\\n\\nSee a [usage example](/docs/integrations/memory/aws_dynamodb).\\n\\n```python\\nfrom langchain_community.chat_message_histories import DynamoDBChatMessageHistory\\n```\\n\\n## Graphs\\n\\n### Amazon Neptune\\n\\n>[Amazon Neptune](https://aws.amazon.com/neptune/)\\n> is a high-performance graph analytics and serverless database for superior scalability and availability.\\n\\nFor the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.\\n\\n```bash\\npip install langchain-aws\\n```\\n\\n### Amazon Neptune with Cypher'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/graphs/amazon_neptune_open_cypher).\\n\\n```python\\nfrom langchain_aws.graphs import NeptuneGraph\\nfrom langchain_aws.graphs import NeptuneAnalyticsGraph\\nfrom langchain_aws.chains import create_neptune_opencypher_qa_chain\\n```\\n\\n### Amazon Neptune with SPARQL\\n\\nSee a [usage example](/docs/integrations/graphs/amazon_neptune_sparql).\\n\\n```python\\nfrom langchain_aws.graphs import NeptuneRdfGraph\\nfrom langchain_aws.chains import create_neptune_sparql_qa_chain\\n```\\n\\n## Callbacks\\n\\n### Bedrock token usage\\n\\n```python\\nfrom langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\\n```\\n\\n### SageMaker Tracking\\n\\n>[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly \\n> and easily build, train and deploy machine learning (ML) models.'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='>[Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability \\n> of `Amazon SageMaker` that lets you organize, track, \\n> compare and evaluate ML experiments and model versions.\\n \\nWe need to install several python libraries.\\n\\n```bash\\npip install google-search-results sagemaker\\n```\\n\\nSee a [usage example](/docs/integrations/callbacks/sagemaker_tracking).\\n\\n```python\\nfrom langchain_community.callbacks import SageMakerCallbackHandler\\n```\\n\\n## Chains\\n\\n### Amazon Comprehend Moderation Chain\\n\\n>[Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that \\n> uses machine learning to uncover valuable insights and connections in text.\\n\\n\\nWe need to install the `boto3` and `nltk` libraries.\\n\\n```bash\\npip install boto3 nltk\\n```\\n\\nSee a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon_comprehend_chain/).'), Document(metadata={'source': 'docs/docs/integrations/providers/aws.mdx', 'file_path': 'docs/docs/integrations/providers/aws.mdx', 'file_name': 'aws.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/azlyrics.mdx', 'file_path': 'docs/docs/integrations/providers/azlyrics.mdx', 'file_name': 'azlyrics.mdx', 'file_type': '.mdx'}, page_content=\"# AZLyrics\\n\\n>[AZLyrics](https://www.azlyrics.com/) is a large, legal, every day growing collection of lyrics.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/azlyrics).\\n\\n```python\\nfrom langchain_community.document_loaders import AZLyricsLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/azure_ai.mdx', 'file_path': 'docs/docs/integrations/providers/azure_ai.mdx', 'file_name': 'azure_ai.mdx', 'file_type': '.mdx'}, page_content='# Azure AI\\n\\nAll functionality related to [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) and its related projects.\\n\\nIntegration packages for Azure AI, Dynamic Sessions, SQL Server are maintained in \\nthe [langchain-azure](https://github.com/langchain-ai/langchain-azure) repository.\\n\\n## Chat models\\n\\nWe recommend developers start with the (`langchain-azure-ai`) to access all the models available in [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/model-catalog-overview). \\n\\n### Azure AI Chat Completions Model \\n\\nAccess models like Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral using the `AzureAIChatCompletionsModel` class.\\n\\n```bash\\npip install -U langchain-azure-ai\\n```\\n\\nConfigure your API key and Endpoint.\\n\\n```bash\\nexport AZURE_INFERENCE_CREDENTIAL=your-api-key\\nexport AZURE_INFERENCE_ENDPOINT=your-endpoint\\n```\\n\\n```python\\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel'), Document(metadata={'source': 'docs/docs/integrations/providers/azure_ai.mdx', 'file_path': 'docs/docs/integrations/providers/azure_ai.mdx', 'file_name': 'azure_ai.mdx', 'file_type': '.mdx'}, page_content='llm = AzureAIChatCompletionsModel(\\n    model_name=\"gpt-4o\",\\n    api_version=\"2024-05-01-preview\",\\n)\\n\\nllm.invoke(\\'Tell me a joke and include some emojis\\')\\n```\\n\\n## Embedding models\\n\\n### Azure AI model inference for embeddings\\n\\n```bash\\npip install -U langchain-azure-ai\\n```\\n\\nConfigure your API key and Endpoint.\\n\\n```bash\\nexport AZURE_INFERENCE_CREDENTIAL=your-api-key\\nexport AZURE_INFERENCE_ENDPOINT=your-endpoint\\n```\\n\\n```python\\nfrom langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\\n\\nembed_model = AzureAIEmbeddingsModel(\\n    model_name=\"text-embedding-ada-002\"\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/baai.mdx', 'file_path': 'docs/docs/integrations/providers/baai.mdx', 'file_name': 'baai.mdx', 'file_type': '.mdx'}, page_content=\"# BAAI\\n\\n>[Beijing Academy of Artificial Intelligence (BAAI) (Wikipedia)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence), \\n> also known as `Zhiyuan Institute`, is a Chinese non-profit artificial \\n> intelligence (AI) research laboratory. `BAAI` conducts AI research \\n> and is dedicated to promoting collaboration among academia and industry, \\n> as well as fostering top talent and a focus on long-term research on \\n> the fundamentals of AI technology. As a collaborative hub, BAAI's founding \\n> members include leading AI companies, universities, and research institutes.\\n\\n\\n## Embedding Models\\n\\n### HuggingFaceBgeEmbeddings\\n\\n>[BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) \\n> are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).\\n\\nSee a [usage example](/docs/integrations/text_embedding/bge_huggingface).\\n\\n```python\\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/baai.mdx', 'file_path': 'docs/docs/integrations/providers/baai.mdx', 'file_name': 'baai.mdx', 'file_type': '.mdx'}, page_content='### IpexLLMBgeEmbeddings\\n\\n>[IPEX-LLM](https://github.com/intel-analytics/ipex-llm) is a PyTorch \\n> library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, \\n> discrete GPU such as Arc, Flex and Max) with very low latency.\\n\\nSee a [usage example running model on Intel CPU](/docs/integrations/text_embedding/ipex_llm).\\nSee a [usage example running model on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu).\\n\\n```python\\nfrom langchain_community.embeddings import IpexLLMBgeEmbeddings\\n```\\n\\n### QuantizedBgeEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/itrex).\\n\\n```python\\nfrom langchain_community.embeddings import QuantizedBgeEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bagel.mdx', 'file_path': 'docs/docs/integrations/providers/bagel.mdx', 'file_name': 'bagel.mdx', 'file_type': '.mdx'}, page_content='# Bagel\\n\\n> [Bagel](https://www.bagel.net/) (`Open Vector Database for AI`), is like GitHub for AI data.\\nIt is a collaborative platform where users can create,\\nshare, and manage vector datasets. It can support private projects for independent developers,\\ninternal collaborations for enterprises, and public contributions for data DAOs.\\n\\n## Installation and Setup\\n\\n```bash\\npip install bagelML\\n```\\n\\n\\n## VectorStore\\n\\nSee a [usage example](/docs/integrations/vectorstores/bagel).\\n\\n```python\\nfrom langchain_community.vectorstores import Bagel\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bageldb.mdx', 'file_path': 'docs/docs/integrations/providers/bageldb.mdx', 'file_name': 'bageldb.mdx', 'file_type': '.mdx'}, page_content='# BagelDB\\n\\n> [BagelDB](https://www.bageldb.ai/) (`Open Vector Database for AI`), is like GitHub for AI data.\\nIt is a collaborative platform where users can create,\\nshare, and manage vector datasets. It can support private projects for independent developers,\\ninternal collaborations for enterprises, and public contributions for data DAOs.\\n\\n## Installation and Setup\\n\\n```bash\\npip install betabageldb\\n```\\n\\n\\n## VectorStore\\n\\nSee a [usage example](/docs/integrations/vectorstores/bageldb).\\n\\n```python\\nfrom langchain_community.vectorstores import Bagel\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/baichuan.mdx', 'file_path': 'docs/docs/integrations/providers/baichuan.mdx', 'file_name': 'baichuan.mdx', 'file_type': '.mdx'}, page_content='# Baichuan\\n\\n>[Baichuan Inc.](https://www.baichuan-ai.com/) is a Chinese startup in the era of AGI, \\n> dedicated to addressing fundamental human needs: Efficiency, Health, and Happiness.\\n\\n\\n## Installation and Setup\\n\\nRegister and get an API key [here](https://platform.baichuan-ai.com/).\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/baichuan).\\n\\n```python\\nfrom langchain_community.llms import BaichuanLLM\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/baichuan).\\n\\n```python\\nfrom langchain_community.chat_models import ChatBaichuan\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/baichuan).\\n\\n```python\\nfrom langchain_community.embeddings import BaichuanTextEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/baidu.mdx', 'file_path': 'docs/docs/integrations/providers/baidu.mdx', 'file_name': 'baidu.mdx', 'file_type': '.mdx'}, page_content='# Baidu\\n\\n>[Baidu Cloud](https://cloud.baidu.com/) is a cloud service provided by `Baidu, Inc.`, \\n> headquartered in Beijing. It offers a cloud storage service, client software, \\n> file management, resource sharing, and Third Party Integration.\\n\\n\\n## Installation and Setup\\n\\nRegister and get the `Qianfan` `AK` and `SK` keys [here](https://cloud.baidu.com/product/wenxinworkshop).\\n\\n## LLMs\\n\\n### Baidu Qianfan\\n\\nSee a [usage example](/docs/integrations/llms/baidu_qianfan_endpoint).\\n\\n```python\\nfrom langchain_community.llms import QianfanLLMEndpoint\\n```\\n\\n## Chat models\\n\\n### Qianfan Chat Endpoint\\n\\nSee a [usage example](/docs/integrations/chat/baidu_qianfan_endpoint).\\nSee another [usage example](/docs/integrations/chat/ernie).\\n\\n```python\\nfrom langchain_community.chat_models import QianfanChatEndpoint\\n```\\n\\n## Embedding models\\n\\n### Baidu Qianfan\\n\\nSee a [usage example](/docs/integrations/text_embedding/baidu_qianfan_endpoint).\\nSee another [usage example](/docs/integrations/text_embedding/ernie).'), Document(metadata={'source': 'docs/docs/integrations/providers/baidu.mdx', 'file_path': 'docs/docs/integrations/providers/baidu.mdx', 'file_name': 'baidu.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.embeddings import QianfanEmbeddingsEndpoint\\n```\\n\\n## Document loaders\\n\\n### Baidu BOS Directory Loader\\n\\n```python\\nfrom langchain_community.document_loaders.baiducloud_bos_directory import BaiduBOSDirectoryLoader\\n```\\n\\n### Baidu BOS File Loader\\n\\n```python\\nfrom langchain_community.document_loaders.baiducloud_bos_file import BaiduBOSFileLoader\\n```\\n\\n## Vector stores\\n\\n### Baidu Cloud ElasticSearch VectorSearch\\n\\nSee a [usage example](/docs/integrations/vectorstores/baiducloud_vector_search).\\n\\n```python\\nfrom langchain_community.vectorstores import BESVectorStore\\n```\\n\\n### Baidu VectorDB\\n\\nSee a [usage example](/docs/integrations/vectorstores/baiduvectordb).\\n\\n```python\\nfrom langchain_community.vectorstores import BaiduVectorDB\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bananadev.mdx', 'file_path': 'docs/docs/integrations/providers/bananadev.mdx', 'file_name': 'bananadev.mdx', 'file_type': '.mdx'}, page_content=\"# Banana\\n\\n>[Banana](https://www.banana.dev/) provided serverless GPU inference for AI models, \\n> a CI/CD build pipeline and a simple Python framework (`Potassium`) to server your models.\\n\\nThis page covers how to use the [Banana](https://www.banana.dev) ecosystem within LangChain.\\n\\n## Installation and Setup\\n\\n- Install the python package `banana-dev`:\\n\\n```bash\\npip install banana-dev\\n```\\n\\n- Get an Banana api key from the [Banana.dev dashboard](https://app.banana.dev) and set it as an environment variable (`BANANA_API_KEY`)\\n- Get your model's key and url slug from the model's details page.\\n\\n## Define your Banana Template\\n\\nYou'll need to set up a Github repo for your Banana app. You can get started in 5 minutes using [this guide](https://docs.banana.dev/banana-docs/).\\n\\nAlternatively, for a ready-to-go LLM example, you can check out Banana's [CodeLlama-7B-Instruct-GPTQ](https://github.com/bananaml/demo-codellama-7b-instruct-gptq) GitHub repository. Just fork it and deploy it within Banana.\"), Document(metadata={'source': 'docs/docs/integrations/providers/bananadev.mdx', 'file_path': 'docs/docs/integrations/providers/bananadev.mdx', 'file_name': 'bananadev.mdx', 'file_type': '.mdx'}, page_content=\"Other starter repos are available [here](https://github.com/orgs/bananaml/repositories?q=demo-&type=all&language=&sort=).\\n\\n## Build the Banana app\\n\\nTo use Banana apps within Langchain, you must include the `outputs` key \\nin the returned json, and the value must be a string.\\n\\n```python\\n# Return the results as a dictionary\\nresult = {'outputs': result}\\n```\\n\\nAn example inference function would be:\"), Document(metadata={'source': 'docs/docs/integrations/providers/bananadev.mdx', 'file_path': 'docs/docs/integrations/providers/bananadev.mdx', 'file_name': 'bananadev.mdx', 'file_type': '.mdx'}, page_content='```python\\n@app.handler(\"/\")\\ndef handler(context: dict, request: Request) -> Response:\\n    \"\"\"Handle a request to generate code from a prompt.\"\"\"\\n    model = context.get(\"model\")\\n    tokenizer = context.get(\"tokenizer\")\\n    max_new_tokens = request.json.get(\"max_new_tokens\", 512)\\n    temperature = request.json.get(\"temperature\", 0.7)\\n    prompt = request.json.get(\"prompt\")\\n    prompt_template=f\\'\\'\\'[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\\n    {prompt}\\n    [/INST]\\n    \\'\\'\\'\\n    input_ids = tokenizer(prompt_template, return_tensors=\\'pt\\').input_ids.cuda()\\n    output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens)\\n    result = tokenizer.decode(output[0])\\n    return Response(json={\"outputs\": result}, status=200)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bananadev.mdx', 'file_path': 'docs/docs/integrations/providers/bananadev.mdx', 'file_name': 'bananadev.mdx', 'file_type': '.mdx'}, page_content='This example is from the `app.py` file in [CodeLlama-7B-Instruct-GPTQ](https://github.com/bananaml/demo-codellama-7b-instruct-gptq).\\n\\n\\n## LLM\\n\\n\\n```python\\nfrom langchain_community.llms import Banana\\n```\\n\\nSee a [usage example](/docs/integrations/llms/banana).'), Document(metadata={'source': 'docs/docs/integrations/providers/beam.mdx', 'file_path': 'docs/docs/integrations/providers/beam.mdx', 'file_name': 'beam.mdx', 'file_type': '.mdx'}, page_content='# Beam\\n\\n>[Beam](https://www.beam.cloud/) is a cloud computing platform that allows you to run your code \\n> on remote servers with GPUs.\\n\\n\\n## Installation and Setup\\n\\n- [Create an account](https://www.beam.cloud/)\\n- Install the Beam CLI with `curl https://raw.githubusercontent.com/slai-labs/get-beam/main/get-beam.sh -sSfL | sh`\\n- Register API keys with `beam configure`\\n- Set environment variables (`BEAM_CLIENT_ID`) and (`BEAM_CLIENT_SECRET`)\\n- Install the Beam SDK:\\n\\n```bash\\npip install beam-sdk\\n```\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/beam).\\n\\nSee another example in the [Beam documentation](https://docs.beam.cloud/examples/langchain).\\n\\n```python\\nfrom langchain_community.llms.beam import Beam\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/beautiful_soup.mdx', 'file_path': 'docs/docs/integrations/providers/beautiful_soup.mdx', 'file_name': 'beautiful_soup.mdx', 'file_type': '.mdx'}, page_content='# Beautiful Soup\\n\\n>[Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) is a Python package for parsing \\n> HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). \\n> It creates a parse tree for parsed pages that can be used to extract data from HTML,[3] which \\n> is useful for web scraping.\\n\\n## Installation and Setup\\n\\n```bash\\npip install beautifulsoup4\\n```\\n\\n## Document Transformer\\n\\nSee a [usage example](/docs/integrations/document_transformers/beautiful_soup).\\n\\n```python\\nfrom langchain_community.document_loaders import BeautifulSoupTransformer\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bibtex.mdx', 'file_path': 'docs/docs/integrations/providers/bibtex.mdx', 'file_name': 'bibtex.mdx', 'file_type': '.mdx'}, page_content='# BibTeX\\n\\n>[BibTeX](https://www.ctan.org/pkg/bibtex) is a file format and reference management system commonly used in conjunction with `LaTeX` typesetting. It serves as a way to organize and store bibliographic information for academic and research documents.\\n\\n## Installation and Setup\\n\\nWe have to install the `bibtexparser` and `pymupdf` packages.\\n\\n```bash\\npip install bibtexparser pymupdf\\n```\\n\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/bibtex).\\n\\n```python\\nfrom langchain_community.document_loaders import BibtexLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bilibili.mdx', 'file_path': 'docs/docs/integrations/providers/bilibili.mdx', 'file_name': 'bilibili.mdx', 'file_type': '.mdx'}, page_content='# BiliBili\\n\\n>[Bilibili](https://www.bilibili.tv/) is one of the most beloved long-form video sites in China.\\n\\n## Installation and Setup\\n\\n```bash\\npip install bilibili-api-python\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/bilibili).\\n\\n```python\\nfrom langchain_community.document_loaders import BiliBiliLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bittensor.mdx', 'file_path': 'docs/docs/integrations/providers/bittensor.mdx', 'file_name': 'bittensor.mdx', 'file_type': '.mdx'}, page_content='# Bittensor\\n\\n>[Neural Internet Bittensor](https://neuralinternet.ai/) network, an open source protocol \\n> that powers a decentralized, blockchain-based, machine learning network.\\n\\n## Installation and Setup\\n\\nGet your API_KEY from [Neural Internet](https://neuralinternet.ai/).\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/bittensor).\\n\\n```python\\nfrom langchain_community.llms import NIBittensorLLM\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/blackboard.mdx', 'file_path': 'docs/docs/integrations/providers/blackboard.mdx', 'file_name': 'blackboard.mdx', 'file_type': '.mdx'}, page_content=\"# Blackboard\\n\\n>[Blackboard Learn](https://en.wikipedia.org/wiki/Blackboard_Learn) (previously the `Blackboard Learning Management System`)\\n> is a web-based virtual learning environment and learning management system developed by Blackboard Inc. \\n> The software features course management, customizable open architecture, and scalable design that allows \\n> integration with student information systems and authentication protocols. It may be installed on local servers, \\n> hosted by `Blackboard ASP Solutions`, or provided as Software as a Service hosted on Amazon Web Services. \\n> Its main purposes are stated to include the addition of online elements to courses traditionally delivered \\n> face-to-face and development of completely online courses with few or no face-to-face meetings.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/blackboard).\"), Document(metadata={'source': 'docs/docs/integrations/providers/blackboard.mdx', 'file_path': 'docs/docs/integrations/providers/blackboard.mdx', 'file_name': 'blackboard.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import BlackboardLoader\\n\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/bookendai.mdx', 'file_path': 'docs/docs/integrations/providers/bookendai.mdx', 'file_name': 'bookendai.mdx', 'file_type': '.mdx'}, page_content='# bookend.ai\\n\\nLangChain implements an integration with embeddings provided by [bookend.ai](https://bookend.ai/).\\n\\n\\n## Installation and Setup\\n\\n\\nYou need to register and get the `API_KEY` \\nfrom the [bookend.ai](https://bookend.ai/) website.\\n\\n## Embedding model\\n\\nSee a [usage example](/docs/integrations/text_embedding/bookend).\\n\\n```python\\nfrom langchain_community.embeddings import BookendEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content='# Box\\n\\n[Box](https://box.com) is the Intelligent Content Cloud, a single platform that enables \\norganizations to fuel collaboration, manage the entire content lifecycle, secure critical content, \\nand transform business workflows with enterprise AI. Founded in 2005, Box simplifies work for \\nleading global organizations, including AstraZeneca, JLL, Morgan Stanley, and Nationwide.\\n\\nIn this package, we make available a number of ways to include Box content in your AI workflows. \\n\\n### Installation and setup\\n\\n```bash\\npip install -U langchain-box\\n\\n```\\n\\n# langchain-box\\n\\nThis package contains the LangChain integration with Box. For more information about\\nBox, check out our [developer documentation](https://developer.box.com).\\n\\n## Pre-requisites\\n\\nIn order to integrate with Box, you need a few things:'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content='* A Box instance — if you are not a current Box customer, sign up for a \\n[free dev account](https://account.box.com/signup/n/developer#ty9l3).\\n* A Box app — more on how to \\n[create an app](https://developer.box.com/guides/getting-started/first-application/)\\n* Your app approved in your Box instance — This is done by your admin.\\nThe good news is if you are using a free developer account, you are the admin.\\n[Authorize your app](https://developer.box.com/guides/authorization/custom-app-approval/#manual-approval)\\n\\n## Authentication'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content='The `box-langchain` package offers some flexibility to authentication. The\\nmost basic authentication method is by using a developer token. This can be\\nfound in the [Box developer console](https://account.box.com/developers/console) \\non the configuration screen. This token is purposely short-lived (1 hour) and is \\nintended for development. With this token, you can add it to your environment as \\n`BOX_DEVELOPER_TOKEN`, you can pass it directly to the loader, or you can use the \\n`BoxAuth` authentication helper class.\\n\\nWe will cover passing it directly to the loader in the section below. \\n\\n### BoxAuth helper class\\n\\n`BoxAuth` supports the following authentication methods:\\n\\n* Token — either a developer token or any token generated through the Box SDK\\n* JWT with a service account\\n* JWT with a specified user\\n* CCG with a service account\\n* CCG with a specified user'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content=':::note\\nIf using JWT authentication, you will need to download the configuration from the Box\\ndeveloper console after generating your public/private key pair. Place this file in your \\napplication directory structure somewhere. You will use the path to this file when using\\nthe `BoxAuth` helper class.\\n:::\\n\\nFor more information, learn about how to \\n[set up a Box application](https://developer.box.com/guides/getting-started/first-application/),\\nand check out the \\n[Box authentication guide](https://developer.box.com/guides/authentication/select/)\\nfor more about our different authentication options.\\n\\nExamples:\\n\\n**Token**\\n\\n```python\\nfrom langchain_box.document_loaders import BoxLoader\\nfrom langchain_box.utilities import BoxAuth, BoxAuthType\\n\\nauth = BoxAuth(\\n    auth_type=BoxAuthType.TOKEN,\\n    box_developer_token=box_developer_token\\n)\\n\\nloader = BoxLoader(\\n    box_auth=auth,\\n    ...\\n)\\n```\\n\\n**JWT with a service account**'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_box.document_loaders import BoxLoader\\nfrom langchain_box.utilities import BoxAuth, BoxAuthType\\n\\nauth = BoxAuth(\\n    auth_type=BoxAuthType.JWT,\\n    box_jwt_path=box_jwt_path\\n)\\n\\nloader = BoxLoader(\\n    box_auth=auth,\\n    ...\\n```\\n\\n**JWT with a specified user**\\n\\n```python\\nfrom langchain_box.document_loaders import BoxLoader\\nfrom langchain_box.utilities import BoxAuth, BoxAuthType\\n\\nauth = BoxAuth(\\n    auth_type=BoxAuthType.JWT,\\n    box_jwt_path=box_jwt_path,\\n    box_user_id=box_user_id\\n)\\n\\nloader = BoxLoader(\\n    box_auth=auth,\\n    ...\\n```\\n\\n**CCG with a service account**\\n\\n```python\\nfrom langchain_box.document_loaders import BoxLoader\\nfrom langchain_box.utilities import BoxAuth, BoxAuthType\\n\\nauth = BoxAuth(\\n    auth_type=BoxAuthType.CCG,\\n    box_client_id=box_client_id,\\n    box_client_secret=box_client_secret,\\n    box_enterprise_id=box_enterprise_id\\n)\\n\\nloader = BoxLoader(\\n    box_auth=auth,\\n    ...\\n```\\n\\n**CCG with a specified user**'), Document(metadata={'source': 'docs/docs/integrations/providers/box.mdx', 'file_path': 'docs/docs/integrations/providers/box.mdx', 'file_name': 'box.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_box.document_loaders import BoxLoader\\nfrom langchain_box.utilities import BoxAuth, BoxAuthType\\n\\nauth = BoxAuth(\\n    auth_type=BoxAuthType.CCG,\\n    box_client_id=box_client_id,\\n    box_client_secret=box_client_secret,\\n    box_user_id=box_user_id\\n)\\n\\nloader = BoxLoader(\\n    box_auth=auth,\\n    ...\\n```\\n\\nIf you wish to use OAuth2 with the authorization_code flow, please use `BoxAuthType.TOKEN` with the token you have acquired.\\n\\n## Document Loaders\\n\\n### BoxLoader\\n\\n[See usage example](/docs/integrations/document_loaders/box)\\n\\n```python\\nfrom langchain_box.document_loaders import BoxLoader\\n\\n```\\n\\n## Retrievers\\n\\n### BoxRetriever\\n\\n[See usage example](/docs/integrations/retrievers/box)\\n\\n```python\\nfrom langchain_box.retrievers import BoxRetriever\\n\\n```\\n\\n## Blob Loaders\\n\\n### BoxBlobLoader\\n\\n[See usage example](/docs/integrations/document_loaders/box)\\n\\n```python\\nfrom langchain_box.blob_loaders import BoxBlobLoader\\n\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/brave_search.mdx', 'file_path': 'docs/docs/integrations/providers/brave_search.mdx', 'file_name': 'brave_search.mdx', 'file_type': '.mdx'}, page_content='# Brave Search'), Document(metadata={'source': 'docs/docs/integrations/providers/brave_search.mdx', 'file_path': 'docs/docs/integrations/providers/brave_search.mdx', 'file_name': 'brave_search.mdx', 'file_type': '.mdx'}, page_content='>[Brave Search](https://en.wikipedia.org/wiki/Brave_Search) is a search engine developed by Brave Software.\\n> - `Brave Search` uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92% \\n> of search results without relying on any third-parties, with the remainder being retrieved \\n> server-side from the Bing API or (on an opt-in basis) client-side from Google. According \\n> to Brave, the index was kept \"intentionally smaller than that of Google or Bing\" in order to \\n> help avoid spam and other low-quality content, with the disadvantage that \"Brave Search is \\n> not yet as good as Google in recovering long-tail queries.\"\\n>- `Brave Search Premium`: As of April 2023 Brave Search is an ad-free website, but it will \\n> eventually switch to a new model that will include ads and premium users will get an ad-free experience.\\n> User data including IP addresses won\\'t be collected from its users by default. A premium account \\n> will be required for opt-in data-collection.'), Document(metadata={'source': 'docs/docs/integrations/providers/brave_search.mdx', 'file_path': 'docs/docs/integrations/providers/brave_search.mdx', 'file_name': 'brave_search.mdx', 'file_type': '.mdx'}, page_content='## Installation and Setup\\n\\nTo get access to the Brave Search API, you need to [create an account and get an API key](https://api.search.brave.com/app/dashboard).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/brave_search).\\n\\n```python\\nfrom langchain_community.document_loaders import BraveSearchLoader\\n```\\n\\n## Tool\\n\\nSee a [usage example](/docs/integrations/tools/brave_search).\\n\\n```python\\nfrom langchain.tools import BraveSearch\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/browserbase.mdx', 'file_path': 'docs/docs/integrations/providers/browserbase.mdx', 'file_name': 'browserbase.mdx', 'file_type': '.mdx'}, page_content='# Browserbase\\n\\n[Browserbase](https://browserbase.com) is a developer platform to reliably run, manage, and monitor headless browsers.\\n\\nPower your AI data retrievals with:\\n- [Serverless Infrastructure](https://docs.browserbase.com/under-the-hood) providing reliable browsers to extract data from complex UIs\\n- [Stealth Mode](https://docs.browserbase.com/features/stealth-mode) with included fingerprinting tactics and automatic captcha solving\\n- [Session Debugger](https://docs.browserbase.com/features/sessions) to inspect your Browser Session with networks timeline and logs\\n- [Live Debug](https://docs.browserbase.com/guides/session-debug-connection/browser-remote-control) to quickly debug your automation\\n\\n## Installation and Setup\\n\\n- Get an API key and Project ID from [browserbase.com](https://browserbase.com) and set it in environment variables (`BROWSERBASE_API_KEY`, `BROWSERBASE_PROJECT_ID`).\\n- Install the [Browserbase SDK](http://github.com/browserbase/python-sdk):'), Document(metadata={'source': 'docs/docs/integrations/providers/browserbase.mdx', 'file_path': 'docs/docs/integrations/providers/browserbase.mdx', 'file_name': 'browserbase.mdx', 'file_type': '.mdx'}, page_content='```python\\npip install browserbase\\n```\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/browserbase).\\n\\n```python\\nfrom langchain_community.document_loaders import BrowserbaseLoader\\n```\\n\\n## Multi-Modal\\n\\nSee a [usage example](/docs/integrations/document_loaders/browserbase).\\n\\n```python\\nfrom browserbase.helpers.gpt4 import GPT4VImage, GPT4VImageDetail\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/browserless.mdx', 'file_path': 'docs/docs/integrations/providers/browserless.mdx', 'file_name': 'browserless.mdx', 'file_type': '.mdx'}, page_content='# Browserless\\n\\n>[Browserless](https://www.browserless.io/docs/start) is a service that allows you to \\n> run headless Chrome instances in the cloud. It’s a great way to run browser-based \\n> automation at scale without having to worry about managing your own infrastructure.\\n\\n## Installation and Setup\\n\\nWe have to get the API key [here](https://www.browserless.io/pricing/).\\n\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/browserless).\\n\\n```python\\nfrom langchain_community.document_loaders import BrowserlessLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/byte_dance.mdx', 'file_path': 'docs/docs/integrations/providers/byte_dance.mdx', 'file_name': 'byte_dance.mdx', 'file_type': '.mdx'}, page_content='# ByteDance\\n\\n>[ByteDance](https://bytedance.com/) is a Chinese internet technology company.\\n\\n## Installation and Setup\\n\\nGet the access token.\\nYou can find the access instructions [here](https://open.larksuite.com/document)\\n\\n\\n## Document Loaders\\n\\n>[Lark Suite](https://www.larksuite.com/) is an enterprise collaboration platform \\n> developed by `ByteDance`.\\n\\n### Lark Suite for Document\\n\\nSee a [usage example](/docs/integrations/document_loaders/larksuite/#load-from-document).\\n\\n```python\\nfrom langchain_community.document_loaders.larksuite import LarkSuiteDocLoader\\n```\\n\\n### Lark Suite for Wiki\\n\\nSee a [usage example](/docs/integrations/document_loaders/larksuite/#load-from-wiki).\\n\\n```python\\nfrom langchain_community.document_loaders.larksuite import LarkSuiteWikiLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cassandra.mdx', 'file_path': 'docs/docs/integrations/providers/cassandra.mdx', 'file_name': 'cassandra.mdx', 'file_type': '.mdx'}, page_content='# Cassandra\\n\\n> [Apache Cassandra®](https://cassandra.apache.org/) is a NoSQL, row-oriented, highly scalable and highly available database.\\n> Starting with version 5.0, the database ships with [vector search capabilities](https://cassandra.apache.org/doc/trunk/cassandra/vector-search/overview.html).\\n\\nThe integrations outlined in this page can be used with `Cassandra` as well as other CQL-compatible databases, \\ni.e. those using the `Cassandra Query Language` protocol.\\n\\n\\n## Installation and Setup\\n\\nInstall the following Python package:\\n\\n```bash\\npip install \"cassio>=0.1.6\"\\n```\\n\\n## Vector Store\\n\\n```python\\nfrom langchain_community.vectorstores import Cassandra\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/vectorstores/cassandra).\\n\\n## Chat message history\\n\\n```python\\nfrom langchain_community.chat_message_histories import CassandraChatMessageHistory\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/memory/cassandra_chat_message_history).\\n\\n\\n## LLM Cache'), Document(metadata={'source': 'docs/docs/integrations/providers/cassandra.mdx', 'file_path': 'docs/docs/integrations/providers/cassandra.mdx', 'file_name': 'cassandra.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_community.cache import CassandraCache\\nset_llm_cache(CassandraCache())\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/llm_caching#cassandra-caches) (scroll to the Cassandra section).\\n\\n\\n## Semantic LLM Cache\\n\\n```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_community.cache import CassandraSemanticCache\\nset_llm_cache(CassandraSemanticCache(\\n    embedding=my_embedding,\\n    table_name=\"my_store\",\\n))\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/llm_caching#cassandra-caches) (scroll to the appropriate section).\\n\\n## Document loader\\n\\n```python\\nfrom langchain_community.document_loaders import CassandraLoader\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/document_loaders/cassandra).\\n\\n#### Attribution statement'), Document(metadata={'source': 'docs/docs/integrations/providers/cassandra.mdx', 'file_path': 'docs/docs/integrations/providers/cassandra.mdx', 'file_name': 'cassandra.mdx', 'file_type': '.mdx'}, page_content='> Apache Cassandra, Cassandra and Apache are either registered trademarks or trademarks of \\n> the [Apache Software Foundation](http://www.apache.org/) in the United States and/or other countries.\\n\\n## Toolkit\\n\\nThe `Cassandra Database toolkit` enables AI engineers to efficiently integrate agents\\nwith Cassandra data.\\n\\n```python\\nfrom langchain_community.agent_toolkits.cassandra_database.toolkit import (\\n    CassandraDatabaseToolkit,\\n)\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/tools/cassandra_database).\\n\\n\\nCassandra Database individual tools:\\n\\n### Get Schema\\n\\nTool for getting the schema of a keyspace in an Apache Cassandra database.\\n\\n```python\\nfrom langchain_community.tools import GetSchemaCassandraDatabaseTool\\n```\\n\\n### Get Table Data\\n\\nTool for getting data from a table in an Apache Cassandra database.\\n\\n```python\\nfrom langchain_community.tools import GetTableDataCassandraDatabaseTool\\n```\\n\\n### Query\\n\\nTool for querying an Apache Cassandra database with provided CQL.'), Document(metadata={'source': 'docs/docs/integrations/providers/cassandra.mdx', 'file_path': 'docs/docs/integrations/providers/cassandra.mdx', 'file_name': 'cassandra.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.tools import QueryCassandraDatabaseTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cerebras.mdx', 'file_path': 'docs/docs/integrations/providers/cerebras.mdx', 'file_name': 'cerebras.mdx', 'file_type': '.mdx'}, page_content=\"# Cerebras\\n\\nAt Cerebras, we've developed the world's largest and fastest AI processor, the Wafer-Scale Engine-3 (WSE-3). The Cerebras CS-3 system, powered by the WSE-3, represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.\\n\\nWith Cerebras as your inference provider, you can:\\n- Achieve unprecedented speed for AI inference workloads\\n- Build commercially with high throughput\\n- Effortlessly scale your AI workloads with our seamless clustering technology\\n\\nOur CS-3 systems can be quickly and easily clustered to create the largest AI supercomputers in the world, making it simple to place and run the largest models. Leading corporations, research institutions, and governments are already using Cerebras solutions to develop proprietary models and train popular open-source models.\"), Document(metadata={'source': 'docs/docs/integrations/providers/cerebras.mdx', 'file_path': 'docs/docs/integrations/providers/cerebras.mdx', 'file_name': 'cerebras.mdx', 'file_type': '.mdx'}, page_content='Want to experience the power of Cerebras? Check out our [website](https://cerebras.ai) for more resources and explore options for accessing our technology through the Cerebras Cloud or on-premise deployments!\\n\\nFor more information about Cerebras Cloud, visit [cloud.cerebras.ai](https://cloud.cerebras.ai/). Our API reference is available at [inference-docs.cerebras.ai](https://inference-docs.cerebras.ai/).\\n\\n## Installation and Setup\\nInstall the integration package:\\n\\n```bash\\npip install langchain-cerebras\\n```\\n\\n## API Key\\nGet an API Key from [cloud.cerebras.ai](https://cloud.cerebras.ai/) and add it to your environment variables:\\n```\\nexport CEREBRAS_API_KEY=\"your-api-key-here\"\\n```\\n\\n## Chat Model\\nSee a [usage example](/docs/integrations/chat/cerebras).'), Document(metadata={'source': 'docs/docs/integrations/providers/cerebriumai.mdx', 'file_path': 'docs/docs/integrations/providers/cerebriumai.mdx', 'file_name': 'cerebriumai.mdx', 'file_type': '.mdx'}, page_content='# CerebriumAI\\n\\n>[Cerebrium](https://docs.cerebrium.ai/cerebrium/getting-started/introduction)  is a serverless GPU infrastructure provider.\\n> It provides API access to several LLM models.\\n\\nSee the examples in the [CerebriumAI documentation](https://docs.cerebrium.ai/examples/langchain).\\n\\n## Installation and Setup\\n\\n- Install a python package:\\n```bash\\npip install cerebrium\\n```\\n\\n- [Get an CerebriumAI api key](https://docs.cerebrium.ai/cerebrium/getting-started/installation) and set \\n  it as an environment variable (`CEREBRIUMAI_API_KEY`)\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/cerebriumai).\\n\\n\\n```python\\nfrom langchain_community.llms import CerebriumAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/chaindesk.mdx', 'file_path': 'docs/docs/integrations/providers/chaindesk.mdx', 'file_name': 'chaindesk.mdx', 'file_type': '.mdx'}, page_content='# Chaindesk\\n\\n>[Chaindesk](https://chaindesk.ai) is an [open-source](https://github.com/gmpetrov/databerry) document retrieval platform that helps to connect your personal data with Large Language Models.\\n\\n\\n## Installation and Setup\\n\\nWe need to sign up for Chaindesk, create a datastore, add some data and get your datastore api endpoint url. \\nWe need the [API Key](https://docs.chaindesk.ai/api-reference/authentication).\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/chaindesk).\\n\\n```python\\nfrom langchain.retrievers import ChaindeskRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/chroma.mdx', 'file_path': 'docs/docs/integrations/providers/chroma.mdx', 'file_name': 'chroma.mdx', 'file_type': '.mdx'}, page_content='# Chroma\\n\\n>[Chroma](https://docs.trychroma.com/getting-started) is a database for building AI applications with embeddings.\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-chroma\\n```\\n\\n\\n## VectorStore\\n\\nThere exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain_chroma import Chroma\\n```\\n\\nFor a more detailed walkthrough of the Chroma wrapper, see [this notebook](/docs/integrations/vectorstores/chroma)\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/self_query/chroma_self_query).\\n\\n```python\\nfrom langchain.retrievers import SelfQueryRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content='# Clarifai'), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content=\">[Clarifai](https://clarifai.com) is one of first deep learning platforms having been founded in 2013. Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference around images, video, text and audio data. In the LangChain ecosystem, as far as we're aware, Clarifai is the only provider that supports LLMs, embeddings and a vector store in one production scale platform, making it an excellent choice to operationalize your LangChain implementations.\\n>\\n> `Clarifai` provides 1,000s of AI models for many different use cases. You can [explore them here](https://clarifai.com/explore) to find the one most suited for your use case. These models include those created by other providers such as OpenAI, Anthropic, Cohere, AI21, etc. as well as state of the art from open source such as Falcon, InstructorXL, etc. so that you build the best in AI into your products. You'll find these organized by the creator's user_id and into projects we call applications denoted by their app_id. Those IDs will be needed in additional to the model_id and optionally the version_id, so make note of all these IDs once you found the best model for your use case!\\n>\\n>Also note that given there are many models for images, video, text and audio understanding, you can build some interested AI agents that utilize the variety of AI models as experts to understand those data types.\"), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content='## Installation and Setup\\n- Install the Python SDK:\\n```bash\\npip install clarifai\\n```\\n[Sign-up](https://clarifai.com/signup) for a Clarifai account, then get a personal access token to access the Clarifai API from your [security settings](https://clarifai.com/settings/security) and set it as an environment variable (`CLARIFAI_PAT`).\\n\\n\\n## LLMs\\n\\nTo find the selection of LLMs in the Clarifai platform you can select the text to text model type [here](https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-to-text%22%5D%7D%5D&page=1&perPage=24).\\n\\n```python\\nfrom langchain_community.llms import Clarifai\\nllm = Clarifai(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\\n```\\n\\nFor more details, the docs on the Clarifai LLM wrapper provide a [detailed walkthrough](/docs/integrations/llms/clarifai).\\n\\n\\n## Embedding Models'), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content='To find the selection of embeddings models in the Clarifai platform you can select the text to embedding model type [here](https://clarifai.com/explore/models?page=1&perPage=24&filterData=%5B%7B%22field%22%3A%22model_type_id%22%2C%22value%22%3A%5B%22text-embedder%22%5D%7D%5D).\\n\\nThere is a Clarifai Embedding model in LangChain, which you can access with:\\n```python\\nfrom langchain_community.embeddings import ClarifaiEmbeddings\\nembeddings = ClarifaiEmbeddings(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/couchbase).\\n\\n\\n## Vectorstore'), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content=\"Clarifai's vector DB was launched in 2016 and has been optimized to support live search queries. With workflows in the Clarifai platform, you data is automatically indexed by am embedding model and optionally other models as well to index that information in the DB for search. You can query the DB not only via the vectors but also filter by metadata matches, other AI predicted concepts, and even do geo-coordinate search. Simply create an application, select the appropriate base workflow for your type of data, and upload it (through the API as [documented here](https://docs.clarifai.com/api-guide/data/create-get-update-delete) or the UIs at clarifai.com).\"), Document(metadata={'source': 'docs/docs/integrations/providers/clarifai.mdx', 'file_path': 'docs/docs/integrations/providers/clarifai.mdx', 'file_name': 'clarifai.mdx', 'file_type': '.mdx'}, page_content=\"You can also add data directly from LangChain as well, and the auto-indexing will take place for you. You'll notice this is a little different than other vectorstores where you need to provide an embedding model in their constructor and have LangChain coordinate getting the embeddings from text and writing those to the index. Not only is it more convenient, but it's much more scalable to use Clarifai's distributed cloud to do all the index in the background.\\n\\n```python\\nfrom langchain_community.vectorstores import Clarifai\\nclarifai_vector_db = Clarifai.from_texts(user_id=USER_ID, app_id=APP_ID, texts=texts, pat=CLARIFAI_PAT, number_of_docs=NUMBER_OF_DOCS, metadatas = metadatas)\\n```\\nFor more details, the docs on the Clarifai vector store provide a [detailed walkthrough](/docs/integrations/vectorstores/clarifai).\"), Document(metadata={'source': 'docs/docs/integrations/providers/clickhouse.mdx', 'file_path': 'docs/docs/integrations/providers/clickhouse.mdx', 'file_name': 'clickhouse.mdx', 'file_type': '.mdx'}, page_content='# ClickHouse\\n\\n> [ClickHouse](https://clickhouse.com/) is the fast and resource efficient open-source database for real-time \\n> apps and analytics with full SQL support and a wide range of functions to assist users in writing analytical queries. \\n> It has data structures and distance search functions (like `L2Distance`) as well as \\n> [approximate nearest neighbor search indexes](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/annindexes) \\n> That enables ClickHouse to be used as a high performance and scalable vector database to store and search vectors with SQL.\\n\\n\\n## Installation and Setup\\n\\nWe need to install `clickhouse-connect` python package.\\n\\n```bash\\npip install clickhouse-connect\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/clickhouse).\\n\\n```python\\nfrom langchain_community.vectorstores import Clickhouse, ClickhouseSettings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/clickup.mdx', 'file_path': 'docs/docs/integrations/providers/clickup.mdx', 'file_name': 'clickup.mdx', 'file_type': '.mdx'}, page_content='# ClickUp\\n\\n>[ClickUp](https://clickup.com/) is an all-in-one productivity platform that provides small and large teams across industries with flexible and customizable work management solutions, tools, and functions.\\n>\\n>It is a cloud-based project management solution for businesses of all sizes featuring communication and collaboration tools to help achieve organizational goals.\\n\\n## Installation and Setup\\n\\n1. Create a [ClickUp App](https://help.clickup.com/hc/en-us/articles/6303422883095-Create-your-own-app-with-the-ClickUp-API)\\n2. Follow [these steps](https://clickup.com/api/developer-portal/authentication/) to get your client_id and client_secret.\\n\\n## Toolkits\\n\\n```python\\nfrom langchain_community.agent_toolkits.clickup.toolkit import ClickupToolkit\\nfrom langchain_community.utilities.clickup import ClickupAPIWrapper\\n```\\n\\nSee a [usage example](/docs/integrations/tools/clickup).'), Document(metadata={'source': 'docs/docs/integrations/providers/cloudflare.mdx', 'file_path': 'docs/docs/integrations/providers/cloudflare.mdx', 'file_name': 'cloudflare.mdx', 'file_type': '.mdx'}, page_content='# Cloudflare\\n\\n>[Cloudflare, Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Cloudflare) is an American company that provides \\n> content delivery network services, cloud cybersecurity, DDoS mitigation, and ICANN-accredited \\n> domain registration services.\\n\\n>[Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine \\n> learning models, on the `Cloudflare` network, from your code via REST API.\\n\\n\\n## LLMs\\n\\nSee [installation instructions and usage example](/docs/integrations/llms/cloudflare_workersai).\\n\\n```python\\nfrom langchain_community.llms.cloudflare_workersai import CloudflareWorkersAI\\n```\\n\\n## Embedding models\\n\\nSee [installation instructions and usage example](/docs/integrations/text_embedding/cloudflare_workersai).\\n\\n```python\\nfrom langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/clova.mdx', 'file_path': 'docs/docs/integrations/providers/clova.mdx', 'file_name': 'clova.mdx', 'file_type': '.mdx'}, page_content='# Clova\\n\\n>[CLOVA Studio](https://api.ncloud-docs.com/docs/ai-naver-clovastudio-summary) is a service \\n> of [Naver Cloud Platform](https://www.ncloud.com/) that uses `HyperCLOVA` language models, \\n> a hyperscale AI technology, to output phrases generated through AI technology based on user input.\\n\\n\\n## Embedding models\\n\\nSee [installation instructions and usage example](/docs/integrations/text_embedding/clova).\\n\\n```python\\nfrom langchain_community.embeddings import ClovaEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='# CnosDB\\n> [CnosDB](https://github.com/cnosdb/cnosdb) is an open-source distributed time series database with high performance, high compression rate and high ease of use.\\n\\n## Installation and Setup\\n\\n```python\\npip install cnos-connector\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='## Connecting to CnosDB\\nYou can connect to CnosDB using the `SQLDatabase.from_cnosdb()` method.\\n### Syntax\\n```python\\ndef SQLDatabase.from_cnosdb(url: str = \"127.0.0.1:8902\",\\n                              user: str = \"root\",\\n                              password: str = \"\",\\n                              tenant: str = \"cnosdb\",\\n                              database: str = \"public\")\\n```\\nArgs:\\n1. url (str): The HTTP connection host name and port number of the CnosDB\\n                service, excluding \"http://\" or \"https://\", with a default value\\n                of \"127.0.0.1:8902\".\\n2. user (str): The username used to connect to the CnosDB service, with a\\n                default value of \"root\".\\n3. password (str): The password of the user connecting to the CnosDB service,\\n                with a default value of \"\".\\n4. tenant (str): The name of the tenant used to connect to the CnosDB service,\\n                with a default value of \"cnosdb\".\\n5. database (str): The name of the database in the CnosDB tenant.\\n## Examples\\n```python\\n# Connecting to CnosDB with SQLDatabase Wrapper\\nfrom langchain_community.utilities import SQLDatabase'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='db = SQLDatabase.from_cnosdb()\\n```\\n```python\\n# Creating a OpenAI Chat LLM Wrapper\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\\n```\\n\\n### SQL Database Chain\\nThis example demonstrates the use of the SQL Chain for answering a question over a CnosDB.\\n```python\\nfrom langchain_community.utilities import SQLDatabaseChain\\n\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='db_chain.run(\\n    \"What is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022?\"\\n)\\n```\\n```shell\\n> Entering new  chain...\\nWhat is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022?\\nSQLQuery:SELECT AVG(temperature) FROM air WHERE station = \\'XiaoMaiDao\\' AND time >= \\'2022-10-19\\' AND time < \\'2022-10-20\\'\\nSQLResult: [(68.0,)]\\nAnswer:The average temperature of air at station XiaoMaiDao between October 19, 2022 and October 20, 2022 is 68.0.\\n> Finished chain.\\n```\\n### SQL Database Agent\\nThis example demonstrates the use of the SQL Database Agent for answering questions over a CnosDB.\\n```python\\nfrom langchain.agents import create_sql_agent\\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='toolkit = SQLDatabaseToolkit(db=db, llm=llm)\\nagent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\\n```\\n```python\\nagent.run(\\n    \"What is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022?\"\\n)\\n```\\n```shell\\n> Entering new  chain...\\nAction: sql_db_list_tables\\nAction Input: \"\"\\nObservation: air\\nThought:The \"air\" table seems relevant to the question. I should query the schema of the \"air\" table to see what columns are available.\\nAction: sql_db_schema\\nAction Input: \"air\"\\nObservation:\\nCREATE TABLE air (\\n\\tpressure FLOAT,\\n\\tstation STRING,\\n\\ttemperature FLOAT,\\n\\ttime TIMESTAMP,\\n\\tvisibility FLOAT\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_path': 'docs/docs/integrations/providers/cnosdb.mdx', 'file_name': 'cnosdb.mdx', 'file_type': '.mdx'}, page_content='/*\\n3 rows from air table:\\npressure\\tstation\\ttemperature\\ttime\\tvisibility\\n75.0\\tXiaoMaiDao\\t67.0\\t2022-10-19T03:40:00\\t54.0\\n77.0\\tXiaoMaiDao\\t69.0\\t2022-10-19T04:40:00\\t56.0\\n76.0\\tXiaoMaiDao\\t68.0\\t2022-10-19T05:40:00\\t55.0\\n*/\\nThought:The \"temperature\" column in the \"air\" table is relevant to the question. I can query the average temperature between the specified dates.\\nAction: sql_db_query\\nAction Input: \"SELECT AVG(temperature) FROM air WHERE station = \\'XiaoMaiDao\\' AND time >= \\'2022-10-19\\' AND time &lt;= \\'2022-10-20\\'\"\\nObservation: [(68.0,)]\\nThought:The average temperature of air at station XiaoMaiDao between October 19, 2022 and October 20, 2022 is 68.0.\\nFinal Answer: 68.0\\n\\n> Finished chain.\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cognee.mdx', 'file_path': 'docs/docs/integrations/providers/cognee.mdx', 'file_name': 'cognee.mdx', 'file_type': '.mdx'}, page_content='# Cognee\\n\\nCognee implements scalable, modular ECL (Extract, Cognify, Load) pipelines that allow\\nyou to interconnect and retrieve past conversations, documents, and audio\\ntranscriptions while reducing hallucinations, developer effort, and cost.\\n\\nCognee merges graph and vector databases to uncover hidden relationships and new\\npatterns in your data. You can automatically model, load and retrieve entities and\\nobjects representing your business domain and analyze their relationships, uncovering\\ninsights that neither vector stores nor graph stores alone can provide.\\n\\nTry it in a Google Colab  <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">notebook</a>  or have a look at the <a href=\"https://docs.cognee.ai\">documentation</a>.\\n\\nIf you have questions, join cognee <a href=\"https://discord.gg/NQPKmU5CCg\">Discord</a> community.\\n\\nHave you seen cognee\\'s <a href=\"https://github.com/topoteretes/cognee-starter\">starter repo</a>? Check it out!'), Document(metadata={'source': 'docs/docs/integrations/providers/cognee.mdx', 'file_path': 'docs/docs/integrations/providers/cognee.mdx', 'file_name': 'cognee.mdx', 'file_type': '.mdx'}, page_content='## Installation and Setup\\n\\n```bash\\npip install langchain-cognee\\n```\\n\\n## Retrievers\\n\\nSee detail on available retrievers [here](/docs/integrations/retrievers/cognee).'), Document(metadata={'source': 'docs/docs/integrations/providers/cogniswitch.mdx', 'file_path': 'docs/docs/integrations/providers/cogniswitch.mdx', 'file_name': 'cogniswitch.mdx', 'file_type': '.mdx'}, page_content='# CogniSwitch\\n\\n>[CogniSwitch](https://www.cogniswitch.ai/aboutus) is an API based data platform that \\n> enhances enterprise data by extracting entities, concepts and their relationships \\n> thereby converting this data into a multidimensional format and storing it in \\n> a database that can accommodate these enhancements. In our case the data is stored \\n> in a knowledge graph. This enhanced data is now ready for consumption by LLMs and \\n> other GenAI applications ensuring the data is consumable and context can be maintained. \\n> Thereby eliminating hallucinations and delivering accuracy.\\n\\n## Toolkit\\n\\nSee [installation instructions and usage example](/docs/integrations/tools/cogniswitch).\\n\\n```python\\nfrom langchain_community.agent_toolkits import CogniswitchToolkit\\n```\\n\\n## Tools\\n\\n### CogniswitchKnowledgeRequest\\n\\n>Tool that uses the CogniSwitch service to answer questions.\\n\\n```python\\nfrom langchain_community.tools.cogniswitch.tool import CogniswitchKnowledgeRequest\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cogniswitch.mdx', 'file_path': 'docs/docs/integrations/providers/cogniswitch.mdx', 'file_name': 'cogniswitch.mdx', 'file_type': '.mdx'}, page_content='### CogniswitchKnowledgeSourceFile\\n\\n>Tool that uses the CogniSwitch services to store data from file.\\n\\n```python\\nfrom langchain_community.tools.cogniswitch.tool import CogniswitchKnowledgeSourceFile\\n```\\n\\n### CogniswitchKnowledgeSourceURL\\n\\n>Tool that uses the CogniSwitch services to store data from a URL.\\n\\n```python\\nfrom langchain_community.tools.cogniswitch.tool import CogniswitchKnowledgeSourceURL\\n```\\n\\n### CogniswitchKnowledgeStatus\\n\\n>Tool that uses the CogniSwitch services to get the status of the document or url uploaded.\\n\\n```python\\nfrom langchain_community.tools.cogniswitch.tool import CogniswitchKnowledgeStatus\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='# Cohere\\n\\n>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models\\n> that help companies improve human-machine interactions.\\n\\n## Installation and Setup\\n- Install the Python SDK :\\n```bash\\npip install langchain-cohere\\n```\\n\\nGet a [Cohere api key](https://dashboard.cohere.ai/) and set it as an environment variable (`COHERE_API_KEY`)\\n\\n## Cohere langchain integrations'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='|API|description|Endpoint docs|Import|Example usage|\\n|---|---|---|---|---|\\n|Chat|Build chat bots|[chat](https://docs.cohere.com/reference/chat)|`from langchain_cohere import ChatCohere`|[cohere.ipynb](/docs/integrations/chat/cohere)|\\n|LLM|Generate text|[generate](https://docs.cohere.com/reference/generate)|`from langchain_cohere.llms import Cohere`|[cohere.ipynb](/docs/integrations/llms/cohere)|\\n|RAG Retriever|Connect to external data sources|[chat + rag](https://docs.cohere.com/reference/chat)|`from langchain.retrievers import CohereRagRetriever`|[cohere.ipynb](/docs/integrations/retrievers/cohere)|\\n|Text Embedding|Embed strings to vectors|[embed](https://docs.cohere.com/reference/embed)|`from langchain_cohere import CohereEmbeddings`|[cohere.ipynb](/docs/integrations/text_embedding/cohere)|\\n|Rerank Retriever|Rank strings based on relevance|[rerank](https://docs.cohere.com/reference/rerank)|`from langchain.retrievers.document_compressors import CohereRerank`|[cohere.ipynb](/docs/integrations/retrievers/cohere-reranker)|'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='## Quick copy examples\\n\\n### Chat\\n\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import HumanMessage\\nchat = ChatCohere()\\nmessages = [HumanMessage(content=\"knock knock\")]\\nprint(chat.invoke(messages))\\n```\\n\\nUsage of the Cohere [chat model](/docs/integrations/chat/cohere)\\n\\n### LLM\\n\\n\\n```python\\nfrom langchain_cohere.llms import Cohere\\n\\nllm = Cohere()\\nprint(llm.invoke(\"Come up with a pet name\"))\\n```\\n\\nUsage of the Cohere (legacy) [LLM model](/docs/integrations/llms/cohere) \\n\\n### Tool calling\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.messages import (\\n    HumanMessage,\\n    ToolMessage,\\n)\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef magic_function(number: int) -> int:\\n    \"\"\"Applies a magic operation to an integer\\n\\n    Args:\\n        number: Number to have magic operation performed on\\n    \"\"\"\\n    return number + 10'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='def invoke_tools(tool_calls, messages):\\n    for tool_call in tool_calls:\\n        selected_tool = {\"magic_function\":magic_function}[\\n            tool_call[\"name\"].lower()\\n        ]\\n        tool_output = selected_tool.invoke(tool_call[\"args\"])\\n        messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\\n    return messages\\n\\ntools = [magic_function]\\n\\nllm = ChatCohere()\\nllm_with_tools = llm.bind_tools(tools=tools)\\nmessages = [\\n    HumanMessage(\\n        content=\"What is the value of magic_function(2)?\"\\n    )\\n]\\n\\nres = llm_with_tools.invoke(messages)\\nwhile res.tool_calls:\\n    messages.append(res)\\n    messages = invoke_tools(res.tool_calls, messages)\\n    res = llm_with_tools.invoke(messages)\\n\\nprint(res.content)\\n```\\nTool calling with Cohere LLM can be done by binding the necessary tools to the llm as seen above. \\nAn alternative, is to support multi hop tool calling with the ReAct agent as seen below.\\n\\n### ReAct Agent'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='The agent is based on the paper\\n[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629).\\n\\n```python\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nfrom langchain_cohere import ChatCohere, create_cohere_react_agent\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.agents import AgentExecutor\\n\\nllm = ChatCohere()\\n\\ninternet_search = TavilySearchResults(max_results=4)\\ninternet_search.name = \"internet_search\"\\ninternet_search.description = \"Route a user query to the internet\"\\n\\nprompt = ChatPromptTemplate.from_template(\"{input}\")\\n\\nagent = create_cohere_react_agent(\\n    llm,\\n    [internet_search],\\n    prompt\\n)\\n\\nagent_executor = AgentExecutor(agent=agent, tools=[internet_search], verbose=True)\\n\\nagent_executor.invoke({\\n    \"input\": \"In what year was the company that was founded as Sound of Music added to the S&P 500?\",\\n})\\n```\\nThe ReAct agent can be used to call multiple tools in sequence.\\n\\n### RAG Retriever'), Document(metadata={'source': 'docs/docs/integrations/providers/cohere.mdx', 'file_path': 'docs/docs/integrations/providers/cohere.mdx', 'file_name': 'cohere.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain.retrievers import CohereRagRetriever\\nfrom langchain_core.documents import Document\\n\\nrag = CohereRagRetriever(llm=ChatCohere())\\nprint(rag.invoke(\"What is cohere ai?\"))\\n```\\n\\nUsage of the Cohere [RAG Retriever](/docs/integrations/retrievers/cohere)\\n\\n### Text Embedding\\n\\n```python\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\\nprint(embeddings.embed_documents([\"This is a test document.\"]))\\n```\\n\\nUsage of the Cohere [Text Embeddings model](/docs/integrations/text_embedding/cohere)\\n\\n### Reranker\\n\\nUsage of the Cohere [Reranker](/docs/integrations/retrievers/cohere-reranker)'), Document(metadata={'source': 'docs/docs/integrations/providers/college_confidential.mdx', 'file_path': 'docs/docs/integrations/providers/college_confidential.mdx', 'file_name': 'college_confidential.mdx', 'file_type': '.mdx'}, page_content=\"# College Confidential\\n\\n>[College Confidential](https://www.collegeconfidential.com/) gives information on 3,800+ colleges and universities.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/college_confidential).\\n\\n```python\\nfrom langchain_community.document_loaders import CollegeConfidentialLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/confident.mdx', 'file_path': 'docs/docs/integrations/providers/confident.mdx', 'file_name': 'confident.mdx', 'file_type': '.mdx'}, page_content='# Confident AI\\n\\n>[Confident AI](https://confident-ai.com) is a creator of the `DeepEval`.\\n>\\n>[DeepEval](https://github.com/confident-ai/deepeval) is a package for unit testing LLMs.\\n> Using `DeepEval`, everyone can build robust language models through faster iterations\\n> using both unit testing and integration testing. `DeepEval provides support for each step in the iteration\\n> from synthetic data creation to testing.\\n\\n## Installation and Setup\\n\\nYou need to get the [DeepEval API credentials](https://app.confident-ai.com).\\n\\nYou need to install the `DeepEval` Python package:\\n\\n```bash\\npip install deepeval\\n```\\n\\n## Callbacks\\n\\nSee an [example](/docs/integrations/callbacks/confident).\\n\\n```python\\nfrom langchain.callbacks.confident_callback import DeepEvalCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/confluence.mdx', 'file_path': 'docs/docs/integrations/providers/confluence.mdx', 'file_name': 'confluence.mdx', 'file_type': '.mdx'}, page_content='# Confluence\\n\\n>[Confluence](https://www.atlassian.com/software/confluence) is a wiki collaboration platform that saves and organizes all of the project-related material. `Confluence` is a knowledge base that primarily handles content management activities. \\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install atlassian-python-api\\n```\\n\\nWe need to set up `username/api_key` or `Oauth2 login`. \\nSee [instructions](https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/confluence).\\n\\n```python\\nfrom langchain_community.document_loaders import ConfluenceLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/connery.mdx', 'file_path': 'docs/docs/integrations/providers/connery.mdx', 'file_name': 'connery.mdx', 'file_type': '.mdx'}, page_content='# Connery\\n\\n>[Connery SDK](https://github.com/connery-io/connery-sdk) is an NPM package that \\n> includes both an SDK and a CLI, designed for the development of plugins and actions.\\n>\\n>The CLI automates many things in the development process. The SDK \\n> offers a JavaScript API for defining plugins and actions and packaging them \\n> into a plugin server with a standardized REST API generated from the metadata. \\n> The plugin server handles authorization, input validation, and logging. \\n> So you can focus on the logic of your actions.\\n> \\n> See the use cases and examples in the [Connery SDK documentation](https://sdk.connery.io/docs/use-cases/)\\n\\n## Toolkit\\n\\nSee [usage example](/docs/integrations/tools/connery).\\n\\n```python\\nfrom langchain_community.agent_toolkits.connery import ConneryToolkit\\n```\\n\\n## Tools\\n\\n### ConneryAction\\n\\n```python\\nfrom langchain_community.tools.connery import ConneryService\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/context.mdx', 'file_path': 'docs/docs/integrations/providers/context.mdx', 'file_name': 'context.mdx', 'file_type': '.mdx'}, page_content='# Context\\n\\n>[Context](https://context.ai/) provides user analytics for LLM-powered products and features.\\n\\n## Installation and Setup\\n\\nWe need to install the  `context-python` Python package:\\n\\n```bash\\npip install context-python\\n```\\n\\n\\n## Callbacks\\n\\nSee a [usage example](/docs/integrations/callbacks/context).\\n\\n```python\\nfrom langchain.callbacks import ContextCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/couchbase.mdx', 'file_path': 'docs/docs/integrations/providers/couchbase.mdx', 'file_name': 'couchbase.mdx', 'file_type': '.mdx'}, page_content='# Couchbase\\n\\n>[Couchbase](http://couchbase.com/) is an award-winning distributed NoSQL cloud database \\n> that delivers unmatched versatility, performance, scalability, and financial value \\n> for all of your cloud, mobile, AI, and edge computing applications.\\n\\n## Installation and Setup\\n\\nWe have to install the `langchain-couchbase` package.\\n\\n```bash\\npip install langchain-couchbase\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/couchbase).\\n\\n```python\\nfrom langchain_couchbase import CouchbaseVectorStore\\n```\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/couchbase).\\n\\n```python\\nfrom langchain_community.document_loaders.couchbase import CouchbaseLoader\\n```\\n\\n## LLM Caches\\n\\n### CouchbaseCache\\nUse Couchbase as a cache for prompts and responses.\\n\\nSee a [usage example](/docs/integrations/llm_caching/#couchbase-caches).\\n\\nTo import this cache:\\n```python\\nfrom langchain_couchbase.cache import CouchbaseCache\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/couchbase.mdx', 'file_path': 'docs/docs/integrations/providers/couchbase.mdx', 'file_name': 'couchbase.mdx', 'file_type': '.mdx'}, page_content='To use this cache with your LLMs:\\n```python\\nfrom langchain_core.globals import set_llm_cache\\n\\ncluster = couchbase_cluster_connection_object\\n\\nset_llm_cache(\\n    CouchbaseCache(\\n        cluster=cluster,\\n        bucket_name=BUCKET_NAME,\\n        scope_name=SCOPE_NAME,\\n        collection_name=COLLECTION_NAME,\\n    )\\n)\\n```\\n\\n\\n### CouchbaseSemanticCache\\nSemantic caching allows users to retrieve cached prompts based on the semantic similarity between the user input and previously cached inputs. Under the hood it uses Couchbase as both a cache and a vectorstore.\\nThe CouchbaseSemanticCache needs a Search Index defined to work. Please look at the [usage example](/docs/integrations/vectorstores/couchbase) on how to set up the index.\\n\\nSee a [usage example](/docs/integrations/llm_caching/#couchbase-caches).\\n\\nTo import this cache:\\n```python\\nfrom langchain_couchbase.cache import CouchbaseSemanticCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain_core.globals import set_llm_cache'), Document(metadata={'source': 'docs/docs/integrations/providers/couchbase.mdx', 'file_path': 'docs/docs/integrations/providers/couchbase.mdx', 'file_name': 'couchbase.mdx', 'file_type': '.mdx'}, page_content='# use any embedding provider...\\nfrom langchain_openai.Embeddings import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings()\\ncluster = couchbase_cluster_connection_object\\n\\nset_llm_cache(\\n    CouchbaseSemanticCache(\\n        cluster=cluster,\\n        embedding = embeddings,\\n        bucket_name=BUCKET_NAME,\\n        scope_name=SCOPE_NAME,\\n        collection_name=COLLECTION_NAME,\\n        index_name=INDEX_NAME,\\n    )\\n)\\n```\\n\\n## Chat Message History\\nUse Couchbase as the storage for your chat messages.\\n\\nSee a [usage example](/docs/integrations/memory/couchbase_chat_message_history).\\n\\nTo use the chat message history in your applications:\\n```python\\nfrom langchain_couchbase.chat_message_histories import CouchbaseChatMessageHistory\\n\\nmessage_history = CouchbaseChatMessageHistory(\\n    cluster=cluster,\\n    bucket_name=BUCKET_NAME,\\n    scope_name=SCOPE_NAME,\\n    collection_name=COLLECTION_NAME,\\n    session_id=\"test-session\",\\n)\\n\\nmessage_history.add_user_message(\"hi!\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/coze.mdx', 'file_path': 'docs/docs/integrations/providers/coze.mdx', 'file_name': 'coze.mdx', 'file_type': '.mdx'}, page_content='# Coze\\n\\n[Coze](https://www.coze.com/) is an AI chatbot development platform that enables\\nthe creation and deployment of chatbots for handling diverse conversations across\\nvarious applications.\\n\\n\\n## Installation and Setup\\n\\nFirst, you need to get the `API_KEY` from the [Coze](https://www.coze.com/) website.\\n\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/coze/).\\n\\n```python\\nfrom langchain_community.chat_models import ChatCoze\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='# CrateDB\\n\\n> [CrateDB] is a distributed and scalable SQL database for storing and\\n> analyzing massive amounts of data in near real-time, even with complex\\n> queries. It is PostgreSQL-compatible, based on Lucene, and inheriting\\n> from Elasticsearch.\\n\\n\\n## Installation and Setup\\n\\n### Setup CrateDB\\nThere are two ways to get started with CrateDB quickly. Alternatively,\\nchoose other [CrateDB installation options].\\n\\n#### Start CrateDB on your local machine\\nExample: Run a single-node CrateDB instance with security disabled,\\nusing Docker or Podman. This is not recommended for production use.\\n\\n```bash\\ndocker run --name=cratedb --rm \\\\\\n  --publish=4200:4200 --publish=5432:5432 --env=CRATE_HEAP_SIZE=2g \\\\\\n  crate:latest -Cdiscovery.type=single-node\\n```\\n\\n#### Deploy cluster on CrateDB Cloud\\n[CrateDB Cloud] is a managed CrateDB service. Sign up for a\\n[free trial][CrateDB Cloud Console].'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='### Install Client\\nInstall the most recent version of the [langchain-cratedb] package\\nand a few others that are needed for this tutorial.\\n```bash\\npip install --upgrade langchain-cratedb langchain-openai unstructured\\n```\\n\\n\\n## Documentation\\nFor a more detailed walkthrough of the CrateDB wrapper, see\\n[using LangChain with CrateDB]. See also [all features of CrateDB]\\nto learn about other functionality provided by CrateDB.\\n\\n\\n## Features\\nThe CrateDB adapter for LangChain provides APIs to use CrateDB as vector store,\\ndocument loader, and storage for chat messages.\\n\\n### Vector Store\\nUse the CrateDB vector store functionality around `FLOAT_VECTOR` and `KNN_MATCH`\\nfor similarity search and other purposes. See also [CrateDBVectorStore Tutorial].'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='Make sure you\\'ve configured a valid OpenAI API key.\\n```bash\\nexport OPENAI_API_KEY=sk-XJZ...\\n```\\n```python\\nfrom langchain_community.document_loaders import UnstructuredURLLoader\\nfrom langchain_cratedb import CrateDBVectorStore\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nloader = UnstructuredURLLoader(urls=[\"https://github.com/langchain-ai/langchain/raw/refs/tags/langchain-core==0.3.28/docs/docs/how_to/state_of_the_union.txt\"])\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocs = text_splitter.split_documents(documents)\\n\\nembeddings = OpenAIEmbeddings()\\n\\n# Connect to a self-managed CrateDB instance on localhost.\\nCONNECTION_STRING = \"crate://?schema=testdrive\"\\n\\nstore = CrateDBVectorStore.from_documents(\\n    documents=docs,\\n    embedding=embeddings,\\n    collection_name=\"state_of_the_union\",\\n    connection=CONNECTION_STRING,\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='query = \"What did the president say about Ketanji Brown Jackson\"\\ndocs_with_score = store.similarity_search_with_score(query)\\n```\\n\\n### Document Loader\\nLoad load documents from a CrateDB database table, using the document loader\\n`CrateDBLoader`, which is based on SQLAlchemy. See also [CrateDBLoader Tutorial].\\n\\nTo use the document loader in your applications:\\n```python\\nimport sqlalchemy as sa\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_cratedb import CrateDBLoader\\n\\n# Connect to a self-managed CrateDB instance on localhost.\\nCONNECTION_STRING = \"crate://?schema=testdrive\"\\n\\ndb = SQLDatabase(engine=sa.create_engine(CONNECTION_STRING))\\n\\nloader = CrateDBLoader(\\n    \\'SELECT * FROM sys.summits LIMIT 42\\',\\n    db=db,\\n)\\ndocuments = loader.load()\\n```\\n\\n### Chat Message History\\nUse CrateDB as the storage for your chat messages.\\nSee also [CrateDBChatMessageHistory Tutorial].'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='To use the chat message history in your applications:\\n```python\\nfrom langchain_cratedb import CrateDBChatMessageHistory\\n\\n# Connect to a self-managed CrateDB instance on localhost.\\nCONNECTION_STRING = \"crate://?schema=testdrive\"\\n\\nmessage_history = CrateDBChatMessageHistory(\\n    session_id=\"test-session\",\\n    connection=CONNECTION_STRING,\\n)\\n\\nmessage_history.add_user_message(\"hi!\")\\n```\\n\\n### Full Cache\\nThe standard / full cache avoids invoking the LLM when the supplied\\nprompt is exactly the same as one encountered already.\\nSee also [CrateDBCache Example].\\n\\nTo use the full cache in your applications:\\n```python\\nimport sqlalchemy as sa\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_cratedb import CrateDBCache\\n\\n# Configure cache.\\nengine = sa.create_engine(\"crate://crate@localhost:4200/?schema=testdrive\")\\nset_llm_cache(CrateDBCache(engine))'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='# Invoke LLM conversation.\\nllm = ChatOpenAI(\\n    model_name=\"chatgpt-4o-latest\",\\n    temperature=0.7,\\n)\\nprint()\\nprint(\"Asking with full cache:\")\\nanswer = llm.invoke(\"What is the answer to everything?\")\\nprint(answer.content)\\n```\\n\\n### Semantic Cache\\n\\nThe semantic cache allows users to retrieve cached prompts based on semantic\\nsimilarity between the user input and previously cached inputs. It also avoids\\ninvoking the LLM when not needed.\\nSee also [CrateDBSemanticCache Example].\\n\\nTo use the semantic cache in your applications:\\n```python\\nimport sqlalchemy as sa\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\nfrom langchain_cratedb import CrateDBSemanticCache\\n\\n# Configure embeddings.\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='# Configure cache.\\nengine = sa.create_engine(\"crate://crate@localhost:4200/?schema=testdrive\")\\nset_llm_cache(\\n    CrateDBSemanticCache(\\n        embedding=embeddings,\\n        connection=engine,\\n        search_threshold=1.0,\\n    )\\n)\\n\\n# Invoke LLM conversation.\\nllm = ChatOpenAI(model_name=\"chatgpt-4o-latest\")\\nprint()\\nprint(\"Asking with semantic cache:\")\\nanswer = llm.invoke(\"What is the answer to everything?\")\\nprint(answer.content)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cratedb.mdx', 'file_path': 'docs/docs/integrations/providers/cratedb.mdx', 'file_name': 'cratedb.mdx', 'file_type': '.mdx'}, page_content='[all features of CrateDB]: https://cratedb.com/docs/guide/feature/\\n[CrateDB]: https://cratedb.com/database\\n[CrateDB Cloud]: https://cratedb.com/database/cloud\\n[CrateDB Cloud Console]: https://console.cratedb.cloud/?utm_source=langchain&utm_content=documentation\\n[CrateDB installation options]: https://cratedb.com/docs/guide/install/\\n[CrateDBCache Example]: https://github.com/crate/langchain-cratedb/blob/main/examples/basic/cache.py\\n[CrateDBSemanticCache Example]: https://github.com/crate/langchain-cratedb/blob/main/examples/basic/cache.py\\n[CrateDBChatMessageHistory Tutorial]: https://github.com/crate/cratedb-examples/blob/main/topic/machine-learning/llm-langchain/conversational_memory.ipynb\\n[CrateDBLoader Tutorial]: https://github.com/crate/cratedb-examples/blob/main/topic/machine-learning/llm-langchain/document_loader.ipynb\\n[CrateDBVectorStore Tutorial]: https://github.com/crate/cratedb-examples/blob/main/topic/machine-learning/llm-langchain/vector_search.ipynb\\n[langchain-cratedb]: https://pypi.org/project/langchain-cratedb/\\n[using LangChain with CrateDB]: https://cratedb.com/docs/guide/integrate/langchain/'), Document(metadata={'source': 'docs/docs/integrations/providers/ctransformers.mdx', 'file_path': 'docs/docs/integrations/providers/ctransformers.mdx', 'file_name': 'ctransformers.mdx', 'file_type': '.mdx'}, page_content=\"# C Transformers\\n\\nThis page covers how to use the [C Transformers](https://github.com/marella/ctransformers) library within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific C Transformers wrappers.\\n\\n## Installation and Setup\\n\\n- Install the Python package with `pip install ctransformers`\\n- Download a supported [GGML model](https://huggingface.co/TheBloke) (see [Supported Models](https://github.com/marella/ctransformers#supported-models))\\n\\n## Wrappers\\n\\n### LLM\\n\\nThere exists a CTransformers LLM wrapper, which you can access with:\\n\\n```python\\nfrom langchain_community.llms import CTransformers\\n```\\n\\nIt provides a unified interface for all models:\\n\\n```python\\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')\\n\\nprint(llm.invoke('AI is going to'))\\n```\\n\\nIf you are getting `illegal instruction` error, try using `lib='avx'` or `lib='basic'`:\"), Document(metadata={'source': 'docs/docs/integrations/providers/ctransformers.mdx', 'file_path': 'docs/docs/integrations/providers/ctransformers.mdx', 'file_name': 'ctransformers.mdx', 'file_type': '.mdx'}, page_content=\"```py\\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')\\n```\\n\\nIt can be used with models hosted on the Hugging Face Hub:\\n\\n```py\\nllm = CTransformers(model='marella/gpt-2-ggml')\\n```\\n\\nIf a model repo has multiple model files (`.bin` files), specify a model file using:\\n\\n```py\\nllm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')\\n```\\n\\nAdditional parameters can be passed using the `config` parameter:\\n\\n```py\\nconfig = {'max_new_tokens': 256, 'repetition_penalty': 1.1}\\n\\nllm = CTransformers(model='marella/gpt-2-ggml', config=config)\\n```\\n\\nSee [Documentation](https://github.com/marella/ctransformers#config) for a list of available parameters.\\n\\nFor a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/ctransformers).\"), Document(metadata={'source': 'docs/docs/integrations/providers/ctranslate2.mdx', 'file_path': 'docs/docs/integrations/providers/ctranslate2.mdx', 'file_name': 'ctranslate2.mdx', 'file_type': '.mdx'}, page_content='# CTranslate2\\n\\n>[CTranslate2](https://opennmt.net/CTranslate2/quickstart.html) is a C++ and Python library \\n> for efficient inference with Transformer models.\\n>\\n>The project implements a custom runtime that applies many performance optimization\\n> techniques such as weights quantization, layers fusion, batch reordering, etc., \\n> to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\\n>\\n>A full list of features and supported models is included in the \\n> [project’s repository](https://opennmt.net/CTranslate2/guides/transformers.html). \\n> To start, please check out the official [quickstart guide](https://opennmt.net/CTranslate2/quickstart.html).\\n\\n\\n## Installation and Setup\\n\\nInstall the Python package:\\n\\n```bash\\npip install ctranslate2\\n```\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/ctranslate2).\\n\\n```python\\nfrom langchain_community.llms import CTranslate2\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/cube.mdx', 'file_path': 'docs/docs/integrations/providers/cube.mdx', 'file_name': 'cube.mdx', 'file_type': '.mdx'}, page_content='# Cube\\n\\n>[Cube](https://cube.dev/) is the Semantic Layer for building data apps. It helps \\n> data engineers and application developers access data from modern data stores, \\n> organize it into consistent definitions, and deliver it to every application.\\n\\n## Installation and Setup\\n\\nWe have to get the API key and the URL of the Cube instance. See \\n[these instructions](https://cube.dev/docs/product/apis-integrations/rest-api#configuration-base-path).\\n\\n\\n## Document loader\\n\\n### Cube Semantic Layer\\n\\nSee a [usage example](/docs/integrations/document_loaders/cube_semantic).\\n\\n```python\\nfrom langchain_community.document_loaders import CubeSemanticLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dappier.mdx', 'file_path': 'docs/docs/integrations/providers/dappier.mdx', 'file_name': 'dappier.mdx', 'file_type': '.mdx'}, page_content=\"# Dappier\\n\\n[Dappier](https://dappier.com) connects any LLM or your Agentic AI to\\nreal-time, rights-cleared, proprietary data from trusted sources,\\nmaking your AI an expert in anything. Our specialized models include\\nReal-Time Web Search, News, Sports, Financial Stock Market Data,\\nCrypto Data, and exclusive content from premium publishers. Explore a\\nwide range of data models in our marketplace at\\n[marketplace.dappier.com](https://marketplace.dappier.com).\\n\\n[Dappier](https://dappier.com) delivers enriched, prompt-ready, and\\ncontextually relevant data strings, optimized for seamless integration\\nwith LangChain. Whether you're building conversational AI, recommendation\\nengines, or intelligent search, Dappier's LLM-agnostic RAG models ensure\\nyour AI has access to verified, up-to-date data—without the complexity of\\nbuilding and managing your own retrieval pipeline.\\n\\n## Installation and Setup\\n\\nInstall ``langchain-dappier`` and set environment variable\\n``DAPPIER_API_KEY``.\"), Document(metadata={'source': 'docs/docs/integrations/providers/dappier.mdx', 'file_path': 'docs/docs/integrations/providers/dappier.mdx', 'file_name': 'dappier.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install -U langchain-dappier\\nexport DAPPIER_API_KEY=\"your-api-key\"\\n```\\n\\nWe also need to set our Dappier API credentials, which can be generated at\\nthe [Dappier site.](https://platform.dappier.com/profile/api-keys).\\n\\nWe can find the supported data models by heading over to the \\n[Dappier marketplace.](https://platform.dappier.com/marketplace)\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/dappier).\\n\\n```python\\nfrom langchain_community.chat_models import ChatDappierAI\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/dappier).\\n\\n```python\\nfrom langchain_dappier import DappierRetriever\\n```\\n\\n## Tool\\n\\nSee a [usage example](/docs/integrations/tools/dappier).\\n\\n```python\\nfrom langchain_dappier import (\\n    DappierRealTimeSearchTool,\\n    DappierAIRecommendationTool\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dashvector.mdx', 'file_path': 'docs/docs/integrations/providers/dashvector.mdx', 'file_name': 'dashvector.mdx', 'file_type': '.mdx'}, page_content='# DashVector\\n\\n> [DashVector](https://help.aliyun.com/document_detail/2510225.html) is a fully-managed vectorDB service that supports high-dimension dense and sparse vectors, real-time insertion and filtered search. It is built to scale automatically and can adapt to different application requirements.  \\n\\nThis document demonstrates to leverage DashVector within the LangChain ecosystem. In particular, it shows how to install DashVector, and how to use it as a VectorStore plugin in LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific DashVector wrappers.\\n\\n## Installation and Setup\\n\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install dashvector\\n```\\n\\nYou must have an API key. Here are the [installation instructions](https://help.aliyun.com/document_detail/2510223.html).\\n\\n\\n## Embedding models\\n\\n```python\\nfrom langchain_community.embeddings import DashScopeEmbeddings\\n```\\n\\nSee the [use example](/docs/integrations/vectorstores/dashvector).\\n\\n\\n## Vector Store'), Document(metadata={'source': 'docs/docs/integrations/providers/dashvector.mdx', 'file_path': 'docs/docs/integrations/providers/dashvector.mdx', 'file_name': 'dashvector.mdx', 'file_type': '.mdx'}, page_content='A DashVector Collection is wrapped as a familiar VectorStore for native usage within LangChain, \\nwhich allows it to be readily used for various scenarios, such as semantic search or example selection.\\n\\nYou may import the vectorstore by:\\n```python\\nfrom langchain_community.vectorstores import DashVector\\n```\\n\\nFor a detailed walkthrough of the DashVector wrapper, please refer to [this notebook](/docs/integrations/vectorstores/dashvector)'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog.mdx', 'file_path': 'docs/docs/integrations/providers/datadog.mdx', 'file_name': 'datadog.mdx', 'file_type': '.mdx'}, page_content='# Datadog Tracing\\n\\n>[ddtrace](https://github.com/DataDog/dd-trace-py) is a Datadog application performance monitoring (APM) library which provides an integration to monitor your LangChain application.\\n\\nKey features of the ddtrace integration for LangChain:\\n- Traces: Capture LangChain requests, parameters, prompt-completions, and help visualize LangChain operations.\\n- Metrics: Capture LangChain request latency, errors, and token/cost usage (for OpenAI LLMs and chat models).\\n- Logs: Store prompt completion data for each LangChain operation.\\n- Dashboard: Combine metrics, logs, and trace data into a single plane to monitor LangChain requests.\\n- Monitors: Provide alerts in response to spikes in LangChain request latency or error rate.\\n\\nNote: The ddtrace LangChain integration currently provides tracing for LLMs, chat models, Text Embedding Models, Chains, and Vectorstores.\\n\\n## Installation and Setup'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog.mdx', 'file_path': 'docs/docs/integrations/providers/datadog.mdx', 'file_name': 'datadog.mdx', 'file_type': '.mdx'}, page_content='1. Enable APM and StatsD in your Datadog Agent, along with a Datadog API key. For example, in Docker:\\n\\n```\\ndocker run -d --cgroupns host \\\\\\n              --pid host \\\\\\n              -v /var/run/docker.sock:/var/run/docker.sock:ro \\\\\\n              -v /proc/:/host/proc/:ro \\\\\\n              -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \\\\\\n              -e DD_API_KEY=<DATADOG_API_KEY> \\\\\\n              -p 127.0.0.1:8126:8126/tcp \\\\\\n              -p 127.0.0.1:8125:8125/udp \\\\\\n              -e DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true \\\\\\n              -e DD_APM_ENABLED=true \\\\\\n              gcr.io/datadoghq/agent:latest\\n```\\n\\n2. Install the Datadog APM Python library.\\n\\n```\\npip install ddtrace>=1.17\\n```\\n\\n\\n3. The LangChain integration can be enabled automatically when you prefix your LangChain Python application command with `ddtrace-run`:\\n\\n```\\nDD_SERVICE=\"my-service\" DD_ENV=\"staging\" DD_API_KEY=<DATADOG_API_KEY> ddtrace-run python <your-app>.py\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog.mdx', 'file_path': 'docs/docs/integrations/providers/datadog.mdx', 'file_name': 'datadog.mdx', 'file_type': '.mdx'}, page_content='**Note**: If the Agent is using a non-default hostname or port, be sure to also set `DD_AGENT_HOST`, `DD_TRACE_AGENT_PORT`, or `DD_DOGSTATSD_PORT`.\\n\\nAdditionally, the LangChain integration can be enabled programmatically by adding `patch_all()` or `patch(langchain=True)` before the first import of `langchain` in your application.\\n\\nNote that using `ddtrace-run` or `patch_all()` will also enable the `requests` and `aiohttp` integrations which trace HTTP requests to LLM providers, as well as the `openai` integration which traces requests to the OpenAI library.\\n\\n```python\\nfrom ddtrace import config, patch\\n\\n# Note: be sure to configure the integration before calling ``patch()``!\\n# e.g. config.langchain[\"logs_enabled\"] = True\\n\\npatch(langchain=True)\\n\\n# to trace synchronous HTTP requests\\n# patch(langchain=True, requests=True)\\n\\n# to trace asynchronous HTTP requests (to the OpenAI library)\\n# patch(langchain=True, aiohttp=True)'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog.mdx', 'file_path': 'docs/docs/integrations/providers/datadog.mdx', 'file_name': 'datadog.mdx', 'file_type': '.mdx'}, page_content='# to include underlying OpenAI spans from the OpenAI integration\\n# patch(langchain=True, openai=True)patch_all\\n```\\n\\nSee the [APM Python library documentation](https://ddtrace.readthedocs.io/en/stable/installation_quickstart.html) for more advanced usage.\\n\\n\\n## Configuration\\n\\nSee the [APM Python library documentation](https://ddtrace.readthedocs.io/en/stable/integrations.html#langchain) for all the available configuration options.\\n\\n\\n### Log Prompt & Completion Sampling\\n\\nTo enable log prompt and completion sampling, set the `DD_LANGCHAIN_LOGS_ENABLED=1` environment variable. By default, 10% of traced requests will emit logs containing the prompts and completions.\\n\\nTo adjust the log sample rate, see the [APM library documentation](https://ddtrace.readthedocs.io/en/stable/integrations.html#langchain).\\n\\n**Note**: Logs submission requires `DD_API_KEY` to be specified when running `ddtrace-run`.\\n\\n\\n## Troubleshooting'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog.mdx', 'file_path': 'docs/docs/integrations/providers/datadog.mdx', 'file_name': 'datadog.mdx', 'file_type': '.mdx'}, page_content='Need help? Create an issue on [ddtrace](https://github.com/DataDog/dd-trace-py) or contact [Datadog support](https://docs.datadoghq.com/help/).'), Document(metadata={'source': 'docs/docs/integrations/providers/datadog_logs.mdx', 'file_path': 'docs/docs/integrations/providers/datadog_logs.mdx', 'file_name': 'datadog_logs.mdx', 'file_type': '.mdx'}, page_content='# Datadog Logs\\n\\n>[Datadog](https://www.datadoghq.com/) is a monitoring and analytics platform for cloud-scale applications.\\n\\n## Installation and Setup\\n\\n```bash\\npip install datadog_api_client\\n```\\n\\nWe must initialize the loader with the Datadog API key and APP key, and we need to set up the query to extract the desired logs.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/datadog_logs).\\n\\n```python\\nfrom langchain_community.document_loaders import DatadogLogsLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dataforseo.mdx', 'file_path': 'docs/docs/integrations/providers/dataforseo.mdx', 'file_name': 'dataforseo.mdx', 'file_type': '.mdx'}, page_content='# DataForSEO\\n\\n>[DataForSeo](https://dataforseo.com/) provides comprehensive SEO and digital marketing data solutions via API.\\n\\nThis page provides instructions on how to use the DataForSEO search APIs within LangChain.\\n\\n## Installation and Setup\\n\\nGet a [DataForSEO API Access login and password](https://app.dataforseo.com/register), and set them as environment variables \\n(`DATAFORSEO_LOGIN` and `DATAFORSEO_PASSWORD` respectively).\\n\\n```python\\nimport os\\n\\nos.environ[\"DATAFORSEO_LOGIN\"] = \"your_login\"\\nos.environ[\"DATAFORSEO_PASSWORD\"] = \"your_password\"\\n```\\n\\n\\n## Utility\\n\\nThe `DataForSEO` utility wraps the API. To import this utility, use:\\n\\n```python\\nfrom langchain_community.utilities.dataforseo_api_search import DataForSeoAPIWrapper\\n```\\n\\nFor a detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/dataforseo).\\n\\n## Tool\\n\\nYou can also load this wrapper as a Tool to use with an Agent:'), Document(metadata={'source': 'docs/docs/integrations/providers/dataforseo.mdx', 'file_path': 'docs/docs/integrations/providers/dataforseo.mdx', 'file_name': 'dataforseo.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"dataforseo-api-search\"])\\n```\\n\\nThis will load the following tools:\\n\\n```python\\nfrom langchain_community.tools import DataForSeoAPISearchRun\\nfrom langchain_community.tools import DataForSeoAPISearchResults\\n```\\n\\n## Example usage\\n\\n```python\\ndataforseo = DataForSeoAPIWrapper(api_login=\"your_login\", api_password=\"your_password\")\\nresult = dataforseo.run(\"Bill Gates\")\\nprint(result)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dataherald.mdx', 'file_path': 'docs/docs/integrations/providers/dataherald.mdx', 'file_name': 'dataherald.mdx', 'file_type': '.mdx'}, page_content='# Dataherald\\n\\n>[Dataherald](https://www.dataherald.com) is a natural language-to-SQL.\\n\\nThis page covers how to use the `Dataherald API` within LangChain.\\n\\n## Installation and Setup\\n- Install requirements with \\n```bash\\npip install dataherald\\n```\\n- Go to dataherald and sign up [here](https://www.dataherald.com)\\n- Create an app and get your `API KEY`\\n- Set your `API KEY` as an environment variable `DATAHERALD_API_KEY`\\n\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a DataheraldAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities.dataherald import DataheraldAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/dataherald).\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/dataherald.mdx', 'file_path': 'docs/docs/integrations/providers/dataherald.mdx', 'file_name': 'dataherald.mdx', 'file_type': '.mdx'}, page_content='You can use the tool in an agent like this:\\n```python\\nfrom langchain_community.utilities.dataherald import DataheraldAPIWrapper\\nfrom langchain_community.tools.dataherald.tool import DataheraldTextToSQL\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain import hub\\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\\n\\napi_wrapper = DataheraldAPIWrapper(db_connection_id=\"<db_connection_id>\")\\ntool = DataheraldTextToSQL(api_wrapper=api_wrapper)\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nprompt = hub.pull(\"hwchase17/react\")\\nagent = create_react_agent(llm, tools, prompt)\\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\\nagent_executor.invoke({\"input\":\"Return the sql for this question: How many employees are in the company?\"})\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dataherald.mdx', 'file_path': 'docs/docs/integrations/providers/dataherald.mdx', 'file_name': 'dataherald.mdx', 'file_type': '.mdx'}, page_content='Output\\n```shell\\n> Entering new AgentExecutor chain...\\nI need to use a tool that can convert this question into SQL.\\nAction: dataherald\\nAction Input: How many employees are in the company?Answer: SELECT\\n    COUNT(*) FROM employeesI now know the final answer\\nFinal Answer: SELECT\\n    COUNT(*)\\nFROM\\n    employees\\n\\n> Finished chain.\\n{\\'input\\': \\'Return the sql for this question: How many employees are in the company?\\', \\'output\\': \"SELECT \\\\n    COUNT(*)\\\\nFROM \\\\n    employees\"}\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/dedoc.mdx', 'file_path': 'docs/docs/integrations/providers/dedoc.mdx', 'file_name': 'dedoc.mdx', 'file_type': '.mdx'}, page_content='# Dedoc\\n\\n>[Dedoc](https://dedoc.readthedocs.io) is an [open-source](https://github.com/ispras/dedoc)\\nlibrary/service that extracts texts, tables, attached files and document structure\\n(e.g., titles, list items, etc.) from files of various formats.\\n\\n`Dedoc` supports `DOCX`, `XLSX`, `PPTX`, `EML`, `HTML`, `PDF`, images and more.\\nFull list of supported formats can be found [here](https://dedoc.readthedocs.io/en/latest/#id1).\\n\\n## Installation and Setup\\n\\n### Dedoc library\\n\\nYou can install `Dedoc` using `pip`.\\nIn this case, you will need to install dependencies,\\nplease go [here](https://dedoc.readthedocs.io/en/latest/getting_started/installation.html)\\nto get more information.\\n\\n```bash\\npip install dedoc\\n```\\n\\n### Dedoc API'), Document(metadata={'source': 'docs/docs/integrations/providers/dedoc.mdx', 'file_path': 'docs/docs/integrations/providers/dedoc.mdx', 'file_name': 'dedoc.mdx', 'file_type': '.mdx'}, page_content=\"If you are going to use `Dedoc` API, you don't need to install `dedoc` library.\\nIn this case, you should run the `Dedoc` service, e.g. `Docker` container (please see\\n[the documentation](https://dedoc.readthedocs.io/en/latest/getting_started/installation.html#install-and-run-dedoc-using-docker)\\nfor more details):\\n\\n```bash\\ndocker pull dedocproject/dedoc\\ndocker run -p 1231:1231\\n```\\n\\n## Document Loader\\n\\n* For handling files of any formats (supported by `Dedoc`), you can use `DedocFileLoader`:\\n\\n    ```python\\n    from langchain_community.document_loaders import DedocFileLoader\\n    ```\\n\\n* For handling PDF files (with or without a textual layer), you can use `DedocPDFLoader`:\\n\\n    ```python\\n    from langchain_community.document_loaders import DedocPDFLoader\\n    ```\\n\\n* For handling files of any formats without library installation,\\nyou can use `Dedoc API` with `DedocAPIFileLoader`:\\n\\n    ```python\\n    from langchain_community.document_loaders import DedocAPIFileLoader\\n    ```\"), Document(metadata={'source': 'docs/docs/integrations/providers/dedoc.mdx', 'file_path': 'docs/docs/integrations/providers/dedoc.mdx', 'file_name': 'dedoc.mdx', 'file_type': '.mdx'}, page_content='Please see a [usage example](/docs/integrations/document_loaders/dedoc) for more details.'), Document(metadata={'source': 'docs/docs/integrations/providers/deepinfra.mdx', 'file_path': 'docs/docs/integrations/providers/deepinfra.mdx', 'file_name': 'deepinfra.mdx', 'file_type': '.mdx'}, page_content='# DeepInfra\\n\\n>[DeepInfra](https://deepinfra.com/docs) allows us to run the \\n> [latest machine learning models](https://deepinfra.com/models) with ease. \\n> DeepInfra takes care of all the heavy lifting related to running, scaling and monitoring \\n> the models. Users can focus on your application and integrate the models with simple REST API calls.\\n\\n>DeepInfra provides [examples](https://deepinfra.com/docs/advanced/langchain) of integration with LangChain.\\n\\nThis page covers how to use the `DeepInfra` ecosystem within `LangChain`.\\nIt is broken into two parts: installation and setup, and then references to specific DeepInfra wrappers.\\n\\n## Installation and Setup\\n\\n- Get your DeepInfra api key from this link [here](https://deepinfra.com/).\\n- Get an DeepInfra api key and set it as an environment variable (`DEEPINFRA_API_TOKEN`)\\n\\n## Available Models\\n\\nDeepInfra provides a range of Open Source LLMs ready for deployment.'), Document(metadata={'source': 'docs/docs/integrations/providers/deepinfra.mdx', 'file_path': 'docs/docs/integrations/providers/deepinfra.mdx', 'file_name': 'deepinfra.mdx', 'file_type': '.mdx'}, page_content='You can see supported models for\\n[text-generation](https://deepinfra.com/models?type=text-generation) and\\n[embeddings](https://deepinfra.com/models?type=embeddings).\\n\\nYou can view a [list of request and response parameters](https://deepinfra.com/meta-llama/Llama-2-70b-chat-hf/api).\\n\\nChat models [follow openai api](https://deepinfra.com/meta-llama/Llama-2-70b-chat-hf/api?example=openai-http)\\n\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/deepinfra).\\n\\n```python\\nfrom langchain_community.llms import DeepInfra\\n```\\n\\n## Embeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/deepinfra).\\n\\n```python\\nfrom langchain_community.embeddings import DeepInfraEmbeddings\\n```\\n\\n## Chat Models\\n\\nSee a [usage example](/docs/integrations/chat/deepinfra).\\n\\n```python\\nfrom langchain_community.chat_models import ChatDeepInfra\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/deeplake.mdx', 'file_path': 'docs/docs/integrations/providers/deeplake.mdx', 'file_name': 'deeplake.mdx', 'file_type': '.mdx'}, page_content='# Deeplake\\n\\n[Deeplake](https://www.deeplake.ai/) is a database optimized for AI and deep learning\\napplications.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-deeplake\\n```\\n\\n## Vector stores\\n\\nSee detail on available vector stores\\n[here](/docs/integrations/vectorstores/activeloop_deeplake).'), Document(metadata={'source': 'docs/docs/integrations/providers/deepsparse.mdx', 'file_path': 'docs/docs/integrations/providers/deepsparse.mdx', 'file_name': 'deepsparse.mdx', 'file_type': '.mdx'}, page_content=\"# DeepSparse\\n\\nThis page covers how to use the [DeepSparse](https://github.com/neuralmagic/deepsparse) inference runtime within LangChain.\\nIt is broken into two parts: installation and setup, and then examples of DeepSparse usage.\\n\\n## Installation and Setup\\n\\n- Install the Python package with `pip install deepsparse`\\n- Choose a [SparseZoo model](https://sparsezoo.neuralmagic.com/?useCase=text_generation) or export a support model to ONNX [using Optimum](https://github.com/neuralmagic/notebooks/blob/main/notebooks/opt-text-generation-deepsparse-quickstart/OPT_Text_Generation_DeepSparse_Quickstart.ipynb)\\n\\n\\n## LLMs\\n\\nThere exists a DeepSparse LLM wrapper, which you can access with:\\n\\n```python\\nfrom langchain_community.llms import DeepSparse\\n```\\n\\nIt provides a unified interface for all models:\\n\\n```python\\nllm = DeepSparse(model='zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none')\\n\\nprint(llm.invoke('def fib():'))\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/deepsparse.mdx', 'file_path': 'docs/docs/integrations/providers/deepsparse.mdx', 'file_name': 'deepsparse.mdx', 'file_type': '.mdx'}, page_content=\"Additional parameters can be passed using the `config` parameter:\\n\\n```python\\nconfig = {'max_generated_tokens': 256}\\n\\nllm = DeepSparse(model='zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none', config=config)\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/diffbot.mdx', 'file_path': 'docs/docs/integrations/providers/diffbot.mdx', 'file_name': 'diffbot.mdx', 'file_type': '.mdx'}, page_content=\"# Diffbot\\n\\n> [Diffbot](https://docs.diffbot.com/docs) is a suite of ML-based products that make it easy to structure and integrate web data.\\n\\n## Installation and Setup\\n\\n[Get a free Diffbot API token](https://app.diffbot.com/get-started/) and [follow these instructions](https://docs.diffbot.com/reference/authentication) to authenticate your requests.\\n\\n## Document Loader\\n\\nDiffbot's [Extract API](https://docs.diffbot.com/reference/extract-introduction) is a service that structures and normalizes data from web pages. \\n\\nUnlike traditional web scraping tools, `Diffbot Extract` doesn't require any rules to read the content on a page. It uses a computer vision model to classify a page into one of 20 possible types, and then transforms raw HTML markup into JSON. The resulting structured JSON follows a consistent [type-based ontology](https://docs.diffbot.com/docs/ontology), which makes it easy to extract data from multiple different web sources with the same schema.\"), Document(metadata={'source': 'docs/docs/integrations/providers/diffbot.mdx', 'file_path': 'docs/docs/integrations/providers/diffbot.mdx', 'file_name': 'diffbot.mdx', 'file_type': '.mdx'}, page_content=\"See a [usage example](/docs/integrations/document_loaders/diffbot).\\n\\n```python\\nfrom langchain_community.document_loaders import DiffbotLoader\\n```\\n\\n## Graphs\\n\\nDiffbot's [Natural Language Processing API](https://www.diffbot.com/products/natural-language/) allows for the extraction of entities, relationships, and semantic meaning from unstructured text data.\\n\\nSee a [usage example](/docs/integrations/graphs/diffbot).\\n\\n```python\\nfrom langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/dingo.mdx', 'file_path': 'docs/docs/integrations/providers/dingo.mdx', 'file_name': 'dingo.mdx', 'file_type': '.mdx'}, page_content='# DingoDB\\n\\n>[DingoDB](https://github.com/dingodb) is a distributed multi-modal vector \\n> database. It combines the features of a data lake and a vector database, \\n> allowing for the storage of any type of data (key-value, PDF, audio, \\n> video, etc.) regardless of its size. Utilizing DingoDB, you can construct \\n> your own Vector Ocean (the next-generation data architecture following data \\n> warehouse and data lake). This enables \\n> the analysis of both structured and unstructured data through \\n> a singular SQL with exceptionally low latency in real time.\\n\\n## Installation and Setup\\n\\nInstall the Python SDK\\n\\n```bash\\npip install dingodb\\n```\\n\\n## VectorStore\\n\\nThere exists a wrapper around DingoDB indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import Dingo\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dingo.mdx', 'file_path': 'docs/docs/integrations/providers/dingo.mdx', 'file_name': 'dingo.mdx', 'file_type': '.mdx'}, page_content='For a more detailed walkthrough of the DingoDB wrapper, see [this notebook](/docs/integrations/vectorstores/dingo)'), Document(metadata={'source': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_path': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_name': 'discord-shikenso.mdx', 'file_type': '.mdx'}, page_content='# Discord\\n\\n> [Discord](https://discord.com/) is an instant messaging, voice, and video communication platform widely used by communities of all types.\\n\\n## Installation and Setup\\n\\nInstall the `langchain-discord-shikenso` package:\\n\\n```bash\\npip install langchain-discord-shikenso\\n```\\n\\nYou must provide a bot token via environment variable so the tools can authenticate with the Discord API:\\n\\n```bash\\nexport DISCORD_BOT_TOKEN=\"your-discord-bot-token\"\\n```\\n\\nIf `DISCORD_BOT_TOKEN` is not set, the tools will raise a `ValueError` when instantiated.\\n\\n---\\n\\n## Tools\\n\\nBelow is a snippet showing how you can read and send messages in Discord. For more details, see the [documentation for Discord tools](/docs/integrations/tools/discord).\\n\\n```python\\nfrom langchain_discord.tools.discord_read_messages import DiscordReadMessages\\nfrom langchain_discord.tools.discord_send_messages import DiscordSendMessage\\n\\n# Create tool instances\\nread_tool = DiscordReadMessages()\\nsend_tool = DiscordSendMessage()'), Document(metadata={'source': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_path': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_name': 'discord-shikenso.mdx', 'file_type': '.mdx'}, page_content='# Example: Read the last 3 messages from channel 1234567890\\nread_result = read_tool({\"channel_id\": \"1234567890\", \"limit\": 3})\\nprint(read_result)\\n\\n# Example: Send a message to channel 1234567890\\nsend_result = send_tool({\"channel_id\": \"1234567890\", \"message\": \"Hello from Markdown example!\"})\\nprint(send_result)\\n```\\n\\n---\\n\\n## Toolkit\\n\\n`DiscordToolkit` groups multiple Discord-related tools into a single interface. For a usage example, see [the Discord toolkit docs](/docs/integrations/tools/discord).\\n\\n```python\\nfrom langchain_discord.toolkits import DiscordToolkit\\n\\ntoolkit = DiscordToolkit()\\ntools = toolkit.get_tools()\\n\\nread_tool = tools[0]  # DiscordReadMessages\\nsend_tool = tools[1]  # DiscordSendMessage\\n```\\n\\n---\\n\\n## Future Integrations'), Document(metadata={'source': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_path': 'docs/docs/integrations/providers/discord-shikenso.mdx', 'file_name': 'discord-shikenso.mdx', 'file_type': '.mdx'}, page_content='Additional integrations (e.g., document loaders, chat loaders) could be added for Discord.\\nCheck the [Discord Developer Docs](https://discord.com/developers/docs/intro) for more information, and watch for updates or advanced usage examples in the [langchain_discord GitHub repo](https://github.com/Shikenso-Analytics/langchain-discord).'), Document(metadata={'source': 'docs/docs/integrations/providers/discord.mdx', 'file_path': 'docs/docs/integrations/providers/discord.mdx', 'file_name': 'discord.mdx', 'file_type': '.mdx'}, page_content='# Discord (community loader)\\n\\n>[Discord](https://discord.com/) is a VoIP and instant messaging social platform. Users have the ability to communicate \\n> with voice calls, video calls, text messaging, media and files in private chats or as part of communities called \\n> \"servers\". A server is a collection of persistent chat rooms and voice channels which can be accessed via invite links.\\n\\n## Installation and Setup\\n\\n```bash\\npip install pandas\\n```\\n\\nFollow these steps to download your `Discord` data:\\n\\n1. Go to your **User Settings**\\n2. Then go to **Privacy and Safety**\\n3. Head over to the **Request all of my Data** and click on **Request Data** button\\n\\nIt might take 30 days for you to receive your data. You\\'ll receive an email at the address which is registered \\nwith Discord. That email will have a download button using which you would be able to download your personal Discord data.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/discord).'), Document(metadata={'source': 'docs/docs/integrations/providers/discord.mdx', 'file_path': 'docs/docs/integrations/providers/discord.mdx', 'file_name': 'discord.mdx', 'file_type': '.mdx'}, page_content='**NOTE:** The  `DiscordChatLoader` is not the `ChatLoader` but a `DocumentLoader`. \\nIt is used to load the data from the `Discord` data dump.\\nFor the `ChatLoader` see Chat Loader section below.\\n\\n```python\\nfrom langchain_community.document_loaders import DiscordChatLoader\\n```\\n\\n## Chat Loader\\n\\nSee a [usage example](/docs/integrations/chat_loaders/discord).'), Document(metadata={'source': 'docs/docs/integrations/providers/docarray.mdx', 'file_path': 'docs/docs/integrations/providers/docarray.mdx', 'file_name': 'docarray.mdx', 'file_type': '.mdx'}, page_content='# DocArray\\n\\n> [DocArray](https://docarray.jina.ai/) is a library for nested, unstructured, multimodal data in transit, \\n> including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, \\n> embed, search, recommend, store, and transfer multimodal data with a Pythonic API.\\n\\n\\n## Installation and Setup\\n\\nWe need to install `docarray` python package.\\n\\n```bash\\npip install docarray\\n```\\n\\n## Vector Store\\n\\nLangChain provides an access to the `In-memory` and `HNSW` vector stores from the `DocArray` library.\\n\\nSee a [usage example](/docs/integrations/vectorstores/docarray_hnsw).\\n\\n```python\\nfrom langchain_community.vectorstores import DocArrayHnswSearch\\n```\\nSee a [usage example](/docs/integrations/vectorstores/docarray_in_memory).\\n\\n```python\\nfrom langchain_community.vectorstores DocArrayInMemorySearch\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/docarray_retriever).'), Document(metadata={'source': 'docs/docs/integrations/providers/docarray.mdx', 'file_path': 'docs/docs/integrations/providers/docarray.mdx', 'file_name': 'docarray.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.retrievers import DocArrayRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/docling.mdx', 'file_path': 'docs/docs/integrations/providers/docling.mdx', 'file_name': 'docling.mdx', 'file_type': '.mdx'}, page_content='# Docling\\n\\n> [Docling](https://github.com/DS4SD/docling) parses PDF, DOCX, PPTX, HTML, and other formats into a rich unified representation including document layout, tables etc., making them ready for generative AI workflows like RAG.\\n>\\n> This integration provides Docling\\'s capabilities via the `DoclingLoader` document loader.\\n\\n## Installation and Setup\\n\\nSimply install `langchain-docling` from your package manager, e.g. pip:\\n\\n```shell\\npip install langchain-docling\\n```\\n\\n## Document Loader\\n\\nThe `DoclingLoader` class in `langchain-docling` seamlessly integrates Docling into\\nLangChain, enabling you to:\\n- use various document types in your LLM applications with ease and speed, and\\n- leverage Docling\\'s rich representation for advanced, document-native grounding.\\n\\nBasic usage looks as follows:\\n\\n```python\\nfrom langchain_docling import DoclingLoader\\n\\nFILE_PATH = [\"https://arxiv.org/pdf/2408.09869\"]  # Docling Technical Report\\n\\nloader = DoclingLoader(file_path=FILE_PATH)'), Document(metadata={'source': 'docs/docs/integrations/providers/docling.mdx', 'file_path': 'docs/docs/integrations/providers/docling.mdx', 'file_name': 'docling.mdx', 'file_type': '.mdx'}, page_content='docs = loader.load()\\n```\\n\\nFor end-to-end usage check out\\n[this example](/docs/integrations/document_loaders/docling).\\n\\n## Additional Resources\\n\\n- [LangChain Docling integration GitHub](https://github.com/DS4SD/docling-langchain)\\n- [LangChain Docling integration PyPI package](https://pypi.org/project/langchain-docling/)\\n- [Docling GitHub](https://github.com/DS4SD/docling)\\n- [Docling docs](https://ds4sd.github.io/docling/)'), Document(metadata={'source': 'docs/docs/integrations/providers/doctran.mdx', 'file_path': 'docs/docs/integrations/providers/doctran.mdx', 'file_name': 'doctran.mdx', 'file_type': '.mdx'}, page_content='# Doctran\\n\\n>[Doctran](https://github.com/psychic-api/doctran) is a python package. It uses LLMs and open-source \\n> NLP libraries to transform raw text into clean, structured, information-dense documents \\n> that are optimized for vector space retrieval. You can think of `Doctran` as a black box where \\n> messy strings go in and nice, clean, labelled strings come out.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install doctran\\n```\\n\\n## Document Transformers\\n\\n### Document Interrogator\\n\\nSee a [usage example for DoctranQATransformer](/docs/integrations/document_transformers/doctran_interrogate_document).\\n\\n```python\\nfrom langchain_community.document_loaders import DoctranQATransformer\\n```\\n### Property Extractor\\n\\nSee a [usage example for DoctranPropertyExtractor](/docs/integrations/document_transformers/doctran_extract_properties).\\n\\n```python\\nfrom langchain_community.document_loaders import DoctranPropertyExtractor\\n```\\n### Document Translator'), Document(metadata={'source': 'docs/docs/integrations/providers/doctran.mdx', 'file_path': 'docs/docs/integrations/providers/doctran.mdx', 'file_name': 'doctran.mdx', 'file_type': '.mdx'}, page_content='See a [usage example for DoctranTextTranslator](/docs/integrations/document_transformers/doctran_translate_document).\\n\\n```python\\nfrom langchain_community.document_loaders import DoctranTextTranslator\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/docugami.mdx', 'file_path': 'docs/docs/integrations/providers/docugami.mdx', 'file_name': 'docugami.mdx', 'file_type': '.mdx'}, page_content='# Docugami\\n\\n>[Docugami](https://docugami.com) converts business documents into a Document XML Knowledge Graph, generating forests \\n> of XML semantic trees representing entire documents. This is a rich representation that includes the semantic and \\n> structural characteristics of various chunks in the document as an XML tree.\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install dgml-utils\\npip install docugami-langchain\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/docugami).\\n\\n```python\\nfrom docugami_langchain.document_loaders import DocugamiLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/docusaurus.mdx', 'file_path': 'docs/docs/integrations/providers/docusaurus.mdx', 'file_name': 'docusaurus.mdx', 'file_type': '.mdx'}, page_content='# Docusaurus\\n\\n>[Docusaurus](https://docusaurus.io/) is a static-site generator which provides \\n> out-of-the-box documentation features.\\n \\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install -U beautifulsoup4 lxml\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/docusaurus).\\n\\n```python\\nfrom langchain_community.document_loaders import DocusaurusLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dria.mdx', 'file_path': 'docs/docs/integrations/providers/dria.mdx', 'file_name': 'dria.mdx', 'file_type': '.mdx'}, page_content='# Dria\\n\\n>[Dria](https://dria.co/) is a hub of public RAG models for developers to \\n> both contribute and utilize a shared embedding lake.\\n\\nSee more details about the LangChain integration with Dria \\nat [this page](https://dria.co/docs/integrations/langchain).\\n\\n## Installation and Setup\\n\\nYou have to install a python package:\\n\\n```bash\\npip install dria\\n```\\n\\nYou have to get an API key from Dria. You can get it by signing up at [Dria](https://dria.co/).\\n\\n## Retrievers\\n\\nSee a [usage example](/docs/integrations/retrievers/dria_index).\\n\\n```python\\nfrom langchain_community.retrievers import DriaRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/dropbox.mdx', 'file_path': 'docs/docs/integrations/providers/dropbox.mdx', 'file_name': 'dropbox.mdx', 'file_type': '.mdx'}, page_content='# Dropbox\\n\\n>[Dropbox](https://en.wikipedia.org/wiki/Dropbox) is a file hosting service that brings everything-traditional \\n> files, cloud content, and web shortcuts together in one place.\\n \\n\\n## Installation and Setup\\n\\nSee the detailed [installation guide](/docs/integrations/document_loaders/dropbox#prerequisites).\\n\\n```bash\\npip install -U dropbox\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/dropbox).\\n\\n```python\\nfrom langchain_community.document_loaders import DropboxLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/duckdb.mdx', 'file_path': 'docs/docs/integrations/providers/duckdb.mdx', 'file_name': 'duckdb.mdx', 'file_type': '.mdx'}, page_content='# DuckDB\\n\\n>[DuckDB](https://duckdb.org/) is an in-process SQL OLAP database management system.\\n\\n## Installation and Setup\\n\\nFirst, you need to install `duckdb` python package.\\n\\n```bash\\npip install duckdb\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/duckdb).\\n\\n```python\\nfrom langchain_community.document_loaders import DuckDBLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/duckduckgo_search.mdx', 'file_path': 'docs/docs/integrations/providers/duckduckgo_search.mdx', 'file_name': 'duckduckgo_search.mdx', 'file_type': '.mdx'}, page_content='# DuckDuckGo Search\\n\\n>[DuckDuckGo Search](https://github.com/deedy5/duckduckgo_search) is a package that\\n> searches for words, documents, images, videos, news, maps and text\\n> translation using the `DuckDuckGo.com` search engine. It is downloading files \\n> and images to a local hard drive.\\n\\n## Installation and Setup\\n\\nYou have to install a python package:\\n\\n```bash\\npip install duckduckgo-search\\n```\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/ddg).\\n\\nThere are two tools available:\\n\\n```python\\nfrom langchain_community.tools import DuckDuckGoSearchRun\\nfrom langchain_community.tools import DuckDuckGoSearchResults\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/e2b.mdx', 'file_path': 'docs/docs/integrations/providers/e2b.mdx', 'file_name': 'e2b.mdx', 'file_type': '.mdx'}, page_content='# E2B\\n\\n>[E2B](https://e2b.dev/) provides open-source secure sandboxes \\n> for AI-generated code execution. See more [here](https://github.com/e2b-dev).\\n\\n## Installation and Setup\\n\\nYou have to install a python package:\\n\\n```bash\\npip install e2b_code_interpreter\\n```\\n\\n## Tool\\n\\nSee a [usage example](/docs/integrations/tools/e2b_data_analysis).\\n\\n```python\\nfrom langchain_community.tools import E2BDataAnalysisTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/edenai.mdx', 'file_path': 'docs/docs/integrations/providers/edenai.mdx', 'file_name': 'edenai.mdx', 'file_type': '.mdx'}, page_content='# Eden AI\\n\\n>[Eden AI](https://docs.edenai.co/docs/getting-started-with-eden-ai) user interface (UI) \\n> is designed for handling the AI projects. With `Eden AI Portal`, \\n> you can perform no-code AI using the best engines for the market.\\n\\n\\n## Installation and Setup\\n\\nAccessing the Eden AI API requires an API key, which you can get by \\n[creating an account](https://app.edenai.run/user/register) and \\nheading [here](https://app.edenai.run/admin/account/settings). \\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/edenai).\\n\\n```python\\nfrom langchain_community.llms import EdenAI\\n\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/edenai).\\n\\n```python\\nfrom langchain_community.chat_models.edenai import ChatEdenAI\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/edenai).\\n\\n```python\\nfrom langchain_community.embeddings.edenai import EdenAiEmbeddings\\n```\\n\\n## Tools'), Document(metadata={'source': 'docs/docs/integrations/providers/edenai.mdx', 'file_path': 'docs/docs/integrations/providers/edenai.mdx', 'file_name': 'edenai.mdx', 'file_type': '.mdx'}, page_content='Eden AI provides a list of tools that grants your Agent the ability to do multiple tasks, such as:\\n* speech to text\\n* text to speech\\n* text explicit content detection\\n* image explicit content detection\\n* object detection\\n* OCR invoice parsing\\n* OCR ID parsing\\n\\nSee a [usage example](/docs/integrations/tools/edenai_tools).\\n\\n```python\\nfrom langchain_community.tools.edenai import (\\n    EdenAiExplicitImageTool,\\n    EdenAiObjectDetectionTool,\\n    EdenAiParsingIDTool,\\n    EdenAiParsingInvoiceTool,\\n    EdenAiSpeechToTextTool,\\n    EdenAiTextModerationTool,\\n    EdenAiTextToSpeechTool,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_path': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_name': 'elasticsearch.mdx', 'file_type': '.mdx'}, page_content='# Elasticsearch\\n\\n> [Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine.\\n> It provides a distributed, multi-tenant-capable full-text search engine with an HTTP web interface and schema-free\\n> JSON documents.\\n\\n## Installation and Setup\\n\\n### Setup Elasticsearch\\n\\nThere are two ways to get started with Elasticsearch:\\n\\n#### Install Elasticsearch on your local machine via Docker\\n\\nExample: Run a single-node Elasticsearch instance with security disabled. \\nThis is not recommended for production use.\\n\\n```bash\\n    docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.9.0\\n```\\n\\n#### Deploy Elasticsearch on Elastic Cloud\\n\\n`Elastic Cloud` is a managed Elasticsearch service. Signup for a [free trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation).\\n\\n### Install Client'), Document(metadata={'source': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_path': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_name': 'elasticsearch.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install elasticsearch\\npip install langchain-elasticsearch\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/elasticsearch).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchEmbeddings\\n```\\n\\n## Vector store\\n\\nSee a [usage example](/docs/integrations/vectorstores/elasticsearch).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchStore\\n```\\n\\n### Third-party integrations\\n\\n#### EcloudESVectorStore\\n\\n```python\\nfrom langchain_community.vectorstores.ecloud_vector_search import EcloudESVectorStore\\n```\\n\\n## Retrievers\\n\\n### ElasticsearchRetriever\\n\\nThe `ElasticsearchRetriever` enables flexible access to all Elasticsearch features \\nthrough the Query DSL. \\n\\nSee a [usage example](/docs/integrations/retrievers/elasticsearch_retriever).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchRetriever\\n```\\n\\n### BM25\\n\\nSee a [usage example](/docs/integrations/retrievers/elastic_search_bm25).'), Document(metadata={'source': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_path': 'docs/docs/integrations/providers/elasticsearch.mdx', 'file_name': 'elasticsearch.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.retrievers import ElasticSearchBM25Retriever\\n```\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/elasticsearch_chat_message_history).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchChatMessageHistory\\n```\\n\\n## LLM cache\\n\\nSee a [usage example](/docs/integrations/llm_caching/#elasticsearch-caches).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchCache\\n```\\n\\n## Byte Store\\n\\nSee a [usage example](/docs/integrations/stores/elasticsearch).\\n\\n```python\\nfrom langchain_elasticsearch import ElasticsearchEmbeddingsCache\\n```\\n\\n## Chain\\n\\nIt is a chain for interacting with Elasticsearch Database.\\n\\n```python\\nfrom langchain.chains.elasticsearch_database import ElasticsearchDatabaseChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/elevenlabs.mdx', 'file_path': 'docs/docs/integrations/providers/elevenlabs.mdx', 'file_name': 'elevenlabs.mdx', 'file_type': '.mdx'}, page_content='# ElevenLabs\\n\\n>[ElevenLabs](https://elevenlabs.io/about) is a voice AI research & deployment company \\n> with a mission to make content universally accessible in any language & voice.\\n>\\n>`ElevenLabs` creates the most realistic, versatile and contextually-aware \\n> AI audio, providing the ability to generate speech in hundreds of \\n> new and existing voices in 29 languages.\\n\\n## Installation and Setup\\n\\nFirst, you need to set up an ElevenLabs account. You can follow the \\n[instructions here](https://docs.elevenlabs.io/welcome/introduction).\\n\\nInstall the Python package:\\n\\n```bash\\npip install elevenlabs\\n```\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/eleven_labs_tts).\\n\\n```python\\nfrom langchain_community.tools import ElevenLabsText2SpeechTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/embedchain.mdx', 'file_path': 'docs/docs/integrations/providers/embedchain.mdx', 'file_name': 'embedchain.mdx', 'file_type': '.mdx'}, page_content='# Embedchain\\n\\n> [Embedchain](https://github.com/embedchain/embedchain) is a RAG framework to create \\n> data pipelines. It loads, indexes, retrieves and syncs all the data.\\n>\\n>It is available as an [open source package](https://github.com/embedchain/embedchain) \\n> and as a [hosted platform solution](https://app.embedchain.ai/).\\n \\n\\n## Installation and Setup\\n\\nInstall the package using pip:\\n\\n```bash\\npip install embedchain\\n```\\n\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/embedchain).\\n\\n```python\\nfrom langchain_community.retrievers import EmbedchainRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/epsilla.mdx', 'file_path': 'docs/docs/integrations/providers/epsilla.mdx', 'file_name': 'epsilla.mdx', 'file_type': '.mdx'}, page_content='# Epsilla\\n\\nThis page covers how to use [Epsilla](https://github.com/epsilla-cloud/vectordb) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Epsilla wrappers.\\n\\n## Installation and Setup\\n\\n- Install the Python SDK with `pip/pip3 install pyepsilla`\\n\\n## Wrappers\\n\\n### VectorStore\\n\\nThere exists a wrapper around Epsilla vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import Epsilla\\n```\\n\\nFor a more detailed walkthrough of the Epsilla wrapper, see [this notebook](/docs/integrations/vectorstores/epsilla)'), Document(metadata={'source': 'docs/docs/integrations/providers/etherscan.mdx', 'file_path': 'docs/docs/integrations/providers/etherscan.mdx', 'file_name': 'etherscan.mdx', 'file_type': '.mdx'}, page_content='# Etherscan\\n\\n>[Etherscan](https://docs.etherscan.io/) is the leading blockchain explorer, \\n> search, API and analytics platform for `Ethereum`, a decentralized smart contracts platform.\\n \\n\\n## Installation and Setup\\n\\nSee the detailed [installation guide](/docs/integrations/document_loaders/etherscan).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/etherscan).\\n\\n```python\\nfrom langchain_community.document_loaders import EtherscanLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/everlyai.mdx', 'file_path': 'docs/docs/integrations/providers/everlyai.mdx', 'file_name': 'everlyai.mdx', 'file_type': '.mdx'}, page_content='# Everly AI\\n\\n> [Everly AI](https://everlyai.xyz/) allows you to run your ML models at scale in the cloud. \\n> It also provides API access to [several LLM models](https://everlyai.xyz/).\\n\\n## Installation and Setup\\n\\nTo use `Everly AI`, you will need an API key. Visit \\n[Everly AI](https://everlyai.xyz/) to create an API key in your profile.\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/everlyai).\\n\\n```python\\nfrom langchain_community.chat_models import ChatEverlyAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/evernote.mdx', 'file_path': 'docs/docs/integrations/providers/evernote.mdx', 'file_name': 'evernote.mdx', 'file_type': '.mdx'}, page_content='# EverNote\\n\\n>[EverNote](https://evernote.com/) is intended for archiving and creating notes in which photos, audio and saved web content can be embedded. Notes are stored in virtual \"notebooks\" and can be tagged, annotated, edited, searched, and exported.\\n\\n## Installation and Setup\\n\\nFirst, you need to install `lxml` and `html2text` python packages.\\n\\n```bash\\npip install lxml\\npip install html2text\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/evernote).\\n\\n```python\\nfrom langchain_community.document_loaders import EverNoteLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/facebook.mdx', 'file_path': 'docs/docs/integrations/providers/facebook.mdx', 'file_name': 'facebook.mdx', 'file_type': '.mdx'}, page_content='# Facebook - Meta\\n\\n>[Meta Platforms, Inc.](https://www.facebook.com/), doing business as `Meta`, formerly \\n> named `Facebook, Inc.`, and `TheFacebook, Inc.`, is an American multinational technology \\n> conglomerate. The company owns and operates `Facebook`, `Instagram`, `Threads`, \\n> and `WhatsApp`, among other products and services.\\n \\n## Embedding models\\n\\n### LASER\\n\\n>[LASER](https://github.com/facebookresearch/LASER) is a Python library developed by \\n> the `Meta AI Research` team and used for \\n> creating multilingual sentence embeddings for \\n> [over 147 languages as of 2/25/2024](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200) \\n\\n```bash\\npip install laser_encoders\\n```\\n\\nSee a [usage example](/docs/integrations/text_embedding/laser).\\n\\n```python\\nfrom langchain_community.embeddings.laser import LaserEmbeddings\\n```\\n\\n## Document loaders\\n\\n### Facebook Messenger'), Document(metadata={'source': 'docs/docs/integrations/providers/facebook.mdx', 'file_path': 'docs/docs/integrations/providers/facebook.mdx', 'file_name': 'facebook.mdx', 'file_type': '.mdx'}, page_content='>[Messenger](https://en.wikipedia.org/wiki/Messenger_(software)) is an instant messaging app and \\n> platform developed by `Meta Platforms`. Originally developed as `Facebook Chat` in 2008, the company revamped its\\n> messaging service in 2010.\\n\\nSee a [usage example](/docs/integrations/document_loaders/facebook_chat).\\n\\n```python\\nfrom langchain_community.document_loaders import FacebookChatLoader\\n```\\n\\n## Vector stores\\n\\n### Facebook Faiss\\n\\n>[Facebook AI Similarity Search (Faiss)](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) \\n> is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that \\n> search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting \\n> code for evaluation and parameter tuning.\\n\\n[Faiss documentation](https://faiss.ai/).\\n\\nWe need to install `faiss` python package.'), Document(metadata={'source': 'docs/docs/integrations/providers/facebook.mdx', 'file_path': 'docs/docs/integrations/providers/facebook.mdx', 'file_name': 'facebook.mdx', 'file_type': '.mdx'}, page_content=\"```bash\\npip install faiss-gpu # For CUDA 7.5+ supported GPU's.\\n```\\n\\nOR\\n\\n```bash\\npip install faiss-cpu # For CPU Installation\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/faiss).\\n\\n```python\\nfrom langchain_community.vectorstores import FAISS\\n```\\n\\n## Chat loaders\\n\\n### Facebook Messenger\\n\\n>[Messenger](https://en.wikipedia.org/wiki/Messenger_(software)) is an instant messaging app and \\n> platform developed by `Meta Platforms`. Originally developed as `Facebook Chat` in 2008, the company revamped its\\n> messaging service in 2010.\\n\\nSee a [usage example](/docs/integrations/chat_loaders/facebook).\\n\\n```python\\nfrom langchain_community.chat_loaders.facebook_messenger import (\\n    FolderFacebookMessengerChatLoader,\\n    SingleFileFacebookMessengerChatLoader,\\n)\\n```\\n\\n### Facebook WhatsApp\\n\\nSee a [usage example](/docs/integrations/chat_loaders/whatsapp).\\n\\n```python\\nfrom langchain_community.chat_loaders.whatsapp import WhatsAppChatLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/fauna.mdx', 'file_path': 'docs/docs/integrations/providers/fauna.mdx', 'file_name': 'fauna.mdx', 'file_type': '.mdx'}, page_content='# Fauna\\n\\n>[Fauna](https://fauna.com/) is a distributed document-relational database \\n> that combines the flexibility of documents with the power of a relational, \\n> ACID compliant database that scales across regions, clouds or the globe.\\n \\n\\n## Installation and Setup\\n\\nWe have to get the secret key.\\nSee the detailed [guide](https://docs.fauna.com/fauna/current/learn/security_model/).\\n\\nWe have to install the `fauna` package.\\n\\n```bash\\npip install -U fauna\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/fauna).\\n\\n```python\\nfrom langchain_community.document_loaders.fauna import FaunaLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/figma.mdx', 'file_path': 'docs/docs/integrations/providers/figma.mdx', 'file_name': 'figma.mdx', 'file_type': '.mdx'}, page_content=\"# Figma\\n\\n>[Figma](https://www.figma.com/) is a collaborative web application for interface design.\\n\\n## Installation and Setup\\n\\nThe Figma API requires an `access token`, `node_ids`, and a `file key`.\\n\\nThe `file key` can be pulled from the URL.  https://www.figma.com/file/\\\\{filekey\\\\}/sampleFilename\\n\\n`Node IDs` are also available in the URL. Click on anything and look for the '?node-id=\\\\{node_id\\\\}' param.\\n\\n`Access token` [instructions](https://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokens).\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/figma).\\n\\n```python\\nfrom langchain_community.document_loaders import FigmaFileLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/firecrawl.mdx', 'file_path': 'docs/docs/integrations/providers/firecrawl.mdx', 'file_name': 'firecrawl.mdx', 'file_type': '.mdx'}, page_content='# FireCrawl\\n\\n>[FireCrawl](https://firecrawl.dev/?ref=langchain) crawls and converts any website into LLM-ready data. \\n> It crawls all accessible subpages and give you clean markdown \\n> and metadata for each. No sitemap required.\\n\\n\\n## Installation and Setup\\n\\nInstall the python SDK:\\n\\n```bash\\npip install firecrawl-py==0.0.20\\n```\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/firecrawl).\\n\\n```python\\nfrom langchain_community.document_loaders import FireCrawlLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='# Flyte\\n\\n> [Flyte](https://github.com/flyteorg/flyte) is an open-source orchestrator that facilitates building production-grade data and ML pipelines.\\n> It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform.\\n\\nThe purpose of this notebook is to demonstrate the integration of a `FlyteCallback` into your Flyte task, enabling you to effectively monitor and track your LangChain experiments.\\n\\n## Installation & Setup\\n\\n- Install the Flytekit library by running the command `pip install flytekit`.\\n- Install the Flytekit-Envd plugin by running the command `pip install flytekitplugins-envd`.\\n- Install LangChain by running the command `pip install langchain`.\\n- Install [Docker](https://docs.docker.com/engine/install/) on your system.\\n\\n## Flyte Tasks'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='A Flyte [task](https://docs.flyte.org/en/latest/user_guide/basics/tasks.html) serves as the foundational building block of Flyte.\\nTo execute LangChain experiments, you need to write Flyte tasks that define the specific steps and operations involved.\\n\\nNOTE: The [getting started guide](https://docs.flyte.org/projects/cookbook/en/latest/index.html) offers detailed, step-by-step instructions on installing Flyte locally and running your initial Flyte pipeline.\\n\\nFirst, import the necessary dependencies to support your LangChain experiments.\\n\\n```python\\nimport os\\n\\nfrom flytekit import ImageSpec, task\\nfrom langchain.agents import AgentType, initialize_agent, load_tools\\nfrom langchain.callbacks import FlyteCallbackHandler\\nfrom langchain.chains import LLMChain\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.messages import HumanMessage\\n```\\n\\nSet up the necessary environment variables to utilize the OpenAI API and Serp API:'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='```python\\n# Set OpenAI API key\\nos.environ[\"OPENAI_API_KEY\"] = \"<your_openai_api_key>\"\\n\\n# Set Serp API key\\nos.environ[\"SERPAPI_API_KEY\"] = \"<your_serp_api_key>\"\\n```\\n\\nReplace `<your_openai_api_key>` and `<your_serp_api_key>` with your respective API keys obtained from OpenAI and Serp API.\\n\\nTo guarantee reproducibility of your pipelines, Flyte tasks are containerized.\\nEach Flyte task must be associated with an image, which can either be shared across the entire Flyte [workflow](https://docs.flyte.org/en/latest/user_guide/basics/workflows.html) or provided separately for each task.\\n\\nTo streamline the process of supplying the required dependencies for each Flyte task, you can initialize an [`ImageSpec`](https://docs.flyte.org/en/latest/user_guide/customizing_dependencies/imagespec.html) object.\\nThis approach automatically triggers a Docker build, alleviating the need for users to manually create a Docker image.'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='```python\\ncustom_image = ImageSpec(\\n    name=\"langchain-flyte\",\\n    packages=[\\n        \"langchain\",\\n        \"openai\",\\n        \"spacy\",\\n        \"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\",\\n        \"textstat\",\\n        \"google-search-results\",\\n    ],\\n    registry=\"<your-registry>\",\\n)\\n```\\n\\nYou have the flexibility to push the Docker image to a registry of your preference.\\n[Docker Hub](https://hub.docker.com/) or [GitHub Container Registry (GHCR)](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) is a convenient option to begin with.\\n\\nOnce you have selected a registry, you can proceed to create Flyte tasks that log the LangChain metrics to Flyte Deck.\\n\\nThe following examples demonstrate tasks related to OpenAI LLM, chains and agent with tools:\\n\\n### LLM'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_llm() -> str:\\n    llm = ChatOpenAI(\\n        model_name=\"gpt-3.5-turbo\",\\n        temperature=0.2,\\n        callbacks=[FlyteCallbackHandler()],\\n    )\\n    return llm.invoke([HumanMessage(content=\"Tell me a joke\")]).content\\n```\\n\\n### Chain'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_chain() -> list[dict[str, str]]:\\n    template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\n    llm = ChatOpenAI(\\n        model_name=\"gpt-3.5-turbo\",\\n        temperature=0,\\n        callbacks=[FlyteCallbackHandler()],\\n    )\\n    prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\n    synopsis_chain = LLMChain(\\n        llm=llm, prompt=prompt_template, callbacks=[FlyteCallbackHandler()]\\n    )\\n    test_prompts = [\\n        {\\n            \"title\": \"documentary about good video games that push the boundary of game design\"\\n        },\\n    ]\\n    return synopsis_chain.apply(test_prompts)\\n```\\n\\n### Agent'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_agent() -> str:\\n    llm = OpenAI(\\n        model_name=\"gpt-3.5-turbo\",\\n        temperature=0,\\n        callbacks=[FlyteCallbackHandler()],\\n    )\\n    tools = load_tools(\\n        [\"serpapi\", \"llm-math\"], llm=llm, callbacks=[FlyteCallbackHandler()]\\n    )\\n    agent = initialize_agent(\\n        tools,\\n        llm,\\n        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\n        callbacks=[FlyteCallbackHandler()],\\n        verbose=True,\\n    )\\n    return agent.run(\\n        \"Who is Leonardo DiCaprio\\'s girlfriend? Could you calculate her current age and raise it to the power of 0.43?\"\\n    )\\n```\\n\\nThese tasks serve as a starting point for running your LangChain experiments within Flyte.\\n\\n## Execute the Flyte Tasks on Kubernetes\\n\\nTo execute the Flyte tasks on the configured Flyte backend, use the following command:\\n\\n```bash\\npyflyte run --image <your-image> langchain_flyte.py langchain_llm\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/flyte.mdx', 'file_path': 'docs/docs/integrations/providers/flyte.mdx', 'file_name': 'flyte.mdx', 'file_type': '.mdx'}, page_content='This command will initiate the execution of the `langchain_llm` task on the Flyte backend. You can trigger the remaining two tasks in a similar manner.\\n\\nThe metrics will be displayed on the Flyte UI as follows:\\n\\n![Screenshot of Flyte Deck showing LangChain metrics and a dependency tree visualization.](https://ik.imagekit.io/c8zl7irwkdda/Screenshot_2023-06-20_at_1.23.29_PM_MZYeG0dKa.png?updatedAt=1687247642993 \"Flyte Deck Metrics Display\")'), Document(metadata={'source': 'docs/docs/integrations/providers/fmp-data.mdx', 'file_path': 'docs/docs/integrations/providers/fmp-data.mdx', 'file_name': 'fmp-data.mdx', 'file_type': '.mdx'}, page_content='# FMP Data (Financial Data Prep)\\n\\n> [FMP-Data](https://pypi.org/project/fmp-data/) is a python package for connecting to\\n> Financial Data Prep API. It simplifies how you can access production quality data.\\n\\n\\n## Installation and Setup\\n\\nGet an `FMP Data` API key by\\nvisiting [this page](https://site.financialmodelingprep.com/pricing-plans?couponCode=mehdi).\\n and set it as an environment variable (`FMP_API_KEY`).\\n\\nThen, install [langchain-fmp-data](https://pypi.org/project/langchain-fmp-data/).\\n\\n## Tools\\n\\nSee an [example](https://github.com/MehdiZare/langchain-fmp-data/tree/main/docs).\\n\\n```python\\nfrom langchain_fmp_data import FMPDataTool, FMPDataToolkit\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/forefrontai.mdx', 'file_path': 'docs/docs/integrations/providers/forefrontai.mdx', 'file_name': 'forefrontai.mdx', 'file_type': '.mdx'}, page_content='# Forefront AI\\n\\n> [Forefront AI](https://forefront.ai/) is a platform enabling you to\\n> fine-tune and inference open-source text generation models \\n\\n\\n## Installation and Setup\\n\\nGet an `ForefrontAI` API key\\nvisiting [this page](https://accounts.forefront.ai/sign-in?redirect_url=https%3A%2F%2Fforefront.ai%2Fapp%2Fapi-keys).\\n and set it as an environment variable (`FOREFRONTAI_API_KEY`).\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/forefrontai).\\n\\n```python\\nfrom langchain_community.llms import ForefrontAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/friendli.mdx', 'file_path': 'docs/docs/integrations/providers/friendli.mdx', 'file_name': 'friendli.mdx', 'file_type': '.mdx'}, page_content='# Friendli AI\\n\\n> [FriendliAI](https://friendli.ai/) enhances AI application performance and optimizes \\n> cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.\\n\\n## Installation and setup\\n\\nInstall the `friendli-client` python package.\\n\\n```bash\\npip install -U langchain_community friendli-client\\n```\\n\\nSign in to [Friendli Suite](https://suite.friendli.ai/) to create a Personal Access Token, \\nand set it as the `FRIENDLI_TOKEN` environment variabzle.\\n\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/friendli).\\n\\n```python\\nfrom langchain_community.chat_models.friendli import ChatFriendli\\n\\nchat = ChatFriendli(model=\\'meta-llama-3.1-8b-instruct\\')\\n\\nfor m in chat.stream(\"Tell me fun things to do in NYC\"):\\n    print(m.content, end=\"\", flush=True)\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/friendli).\\n\\n```python\\nfrom langchain_community.llms.friendli import Friendli\\n\\nllm = Friendli(model=\\'meta-llama-3.1-8b-instruct\\')'), Document(metadata={'source': 'docs/docs/integrations/providers/friendli.mdx', 'file_path': 'docs/docs/integrations/providers/friendli.mdx', 'file_name': 'friendli.mdx', 'file_type': '.mdx'}, page_content='print(llm.invoke(\"def bubble_sort(): \"))\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/geopandas.mdx', 'file_path': 'docs/docs/integrations/providers/geopandas.mdx', 'file_name': 'geopandas.mdx', 'file_type': '.mdx'}, page_content='# Geopandas\\n\\n>[GeoPandas](https://geopandas.org/) is an open source project to make working \\n> with geospatial data in python easier. `GeoPandas` extends the datatypes used by \\n> `pandas` to allow spatial operations on geometric types. \\n> Geometric operations are performed by `shapely`.\\n \\n\\n## Installation and Setup\\n\\nWe have to install several python packages.\\n\\n```bash\\npip install -U sodapy pandas geopandas\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/geopandas).\\n\\n```python\\nfrom langchain_community.document_loaders import OpenCityDataLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/git.mdx', 'file_path': 'docs/docs/integrations/providers/git.mdx', 'file_name': 'git.mdx', 'file_type': '.mdx'}, page_content='# Git\\n\\n>[Git](https://en.wikipedia.org/wiki/Git) is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers collaboratively developing source code during software development.\\n\\n## Installation and Setup\\n\\nFirst, you need to install `GitPython` python package.\\n\\n```bash\\npip install GitPython\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/git).\\n\\n```python\\nfrom langchain_community.document_loaders import GitLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/gitbook.mdx', 'file_path': 'docs/docs/integrations/providers/gitbook.mdx', 'file_name': 'gitbook.mdx', 'file_type': '.mdx'}, page_content=\"# GitBook\\n\\n>[GitBook](https://docs.gitbook.com/) is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/gitbook).\\n\\n```python\\nfrom langchain_community.document_loaders import GitbookLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/github.mdx', 'file_path': 'docs/docs/integrations/providers/github.mdx', 'file_name': 'github.mdx', 'file_type': '.mdx'}, page_content='# GitHub\\n\\n>[GitHub](https://github.com/) is a developer platform that allows developers to create, \\n> store, manage and share their code. It uses `Git` software, providing the \\n> distributed version control of Git plus access control, bug tracking, \\n> software feature requests, task management, continuous integration, and wikis for every project.\\n \\n\\n## Installation and Setup\\n\\nTo access the GitHub API, you need a [personal access token](https://github.com/settings/tokens).\\n\\n\\n## Document Loader\\n\\nThere are two document loaders available for GitHub.\\n\\nSee a [usage example](/docs/integrations/document_loaders/github).\\n\\n```python\\nfrom langchain_community.document_loaders import GitHubIssuesLoader, GithubFileLoader\\n```\\n\\n## Tools/Toolkit\\n\\n### GitHubToolkit\\nThe `GitHub` toolkit contains tools that enable an LLM agent to interact \\nwith a GitHub repository. \\n\\nThe toolkit is a wrapper for the `PyGitHub` library.'), Document(metadata={'source': 'docs/docs/integrations/providers/github.mdx', 'file_path': 'docs/docs/integrations/providers/github.mdx', 'file_name': 'github.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.agent_toolkits.github.toolkit import GitHubToolkit\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/tools/github).\\n\\n### GitHubAction\\n\\nTool for interacting with the GitHub API.\\n\\n```python\\nfrom langchain_community.tools.github.tool import GitHubAction\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/gitlab.mdx', 'file_path': 'docs/docs/integrations/providers/gitlab.mdx', 'file_name': 'gitlab.mdx', 'file_type': '.mdx'}, page_content='# GitLab\\n\\n>[GitLab Inc.](https://about.gitlab.com/) is an open-core company \\n> that operates `GitLab`, a DevOps software package that can develop, \\n> secure, and operate software. `GitLab` includes a distributed version \\n> control based on Git, including features such as access control, bug tracking,\\n> software feature requests, task management, and wikis for every project, \\n> as well as snippets. \\n\\n\\n## Tools/Toolkits\\n\\n### GitLabToolkit\\n\\nThe `Gitlab` toolkit contains tools that enable an LLM agent to interact with a gitlab repository. \\n\\nThe toolkit is a wrapper for the `python-gitlab` library.\\n\\nSee a [usage example](/docs/integrations/tools/gitlab).\\n\\n```python\\nfrom langchain_community.agent_toolkits.gitlab.toolkit import GitLabToolkit\\n```\\n\\n### GitLabAction\\n\\nTool for interacting with the GitLab API.\\n\\n```python\\nfrom langchain_community.tools.gitlab.tool import GitLabAction\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/golden.mdx', 'file_path': 'docs/docs/integrations/providers/golden.mdx', 'file_name': 'golden.mdx', 'file_type': '.mdx'}, page_content='# Golden\\n\\n>[Golden](https://golden.com) provides a set of natural language APIs for querying and enrichment using the Golden Knowledge Graph e.g. queries such as: `Products from OpenAI`, `Generative ai companies with series a funding`, and `rappers who invest` can be used to retrieve structured data about relevant entities.\\n>\\n>The `golden-query` langchain tool is a wrapper on top of the [Golden Query API](https://docs.golden.com/reference/query-api) which enables programmatic access to these results.\\n>See the [Golden Query API docs](https://docs.golden.com/reference/query-api) for more information.\\n\\n## Installation and Setup\\n- Go to the [Golden API docs](https://docs.golden.com/) to get an overview about the Golden API.\\n- Get your API key from the [Golden API Settings](https://golden.com/settings/api) page.\\n- Save your API key into GOLDEN_API_KEY env variable\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a GoldenQueryAPIWrapper utility which wraps this API. To import this utility:'), Document(metadata={'source': 'docs/docs/integrations/providers/golden.mdx', 'file_path': 'docs/docs/integrations/providers/golden.mdx', 'file_name': 'golden.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.utilities.golden_query import GoldenQueryAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/golden_query).\\n\\n### Tool\\n\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"golden-query\"])\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/goodfire.mdx', 'file_path': 'docs/docs/integrations/providers/goodfire.mdx', 'file_name': 'goodfire.mdx', 'file_type': '.mdx'}, page_content='# Goodfire\\n\\n[Goodfire](https://www.goodfire.ai/) is a research lab focused on AI safety and\\ninterpretability.\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-goodfire\\n```\\n\\n## Chat models\\n\\nSee detail on available chat models [here](/docs/integrations/chat/goodfire).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='# Google\\n\\nAll functionality related to [Google Cloud Platform](https://cloud.google.com/) and other `Google` products.\\n\\nIntegration packages for Gemini models and the VertexAI platform are maintained in\\nthe [langchain-google](https://github.com/langchain-ai/langchain-google) repository.\\nYou can find a host of LangChain integrations with other Google APIs in the\\n[googleapis](https://github.com/googleapis?q=langchain-&type=all&language=&sort=)\\nGithub organization.\\n\\n## Chat models\\n\\nWe recommend individual developers to start with Gemini API (`langchain-google-genai`) and move to Vertex AI (`langchain-google-vertexai`) when they need access to commercial support and higher rate limits. If you’re already Cloud-friendly or Cloud-native, then you can get started in Vertex AI straight away.\\nPlease see [here](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more information.\\n\\n### Google Generative AI'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `ChatGoogleGenerativeAI` class.\\n\\n```bash\\npip install -U langchain-google-genai\\n```\\n\\nConfigure your API key.\\n\\n```bash\\nexport GOOGLE_API_KEY=your-api-key\\n```\\n\\n```python\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\n\\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\\nllm.invoke(\"Sing a ballad of LangChain.\")\\n```\\n\\nGemini vision model supports image inputs when providing a single chat message.\\n\\n```python\\nfrom langchain_core.messages import HumanMessage\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\n\\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\\n\\nmessage = HumanMessage(\\n    content=[\\n        {\\n            \"type\": \"text\",\\n            \"text\": \"What\\'s in this image?\",\\n        },  # You can optionally provide text parts\\n        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\\n    ]\\n)\\nllm.invoke([message])\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='The value of image_url can be any of the following:\\n\\n- A public image URL\\n- A gcs file (e.g., \"gcs://path/to/file.png\")\\n- A local file path\\n- A base64 encoded image (e.g., data:image/png;base64,abcd124)\\n- A PIL image\\n\\n### Vertex AI\\n\\nAccess chat models like `Gemini` via Google Cloud.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\nSee a [usage example](/docs/integrations/chat/google_vertex_ai_palm).\\n\\n```python\\nfrom langchain_google_vertexai import ChatVertexAI\\n```\\n\\n### Anthropic on Vertex AI Model Garden\\n\\nSee a [usage example](/docs/integrations/llms/google_vertex_ai_palm).\\n\\n```python\\nfrom langchain_google_vertexai.model_garden import ChatAnthropicVertex\\n```\\n\\n### Llama on Vertex AI Model Garden\\n\\n```python\\nfrom langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama\\n```\\n\\n### Mistral on Vertex AI Model Garden'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral\\n```\\n\\n### Gemma local from Hugging Face\\n\\n>Local `Gemma` model loaded from `HuggingFace`.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaChatLocalHF\\n```\\n\\n### Gemma local from Kaggle\\n\\n>Local `Gemma` model loaded from `Kaggle`.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaChatLocalKaggle\\n```\\n\\n### Gemma on Vertex AI Model Garden\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden\\n```\\n\\n### Vertex AI image captioning\\n\\n>Implementation of the `Image Captioning model` as a chat.'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='We need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat\\n```\\n\\n### Vertex AI image editor\\n\\n>Given an image and a prompt, edit the image. Currently only supports mask-free editing.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.vision_models import VertexAIImageEditorChat\\n```\\n\\n### Vertex AI image generator\\n\\n>Generates an image from a prompt.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat\\n```\\n\\n### Vertex AI visual QnA\\n\\n>Chat implementation of a visual QnA model\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_vertexai.vision_models import VertexAIVisualQnAChat\\n```\\n\\n## LLMs\\n\\n### Google Generative AI\\n\\nAccess GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `GoogleGenerativeAI` class.\\n\\nInstall python package.\\n\\n```bash\\npip install langchain-google-genai\\n```\\n\\nSee a [usage example](/docs/integrations/llms/google_ai).\\n\\n```python\\nfrom langchain_google_genai import GoogleGenerativeAI\\n```\\n\\n### Vertex AI Model Garden\\n\\nAccess `PaLM` and hundreds of OSS models via `Vertex AI Model Garden` service.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\nSee a [usage example](/docs/integrations/llms/google_vertex_ai_palm#vertex-model-garden).\\n\\n```python\\nfrom langchain_google_vertexai import VertexAIModelGarden\\n```\\n\\n### Gemma local from Hugging Face\\n\\n>Local `Gemma` model loaded from `HuggingFace`.\\n\\nWe need to install `langchain-google-vertexai` python package.'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaLocalHF\\n```\\n\\n### Gemma local from Kaggle\\n\\n>Local `Gemma` model loaded from `Kaggle`.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaLocalKaggle\\n```\\n\\n### Gemma on Vertex AI Model Garden\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\\n```\\n\\n### Vertex AI image captioning\\n\\n>Implementation of the `Image Captioning model` as an LLM.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.vision_models import VertexAIImageCaptioning\\n```\\n\\n## Embedding models\\n\\n### Google Generative AI embedding'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/text_embedding/google_generative_ai).\\n\\n```bash\\npip install -U langchain-google-genai\\n```\\n\\nConfigure your API key.\\n\\n```bash\\nexport GOOGLE_API_KEY=your-api-key\\n```\\n\\n```python\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n```\\n\\n### Google Generative AI server-side embedding\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-genai\\n```\\n\\n```python\\nfrom langchain_google_genai.google_vector_store import ServerSideEmbedding\\n```\\n\\n### Vertex AI\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\nSee a [usage example](/docs/integrations/text_embedding/google_vertex_ai_palm).\\n\\n```python\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\n```\\n\\n### Palm embedding\\n\\nWe need to install `langchain-community` python package.\\n\\n```bash\\npip install langchain-community\\n```\\n\\n```python\\nfrom langchain_community.embeddings.google_palm import GooglePalmEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='## Document Loaders\\n\\n### AlloyDB for PostgreSQL\\n\\n> [Google Cloud AlloyDB](https://cloud.google.com/alloydb) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-alloydb-pg\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_alloydb).\\n\\n```python\\nfrom langchain_google_alloydb_pg import AlloyDBEngine, AlloyDBLoader\\n```\\n\\n### BigQuery\\n\\n> [Google Cloud BigQuery](https://cloud.google.com/bigquery) is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data in Google Cloud.\\n\\nWe need to install `langchain-google-community` with Big Query dependencies:\\n\\n```bash\\npip install langchain-google-community[bigquery]\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/google_bigquery).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content=\"```python\\nfrom langchain_google_community import BigQueryLoader\\n```\\n\\n### Bigtable\\n\\n> [Google Cloud Bigtable](https://cloud.google.com/bigtable/docs) is Google's fully managed NoSQL Big Data database service in Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-bigtable\\n```\\n\\nSee [Googel Cloud usage example](/docs/integrations/document_loaders/google_bigtable).\\n\\n```python\\nfrom langchain_google_bigtable import BigtableLoader\\n```\\n\\n### Cloud SQL for MySQL\\n\\n> [Google Cloud SQL for MySQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-mysql\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_cloud_sql_mysql).\\n\\n```python\\nfrom langchain_google_cloud_sql_mysql import MySQLEngine, MySQLLoader\\n```\\n\\n### Cloud SQL for SQL Server\"), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Google Cloud SQL for SQL Server](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your SQL Server databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-mssql\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_cloud_sql_mssql).\\n\\n```python\\nfrom langchain_google_cloud_sql_mssql import MSSQLEngine, MSSQLLoader\\n```\\n\\n### Cloud SQL for PostgreSQL\\n\\n> [Google Cloud SQL for PostgreSQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-pg\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_cloud_sql_pg).\\n\\n```python\\nfrom langchain_google_cloud_sql_pg import PostgresEngine, PostgresLoader\\n```\\n\\n### Cloud Storage'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='>[Cloud Storage](https://en.wikipedia.org/wiki/Google_Cloud_Storage) is a managed service for storing unstructured data in Google Cloud.\\n\\nWe need to install `langchain-google-community` with Google Cloud Storage dependencies.\\n\\n```bash\\npip install langchain-google-community[gcs]\\n```\\n\\nThere are two loaders for the `Google Cloud Storage`: the `Directory` and the `File` loaders.\\n\\nSee a [usage example](/docs/integrations/document_loaders/google_cloud_storage_directory).\\n\\n```python\\nfrom langchain_google_community import GCSDirectoryLoader\\n```\\nSee a [usage example](/docs/integrations/document_loaders/google_cloud_storage_file).\\n\\n```python\\nfrom langchain_google_community import GCSFileLoader\\n```\\n\\n### Cloud Vision loader\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-community[vision]\\n```\\n\\n```python\\nfrom langchain_google_community.vision import CloudVisionLoader\\n```\\n\\n### El Carro for Oracle Workloads'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> Google [El Carro Oracle Operator](https://github.com/GoogleCloudPlatform/elcarro-oracle-operator)\\noffers a way to run Oracle databases in Kubernetes as a portable, open source,\\ncommunity driven, no vendor lock-in container orchestration system.\\n\\n```bash\\npip install langchain-google-el-carro\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_el_carro).\\n\\n```python\\nfrom langchain_google_el_carro import ElCarroLoader\\n```\\n\\n### Google Drive\\n\\n>[Google Drive](https://en.wikipedia.org/wiki/Google_Drive) is a file storage and synchronization service developed by Google.\\n\\nCurrently, only `Google Docs` are supported.\\n\\nWe need to install `langchain-google-community` with Google Drive dependencies.\\n\\n```bash\\npip install langchain-google-community[drive]\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/document_loaders/google_drive).\\n\\n```python\\nfrom langchain_google_community import GoogleDriveLoader\\n```\\n\\n### Firestore (Native Mode)'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Google Cloud Firestore](https://cloud.google.com/firestore/docs/) is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-firestore\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_firestore).\\n\\n```python\\nfrom langchain_google_firestore import FirestoreLoader\\n```\\n\\n### Firestore (Datastore Mode)\\n\\n> [Google Cloud Firestore in Datastore mode](https://cloud.google.com/datastore/docs) is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\\n> Firestore is the newest version of Datastore and introduces several improvements over Datastore.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-datastore\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_datastore).\\n\\n```python\\nfrom langchain_google_datastore import DatastoreLoader\\n```\\n\\n### Memorystore for Redis'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Google Cloud Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis) is a fully managed Redis service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-memorystore-redis\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_memorystore_redis).\\n\\n```python\\nfrom langchain_google_memorystore_redis import MemorystoreDocumentLoader\\n```\\n\\n### Spanner\\n\\n> [Google Cloud Spanner](https://cloud.google.com/spanner/docs) is a fully managed, mission-critical, relational database service on Google Cloud that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL.\\n\\nInstall the python package:'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content=\"```bash\\npip install langchain-google-spanner\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_spanner).\\n\\n```python\\nfrom langchain_google_spanner import SpannerLoader\\n```\\n\\n### Speech-to-Text\\n\\n> [Google Cloud Speech-to-Text](https://cloud.google.com/speech-to-text) is an audio transcription API powered by Google's speech recognition models in Google Cloud.\\n\\nThis document loader transcribes audio files and outputs the text results as Documents.\\n\\nFirst, we need to install `langchain-google-community` with speech-to-text dependencies.\\n\\n```bash\\npip install langchain-google-community[speech]\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/document_loaders/google_speech_to_text).\\n\\n```python\\nfrom langchain_google_community import SpeechToTextLoader\\n```\\n\\n## Document Transformers\\n\\n### Document AI\"), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='>[Google Cloud Document AI](https://cloud.google.com/document-ai/docs/overview) is a Google Cloud\\n> service that transforms unstructured data from documents into structured data, making it easier\\n> to understand, analyze, and consume.\\n\\nWe need to set up a [`GCS` bucket and create your own OCR processor](https://cloud.google.com/document-ai/docs/create-processor)\\nThe `GCS_OUTPUT_PATH` should be a path to a folder on GCS (starting with `gs://`)\\nand a processor name should look like `projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID`.\\nWe can get it either programmatically or copy from the `Prediction endpoint` section of the `Processor details`\\ntab in the Google Cloud Console.\\n\\n```bash\\npip install langchain-google-community[docai]\\n```\\n\\nSee a [usage example](/docs/integrations/document_transformers/google_docai).\\n\\n```python\\nfrom langchain_core.document_loaders.blob_loaders import Blob\\nfrom langchain_google_community import DocAIParser\\n```\\n\\n### Google Translate'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Google Translate](https://translate.google.com/) is a multilingual neural machine\\n> translation service developed by Google to translate text, documents and websites\\n> from one language into another.\\n\\nThe `GoogleTranslateTransformer` allows you to translate text and HTML with the [Google Cloud Translation API](https://cloud.google.com/translate).\\n\\nFirst, we need to install the `langchain-google-community` with translate dependencies.\\n\\n```bash\\npip install langchain-google-community[translate]\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/document_transformers/google_translate).\\n\\n```python\\nfrom langchain_google_community import GoogleTranslateTransformer\\n```\\n\\n## Vector Stores\\n\\n### AlloyDB for PostgreSQL\\n\\n> [Google Cloud AlloyDB](https://cloud.google.com/alloydb) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='Install the python package:\\n\\n```bash\\npip install langchain-google-alloydb-pg\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_alloydb).\\n\\n```python\\nfrom langchain_google_alloydb_pg import AlloyDBEngine, AlloyDBVectorStore\\n```\\n\\n### BigQuery Vector Search\\n\\n> [Google Cloud BigQuery](https://cloud.google.com/bigquery),\\n> BigQuery is a serverless and cost-effective enterprise data warehouse in Google Cloud.\\n>\\n> [Google Cloud BigQuery Vector Search](https://cloud.google.com/bigquery/docs/vector-search-intro)\\n> BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.\\n\\n> It can calculate Euclidean or Cosine distance. With LangChain, we default to use Euclidean distance.\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install google-cloud-bigquery\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/google_bigquery_vector_search).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain.vectorstores import BigQueryVectorSearch\\n```\\n\\n### Memorystore for Redis\\n\\n> [Google Cloud Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis) is a fully managed Redis service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-memorystore-redis\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_memorystore_redis).\\n\\n```python\\nfrom langchain_google_memorystore_redis import RedisVectorStore\\n```\\n\\n### Spanner'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Google Cloud Spanner](https://cloud.google.com/spanner/docs) is a fully managed, mission-critical, relational database service on Google Cloud that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-spanner\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_spanner).\\n\\n```python\\nfrom langchain_google_spanner import SpannerVectorStore\\n```\\n\\n### Firestore (Native Mode)\\n\\n> [Google Cloud Firestore](https://cloud.google.com/firestore/docs/) is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-firestore\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_firestore).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_firestore import FirestoreVectorStore\\n```\\n\\n### Cloud SQL for MySQL\\n\\n> [Google Cloud SQL for MySQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-mysql\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_cloud_sql_mysql).\\n\\n```python\\nfrom langchain_google_cloud_sql_mysql import MySQLEngine, MySQLVectorStore\\n```\\n\\n### Cloud SQL for PostgreSQL\\n\\n> [Google Cloud SQL for PostgreSQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-pg\\n```\\n\\nSee [usage example](/docs/integrations/vectorstores/google_cloud_sql_pg).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content=\"```python\\nfrom langchain_google_cloud_sql_pg import PostgresEngine, PostgresVectorStore\\n```\\n\\n### Vertex AI Vector Search\\n\\n> [Google Cloud Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) from Google Cloud,\\n> formerly known as `Vertex AI Matching Engine`, provides the industry's leading high-scale\\n> low latency vector database. These vector databases are commonly\\n> referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/google_vertex_ai_vector_search).\\n\\n```python\\nfrom langchain_google_vertexai import VectorSearchVectorStore\\n```\\n\\n### Vertex AI Vector Search with DataStore\\n\\n> VectorSearch with DatasTore document storage.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-vertexai\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/vectorstores/google_vertex_ai_vector_search/#optional--you-can-also-create-vectore-and-store-chunks-in-a-datastore).\\n\\n```python\\nfrom langchain_google_vertexai import VectorSearchVectorStoreDatastore\\n```\\n\\n### VectorSearchVectorStoreGCS \\n\\n> Alias of `VectorSearchVectorStore` for consistency \\n> with the rest of vector stores with different document storage backends.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai import VectorSearchVectorStoreGCS\\n```\\n\\n### Google Generative AI Vector Store \\n\\n> Currently, it computes the embedding vectors on the server side.\\n> For more information visit [Guide](https://developers.generativeai.google/guide).\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-genai\\n```\\n\\n```python\\nfrom langchain_google_genai.google_vector_store import GoogleVectorStore\\n```\\n\\n### ScaNN'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='>[Google ScaNN](https://github.com/google-research/google-research/tree/master/scann)\\n> (Scalable Nearest Neighbors) is a python package.\\n>\\n>`ScaNN` is a method for efficient vector similarity search at scale.\\n\\n>`ScaNN` includes search space pruning and quantization for Maximum Inner\\n> Product Search and also supports other distance functions such as\\n> Euclidean distance. The implementation is optimized for x86 processors\\n> with AVX2 support. See its [Google Research github](https://github.com/google-research/google-research/tree/master/scann)\\n> for more details.\\n\\nWe need to install `scann` python package.\\n\\n```bash\\npip install scann\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/scann).\\n\\n```python\\nfrom langchain_community.vectorstores import ScaNN\\n```\\n\\n## Retrievers\\n\\n### Google Drive\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See a [usage example and authorization instructions](/docs/integrations/retrievers/google_drive).\\n\\n```python\\nfrom langchain_googledrive.retrievers import GoogleDriveRetriever\\n```\\n\\n### Vertex AI Search\\n\\n> [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/introduction)\\n> from Google Cloud allows developers to quickly build generative AI powered search engines for customers and employees.\\n\\nSee a [usage example](/docs/integrations/retrievers/google_vertex_ai_search).\\n\\nNote: `GoogleVertexAISearchRetriever` is deprecated, use `VertexAIMultiTurnSearchRetriever`,\\n`VertexAISearchSummaryTool`, and `VertexAISearchRetriever` (see below).\\n\\n#### GoogleVertexAISearchRetriever\\n\\nWe need to install the `google-cloud-discoveryengine` python package.\\n\\n```bash\\npip install google-cloud-discoveryengine\\n```\\n\\n```python\\nfrom langchain_community.retrievers import GoogleVertexAISearchRetriever\\n```\\n\\n#### VertexAIMultiTurnSearchRetriever'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_community import VertexAIMultiTurnSearchRetriever\\n```\\n#### VertexAISearchRetriever\\n\\n```python\\nfrom langchain_google_community import VertexAIMultiTurnSearchRetriever\\n```\\n\\n#### VertexAISearchSummaryTool\\n\\n```python\\nfrom langchain_google_community import VertexAISearchSummaryTool\\n```\\n\\n### Document AI Warehouse\\n\\n> [Document AI Warehouse](https://cloud.google.com/document-ai-warehouse)\\n> from Google Cloud allows enterprises to search, store, govern, and manage documents and their AI-extracted\\n> data and metadata in a single platform.\\n\\nNote: `GoogleDocumentAIWarehouseRetriever` is deprecated, use `DocumentAIWarehouseRetriever` (see below).\\n```python\\nfrom langchain.retrievers import GoogleDocumentAIWarehouseRetriever\\ndocai_wh_retriever = GoogleDocumentAIWarehouseRetriever(\\n    project_number=...\\n)\\nquery = ...\\ndocuments = docai_wh_retriever.invoke(\\n    query, user_ldap=...\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_community.documentai_warehouse import DocumentAIWarehouseRetriever\\n```\\n\\n## Tools\\n\\n### Text-to-Speech\\n\\n>[Google Cloud Text-to-Speech](https://cloud.google.com/text-to-speech) is a Google Cloud service that enables developers to\\n> synthesize natural-sounding speech with 100+ voices, available in multiple languages and variants.\\n> It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks\\n> to deliver the highest fidelity possible.\\n\\nWe need to install python packages.\\n\\n```bash\\npip install google-cloud-text-to-speech langchain-google-community\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_cloud_texttospeech).\\n\\n```python\\nfrom langchain_google_community import TextToSpeechTool\\n```\\n\\n### Google Drive\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\\npip install langchain-googledrive\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See a [usage example and authorization instructions](/docs/integrations/tools/google_drive).\\n\\n```python\\nfrom langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper\\nfrom langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool\\n```\\n\\n### Google Finance\\n\\nWe need to install a python package.\\n\\n```bash\\npip install google-search-results\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_finance).\\n\\n```python\\nfrom langchain_community.tools.google_finance import GoogleFinanceQueryRun\\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\n```\\n\\n### Google Jobs\\n\\nWe need to install a python package.\\n\\n```bash\\npip install google-search-results\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_jobs).\\n\\n```python\\nfrom langchain_community.tools.google_jobs import GoogleJobsQueryRun\\nfrom langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='### Google Lens\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_lens).\\n\\n```python\\nfrom langchain_community.tools.google_lens import GoogleLensQueryRun\\nfrom langchain_community.utilities.google_lens import GoogleLensAPIWrapper\\n```\\n\\n### Google Places\\n\\nWe need to install a python package.\\n\\n```bash\\npip install googlemaps\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_places).\\n\\n```python\\nfrom langchain.tools import GooglePlacesTool\\n```\\n\\n### Google Scholar\\n\\nWe need to install a python package.\\n\\n```bash\\npip install google-search-results\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_scholar).\\n\\n```python\\nfrom langchain_community.tools.google_scholar import GoogleScholarQueryRun\\nfrom langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper\\n```\\n\\n### Google Search'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='- Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)\\n- Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables\\n`GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively.\\n\\n```python\\nfrom langchain_google_community import GoogleSearchAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/google_search).\\n\\nWe can easily load this wrapper as a Tool (to use with an Agent). We can do this with:\\n\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-search\"])\\n```\\n\\n#### GoogleSearchResults\\n\\nTool that queries the `Google Search` API (via `GoogleSearchAPIWrapper`) and gets back JSON.\\n\\n```python\\nfrom langchain_community.tools import GoogleSearchResults\\n```\\n\\n#### GoogleSearchRun\\n\\nTool that queries the `Google Search` API (via `GoogleSearchAPIWrapper`).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.tools import GoogleSearchRun\\n```\\n\\n\\n### Google Trends\\n\\nWe need to install a python package.\\n\\n```bash\\npip install google-search-results\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_trends).\\n\\n```python\\nfrom langchain_community.tools.google_trends import GoogleTrendsQueryRun\\nfrom langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper\\n```\\n\\n## Toolkits\\n\\n### GMail\\n\\n> [Google Gmail](https://en.wikipedia.org/wiki/Gmail) is a free email service provided by Google.\\nThis toolkit works with emails through the `Gmail API`.\\n\\nWe need to install `langchain-google-community` with required dependencies:\\n\\n```bash\\npip install langchain-google-community[gmail]\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/gmail).\\n\\n```python\\nfrom langchain_google_community import GmailToolkit\\n```\\n\\n#### GMail individual tools\\n\\nYou can use individual tools from GMail Toolkit.'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_google_community.gmail.create_draft import GmailCreateDraft\\nfrom langchain_google_community.gmail.get_message import GmailGetMessage\\nfrom langchain_google_community.gmail.get_thread import GmailGetThread\\nfrom langchain_google_community.gmail.search import GmailSearch\\nfrom langchain_google_community.gmail.send_message import GmailSendMessage\\n```\\n\\n## Memory\\n\\n### AlloyDB for PostgreSQL\\n\\n> [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb) is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-alloydb-pg\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_alloydb).\\n\\n```python\\nfrom langchain_google_alloydb_pg import AlloyDBEngine, AlloyDBChatMessageHistory\\n```\\n\\n### Cloud SQL for PostgreSQL'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Cloud SQL for PostgreSQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-pg\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_sql_pg).\\n\\n\\n```python\\nfrom langchain_google_cloud_sql_pg import PostgresEngine, PostgresChatMessageHistory\\n```\\n\\n### Cloud SQL for MySQL\\n\\n> [Cloud SQL for MySQL](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-mysql\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_sql_mysql).\\n\\n```python\\nfrom langchain_google_cloud_sql_mysql import MySQLEngine, MySQLChatMessageHistory\\n```\\n\\n### Cloud SQL for SQL Server'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Cloud SQL for SQL Server](https://cloud.google.com/sql) is a fully-managed database service that helps you set up, maintain, manage, and administer your SQL Server databases on Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-cloud-sql-mssql\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_sql_mssql).\\n\\n```python\\nfrom langchain_google_cloud_sql_mssql import MSSQLEngine, MSSQLChatMessageHistory\\n```\\n\\n### Spanner\\n\\n> [Google Cloud Spanner](https://cloud.google.com/spanner/docs) is a fully managed, mission-critical, relational database service on Google Cloud that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-spanner\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_spanner).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content=\"```python\\nfrom langchain_google_spanner import SpannerChatMessageHistory\\n```\\n\\n### Memorystore for Redis\\n\\n> [Google Cloud Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis) is a fully managed Redis service for Google Cloud. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-memorystore-redis\\n```\\n\\nSee [usage example](/docs/integrations/document_loaders/google_memorystore_redis).\\n\\n```python\\nfrom langchain_google_memorystore_redis import MemorystoreChatMessageHistory\\n```\\n\\n### Bigtable\\n\\n> [Google Cloud Bigtable](https://cloud.google.com/bigtable/docs) is Google's fully managed NoSQL Big Data database service in Google Cloud.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-bigtable\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See [usage example](/docs/integrations/memory/google_bigtable).\\n\\n```python\\nfrom langchain_google_bigtable import BigtableChatMessageHistory\\n```\\n\\n### Firestore (Native Mode)\\n\\n> [Google Cloud Firestore](https://cloud.google.com/firestore/docs/) is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-firestore\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_firestore).\\n\\n```python\\nfrom langchain_google_firestore import FirestoreChatMessageHistory\\n```\\n\\n### Firestore (Datastore Mode)\\n\\n> [Google Cloud Firestore in Datastore mode](https://cloud.google.com/datastore/docs) is a NoSQL document database built for automatic scaling, high performance, and ease of application development.\\n> Firestore is the newest version of Datastore and introduces several improvements over Datastore.\\n\\nInstall the python package:\\n\\n```bash\\npip install langchain-google-datastore\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='See [usage example](/docs/integrations/memory/google_firestore_datastore).\\n\\n```python\\nfrom langchain_google_datastore import DatastoreChatMessageHistory\\n```\\n\\n### El Carro: The Oracle Operator for Kubernetes\\n\\n> Google [El Carro Oracle Operator for Kubernetes](https://github.com/GoogleCloudPlatform/elcarro-oracle-operator)\\noffers a way to run `Oracle` databases in `Kubernetes` as a portable, open source,\\ncommunity driven, no vendor lock-in container orchestration system.\\n\\n```bash\\npip install langchain-google-el-carro\\n```\\n\\nSee [usage example](/docs/integrations/memory/google_el_carro).\\n\\n```python\\nfrom langchain_google_el_carro import ElCarroChatMessageHistory\\n```\\n\\n## Callbacks\\n\\n### Vertex AI callback handler\\n\\n>Callback Handler that tracks `VertexAI` info.\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n```python\\nfrom langchain_google_vertexai.callbacks import VertexAICallbackHandler\\n```\\n\\n## Chat Loaders\\n\\n### GMail'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='> [Gmail](https://en.wikipedia.org/wiki/Gmail) is a free email service provided by Google.\\nThis loader works with emails through the `Gmail API`.\\n\\nWe need to install `langchain-google-community` with underlying dependencies.\\n\\n```bash\\npip install langchain-google-community[gmail]\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/chat_loaders/gmail).\\n\\n```python\\nfrom langchain_google_community import GMailLoader\\n```\\n\\n## Evaluators\\n\\nWe need to install `langchain-google-vertexai` python package.\\n\\n```bash\\npip install langchain-google-vertexai\\n```\\n\\n### VertexPairWiseStringEvaluator\\n\\n>Pair-wise evaluation of the perplexity of a predicted string.\\n\\n```python\\nfrom langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator\\n```\\n\\n### VertexStringEvaluator\\n\\n>Evaluate the perplexity of a predicted string.\\n\\n```python\\nfrom langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator\\n```\\n\\n## 3rd Party Integrations\\n\\n### SearchApi'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='>[SearchApi](https://www.searchapi.io/) provides a 3rd-party API to access Google search results, YouTube search & transcripts, and other Google-related engines.\\n\\nSee [usage examples and authorization instructions](/docs/integrations/tools/searchapi).\\n\\n```python\\nfrom langchain_community.utilities import SearchApiAPIWrapper\\n```\\n\\n### SerpApi\\n\\n>[SerpApi](https://serpapi.com/) provides a 3rd-party API to access Google search results.\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/serpapi).\\n\\n```python\\nfrom langchain_community.utilities import SerpAPIWrapper\\n```\\n\\n### Serper.dev\\n\\nSee a [usage example and authorization instructions](/docs/integrations/tools/google_serper).\\n\\n```python\\nfrom langchain_community.utilities import GoogleSerperAPIWrapper\\n```\\n\\n### YouTube'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='>[YouTube Search](https://github.com/joetats/youtube_search) package searches `YouTube` videos avoiding using their heavily rate-limited API.\\n>\\n>It uses the form on the YouTube homepage and scrapes the resulting page.\\n\\nWe need to install a python package.\\n\\n```bash\\npip install youtube_search\\n```\\n\\nSee a [usage example](/docs/integrations/tools/youtube).\\n\\n```python\\nfrom langchain.tools import YouTubeSearchTool\\n```\\n\\n### YouTube audio\\n\\n>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform created by `Google`.\\n\\nUse `YoutubeAudioLoader` to fetch / download the audio files.\\n\\nThen, use `OpenAIWhisperParser` to transcribe them to text.\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install yt_dlp pydub librosa\\n```\\n\\nSee a [usage example and authorization instructions](/docs/integrations/document_loaders/youtube_audio).'), Document(metadata={'source': 'docs/docs/integrations/providers/google.mdx', 'file_path': 'docs/docs/integrations/providers/google.mdx', 'file_name': 'google.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\\nfrom langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal\\n```\\n\\n### YouTube transcripts\\n\\n>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform created by `Google`.\\n\\nWe need to install `youtube-transcript-api` python package.\\n\\n```bash\\npip install youtube-transcript-api\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/youtube_transcript).\\n\\n```python\\nfrom langchain_community.document_loaders import YoutubeLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/google_serper.mdx', 'file_path': 'docs/docs/integrations/providers/google_serper.mdx', 'file_name': 'google_serper.mdx', 'file_type': '.mdx'}, page_content='# Serper - Google Search API\\n\\nThis page covers how to use the [Serper](https://serper.dev) Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search. \\nIt is broken into two parts: setup, and then references to the specific Google Serper wrapper.\\n\\n## Setup\\n\\n- Go to [serper.dev](https://serper.dev) to sign up for a free account\\n- Get the api key and set it as an environment variable (`SERPER_API_KEY`)\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a GoogleSerperAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities import GoogleSerperAPIWrapper\\n```\\n\\nYou can use it as part of a Self Ask chain:\\n\\n```python\\nfrom langchain_community.utilities import GoogleSerperAPIWrapper\\nfrom langchain_openai import OpenAI\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\n\\nimport os'), Document(metadata={'source': 'docs/docs/integrations/providers/google_serper.mdx', 'file_path': 'docs/docs/integrations/providers/google_serper.mdx', 'file_name': 'google_serper.mdx', 'file_type': '.mdx'}, page_content='os.environ[\"SERPER_API_KEY\"] = \"\"\\nos.environ[\\'OPENAI_API_KEY\\'] = \"\"\\n\\nllm = OpenAI(temperature=0)\\nsearch = GoogleSerperAPIWrapper()\\ntools = [\\n    Tool(\\n        name=\"Intermediate Answer\",\\n        func=search.run,\\n        description=\"useful for when you need to ask with search\"\\n    )\\n]\\n\\nself_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\\nself_ask_with_search.run(\"What is the hometown of the reigning men\\'s U.S. Open champion?\")\\n```\\n\\n#### Output\\n```\\nEntering new AgentExecutor chain...\\n Yes.\\nFollow up: Who is the reigning men\\'s U.S. Open champion?\\nIntermediate answer: Current champions Carlos Alcaraz, 2022 men\\'s singles champion.\\nFollow up: Where is Carlos Alcaraz from?\\nIntermediate answer: El Palmar, Spain\\nSo the final answer is: El Palmar, Spain\\n\\n> Finished chain.\\n\\n\\'El Palmar, Spain\\'\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/google_serper).\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/google_serper.mdx', 'file_path': 'docs/docs/integrations/providers/google_serper.mdx', 'file_name': 'google_serper.mdx', 'file_type': '.mdx'}, page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-serper\"])\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/gooseai.mdx', 'file_path': 'docs/docs/integrations/providers/gooseai.mdx', 'file_name': 'gooseai.mdx', 'file_type': '.mdx'}, page_content='# GooseAI\\n\\n>[GooseAI](https://goose.ai) makes deploying NLP services easier and more accessible. \\n> `GooseAI` is a fully managed inference service delivered via API. \\n> With feature parity to other well known APIs, `GooseAI` delivers a plug-and-play solution \\n> for serving open source language models at the industry\\'s best economics by simply \\n> changing 2 lines in your code.\\n\\n## Installation and Setup\\n\\n- Install the Python SDK with `pip install openai`\\n- Get your GooseAI api key from this link [here](https://goose.ai/).\\n- Set the environment variable (`GOOSEAI_API_KEY`).\\n\\n```python\\nimport os\\nos.environ[\"GOOSEAI_API_KEY\"] = \"YOUR_API_KEY\"\\n```\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/gooseai).\\n\\n```python\\nfrom langchain_community.llms import GooseAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/gpt4all.mdx', 'file_path': 'docs/docs/integrations/providers/gpt4all.mdx', 'file_name': 'gpt4all.mdx', 'file_type': '.mdx'}, page_content='# GPT4All\\n\\nThis page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.\\n\\n## Installation and Setup\\n\\n- Install the Python package with `pip install gpt4all`\\n- Download a [GPT4All model](https://gpt4all.io/index.html) and place it in your desired directory\\n\\nIn this example, we are using `mistral-7b-openorca.Q4_0.gguf`:\\n\\n```bash\\nmkdir models\\nwget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/mistral-7b-openorca.Q4_0.gguf\\n```\\n\\n## Usage\\n\\n### GPT4All\\n\\nTo use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model\\'s configuration.\\n\\n```python\\nfrom langchain_community.llms import GPT4All\\n\\n# Instantiate the model. Callbacks support token-wise streaming\\nmodel = GPT4All(model=\"./models/mistral-7b-openorca.Q4_0.gguf\", n_threads=8)\\n\\n# Generate text\\nresponse = model.invoke(\"Once upon a time, \")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/gpt4all.mdx', 'file_path': 'docs/docs/integrations/providers/gpt4all.mdx', 'file_name': 'gpt4all.mdx', 'file_type': '.mdx'}, page_content='You can also customize the generation parameters, such as `n_predict`, `temp`, `top_p`, `top_k`, and others.\\n\\nTo stream the model\\'s predictions, add in a CallbackManager.\\n\\n```python\\nfrom langchain_community.llms import GPT4All\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n\\n# There are many CallbackHandlers supported, such as\\n# from langchain.callbacks.streamlit import StreamlitCallbackHandler\\n\\ncallbacks = [StreamingStdOutCallbackHandler()]\\nmodel = GPT4All(model=\"./models/mistral-7b-openorca.Q4_0.gguf\", n_threads=8)\\n\\n# Generate text. Tokens are streamed through the callback manager.\\nmodel.invoke(\"Once upon a time, \", callbacks=callbacks)\\n```\\n\\n## Model File\\n\\nYou can download model files from the GPT4All client. You can download the client from the [GPT4All](https://gpt4all.io/index.html) website.\\n\\nFor a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/gpt4all)'), Document(metadata={'source': 'docs/docs/integrations/providers/gradient.mdx', 'file_path': 'docs/docs/integrations/providers/gradient.mdx', 'file_name': 'gradient.mdx', 'file_type': '.mdx'}, page_content='# Gradient\\n\\n>[Gradient](https://gradient.ai/) allows to fine tune and get completions on LLMs with a simple web API.\\n\\n## Installation and Setup\\n- Install the Python SDK :\\n```bash\\npip install gradientai\\n```\\nGet a [Gradient access token and workspace](https://gradient.ai/) and set it as an environment variable (`Gradient_ACCESS_TOKEN`) and (`GRADIENT_WORKSPACE_ID`)\\n\\n## LLM\\n\\nThere exists an Gradient LLM wrapper, which you can access with \\nSee a [usage example](/docs/integrations/llms/gradient).\\n\\n```python\\nfrom langchain_community.llms import GradientLLM\\n```\\n\\n## Text Embedding Model\\n\\nThere exists an Gradient Embedding model, which you can access with \\n```python\\nfrom langchain_community.embeddings import GradientEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/gradient)'), Document(metadata={'source': 'docs/docs/integrations/providers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/providers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='# Graph RAG\\n\\n## Overview\\n\\n[Graph RAG](https://datastax.github.io/graph-rag/) provides a retriever interface\\nthat combines **unstructured** similarity search on vectors with **structured**\\ntraversal of metadata properties. This enables graph-based retrieval over **existing**\\nvector stores.\\n\\n## Installation and setup\\n\\n```bash\\npip install langchain-graph-retriever\\n```\\n\\n## Retrievers\\n\\n```python\\nfrom langchain_graph_retriever import GraphRetriever\\n```\\n\\nFor more information, see the [Graph RAG Integration Guide](/docs/integrations/retrievers/graph_rag).'), Document(metadata={'source': 'docs/docs/integrations/providers/graphsignal.mdx', 'file_path': 'docs/docs/integrations/providers/graphsignal.mdx', 'file_name': 'graphsignal.mdx', 'file_type': '.mdx'}, page_content=\"# Graphsignal\\n\\nThis page covers how to use [Graphsignal](https://app.graphsignal.com) to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more.\\n\\n## Installation and Setup\\n\\n- Install the Python library with `pip install graphsignal`\\n- Create free Graphsignal account [here](https://graphsignal.com)\\n- Get an API key and set it as an environment variable (`GRAPHSIGNAL_API_KEY`)\\n\\n## Tracing and Monitoring\\n\\nGraphsignal automatically instruments and starts tracing and monitoring chains. Traces and metrics are then available in your [Graphsignal dashboards](https://app.graphsignal.com).\\n\\nInitialize the tracer by providing a deployment name:\\n\\n```python\\nimport graphsignal\\n\\ngraphsignal.configure(deployment='my-langchain-app-prod')\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/graphsignal.mdx', 'file_path': 'docs/docs/integrations/providers/graphsignal.mdx', 'file_name': 'graphsignal.mdx', 'file_type': '.mdx'}, page_content='To additionally trace any function or code, you can use a decorator or a context manager:\\n\\n```python\\n@graphsignal.trace_function\\ndef handle_request():    \\n    chain.run(\"some initial text\")\\n```\\n\\n```python\\nwith graphsignal.start_trace(\\'my-chain\\'):\\n    chain.run(\"some initial text\")\\n```\\n\\nOptionally, enable profiling to record function-level statistics for each trace.\\n\\n```python\\nwith graphsignal.start_trace(\\n        \\'my-chain\\', options=graphsignal.TraceOptions(enable_profiling=True)):\\n    chain.run(\"some initial text\")\\n```\\n\\nSee the [Quick Start](https://graphsignal.com/docs/guides/quick-start/) guide for complete setup instructions.'), Document(metadata={'source': 'docs/docs/integrations/providers/grobid.mdx', 'file_path': 'docs/docs/integrations/providers/grobid.mdx', 'file_name': 'grobid.mdx', 'file_type': '.mdx'}, page_content=\"# Grobid\\n\\nGROBID is a machine learning library for extracting, parsing, and re-structuring raw documents.\\n\\nIt is designed and expected to be used to parse academic papers, where it works particularly well.\\n\\n*Note*: if the articles supplied to Grobid are large documents (e.g. dissertations) exceeding a certain number\\nof elements, they might not be processed.\\n\\nThis page covers how to use the Grobid to parse articles for LangChain.\\n\\n## Installation\\nThe grobid installation is described in details in https://grobid.readthedocs.io/en/latest/Install-Grobid/.\\nHowever, it is probably easier and less troublesome to run grobid through a docker container,\\nas documented [here](https://grobid.readthedocs.io/en/latest/Grobid-docker/).\\n\\n## Use Grobid with LangChain\\n\\nOnce grobid is installed and up and running (you can check by accessing it http://localhost:8070),\\nyou're ready to go.\"), Document(metadata={'source': 'docs/docs/integrations/providers/grobid.mdx', 'file_path': 'docs/docs/integrations/providers/grobid.mdx', 'file_name': 'grobid.mdx', 'file_type': '.mdx'}, page_content='You can now use the GrobidParser to produce documents\\n```python\\nfrom langchain_community.document_loaders.parsers import GrobidParser\\nfrom langchain_community.document_loaders.generic import GenericLoader\\n\\n#Produce chunks from article paragraphs\\nloader = GenericLoader.from_filesystem(\\n    \"/Users/31treehaus/Desktop/Papers/\",\\n    glob=\"*\",\\n    suffixes=[\".pdf\"],\\n    parser= GrobidParser(segment_sentences=False)\\n)\\ndocs = loader.load()\\n\\n#Produce chunks from article sentences\\nloader = GenericLoader.from_filesystem(\\n    \"/Users/31treehaus/Desktop/Papers/\",\\n    glob=\"*\",\\n    suffixes=[\".pdf\"],\\n    parser= GrobidParser(segment_sentences=True)\\n)\\ndocs = loader.load()\\n```\\nChunk metadata will include Bounding Boxes. Although these are a bit funky to parse,\\nthey are explained in https://grobid.readthedocs.io/en/latest/Coordinates-in-PDF/'), Document(metadata={'source': 'docs/docs/integrations/providers/groq.mdx', 'file_path': 'docs/docs/integrations/providers/groq.mdx', 'file_name': 'groq.mdx', 'file_type': '.mdx'}, page_content=\"# Groq\\n\\n>[Groq](https://groq.com)developed the world's first Language Processing Unit™, or `LPU`. \\n> The `Groq LPU` has a deterministic, single core streaming architecture that sets the standard \\n> for GenAI inference speed with predictable and repeatable performance for any given workload.\\n>\\n>Beyond the architecture, `Groq` software is designed to empower developers like you with \\n> the tools you need to create innovative, powerful AI applications. \\n> \\n>With Groq as your engine, you can:\\n>* Achieve uncompromised low latency and performance for real-time AI and HPC inferences 🔥\\n>* Know the exact performance and compute time for any given workload 🔮\\n>* Take advantage of our cutting-edge technology to stay ahead of the competition 💪\\n\\n\\n## Installation and Setup\\n\\nInstall the integration package:\\n\\n```bash\\npip install langchain-groq\\n```\\n\\nRequest an [API key](https://console.groq.com/login?utm_source=langchain&utm_content=provider_page) and set it as an environment variable:\"), Document(metadata={'source': 'docs/docs/integrations/providers/groq.mdx', 'file_path': 'docs/docs/integrations/providers/groq.mdx', 'file_name': 'groq.mdx', 'file_type': '.mdx'}, page_content='```bash\\nexport GROQ_API_KEY=gsk_...\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/groq).\\n\\n```python\\nfrom langchain_groq import ChatGroq\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/gutenberg.mdx', 'file_path': 'docs/docs/integrations/providers/gutenberg.mdx', 'file_name': 'gutenberg.mdx', 'file_type': '.mdx'}, page_content=\"# Gutenberg\\n\\n>[Project Gutenberg](https://www.gutenberg.org/about/) is an online library of free eBooks.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/gutenberg).\\n\\n```python\\nfrom langchain_community.document_loaders import GutenbergLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/hacker_news.mdx', 'file_path': 'docs/docs/integrations/providers/hacker_news.mdx', 'file_name': 'hacker_news.mdx', 'file_type': '.mdx'}, page_content='# Hacker News\\n\\n>[Hacker News](https://en.wikipedia.org/wiki/Hacker_News) (sometimes abbreviated as `HN`) is a social news \\n> website focusing on computer science and entrepreneurship. It is run by the investment fund and startup \\n> incubator `Y Combinator`. In general, content that can be submitted is defined as \"anything that gratifies \\n> one\\'s intellectual curiosity.\"\\n\\n## Installation and Setup\\n\\nThere isn\\'t any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/hacker_news).\\n\\n```python\\nfrom langchain_community.document_loaders import HNLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/hazy_research.mdx', 'file_path': 'docs/docs/integrations/providers/hazy_research.mdx', 'file_name': 'hazy_research.mdx', 'file_type': '.mdx'}, page_content=\"# Hazy Research\\n\\nThis page covers how to use the Hazy Research ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers.\\n\\n## Installation and Setup\\n- To use the `manifest`, install it with `pip install manifest-ml`\\n\\n## Wrappers\\n\\n### LLM\\n\\nThere exists an LLM wrapper around Hazy Research's `manifest` library. \\n`manifest` is a python library which is itself a wrapper around many model providers, and adds in caching, history, and more.\\n\\nTo use this wrapper:\\n```python\\nfrom langchain_community.llms.manifest import ManifestWrapper\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/helicone.mdx', 'file_path': 'docs/docs/integrations/providers/helicone.mdx', 'file_name': 'helicone.mdx', 'file_type': '.mdx'}, page_content='# Helicone\\n\\nThis page covers how to use the [Helicone](https://helicone.ai) ecosystem within LangChain.\\n\\n## What is Helicone?\\n\\nHelicone is an [open-source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\\n\\n![Screenshot of the Helicone dashboard showing average requests per day, response time, tokens per response, total cost, and a graph of requests over time.](/img/HeliconeDashboard.png \"Helicone Dashboard\")\\n\\n## Quick start\\n\\nWith your LangChain environment you can just add the following parameter.\\n\\n```bash\\nexport OPENAI_API_BASE=\"https://oai.hconeai.com/v1\"\\n```\\n\\nNow head over to [helicone.ai](https://www.helicone.ai/signup) to create your account, and add your OpenAI API key within our dashboard to view your logs.\\n\\n![Interface for entering and managing OpenAI API keys in the Helicone dashboard.](/img/HeliconeKeys.png \"Helicone API Key Input\")\\n\\n## How to enable Helicone caching'), Document(metadata={'source': 'docs/docs/integrations/providers/helicone.mdx', 'file_path': 'docs/docs/integrations/providers/helicone.mdx', 'file_name': 'helicone.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_openai import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\n\\nllm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"})\\ntext = \"What is a helicone?\"\\nprint(llm.invoke(text))\\n```\\n\\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\\n\\n## How to use Helicone custom properties\\n\\n```python\\nfrom langchain_openai import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"\\n\\nllm = OpenAI(temperature=0.9, headers={\\n        \"Helicone-Property-Session\": \"24\",\\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\\n        \"Helicone-Property-App\": \"mobile\",\\n      })\\ntext = \"What is a helicone?\"\\nprint(llm.invoke(text))\\n```\\n\\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)'), Document(metadata={'source': 'docs/docs/integrations/providers/hologres.mdx', 'file_path': 'docs/docs/integrations/providers/hologres.mdx', 'file_name': 'hologres.mdx', 'file_type': '.mdx'}, page_content='# Hologres\\n\\n>[Hologres](https://www.alibabacloud.com/help/en/hologres/latest/introduction) is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time. \\n>`Hologres` supports standard `SQL` syntax, is compatible with `PostgreSQL`, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services.'), Document(metadata={'source': 'docs/docs/integrations/providers/hologres.mdx', 'file_path': 'docs/docs/integrations/providers/hologres.mdx', 'file_name': 'hologres.mdx', 'file_type': '.mdx'}, page_content='>`Hologres` provides **vector database** functionality by adopting [Proxima](https://www.alibabacloud.com/help/en/hologres/latest/vector-processing).\\n>`Proxima` is a high-performance software library developed by `Alibaba DAMO Academy`. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open-source software such as Faiss. Proxima allows you to search for similar text or image embeddings with high throughput and low latency. Hologres is deeply integrated with Proxima to provide a high-performance vector search service.\\n\\n## Installation and Setup\\n\\nClick [here](https://www.alibabacloud.com/zh/product/hologres) to fast deploy a Hologres cloud instance.\\n\\n```bash\\npip install hologres-vector\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/hologres).\\n\\n```python\\nfrom langchain_community.vectorstores import Hologres\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/html2text.mdx', 'file_path': 'docs/docs/integrations/providers/html2text.mdx', 'file_name': 'html2text.mdx', 'file_type': '.mdx'}, page_content='# HTML to text\\n\\n>[html2text](https://github.com/Alir3z4/html2text/) is a Python package that converts a page of `HTML` into clean, easy-to-read plain `ASCII text`. \\n\\nThe ASCII also happens to be a valid `Markdown` (a text-to-HTML format).\\n\\n## Installation and Setup\\n\\n```bash\\npip install html2text\\n```\\n\\n## Document Transformer\\n\\nSee a [usage example](/docs/integrations/document_transformers/html2text).\\n\\n```python\\nfrom langchain_community.document_loaders import Html2TextTransformer\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/huawei.mdx', 'file_path': 'docs/docs/integrations/providers/huawei.mdx', 'file_name': 'huawei.mdx', 'file_type': '.mdx'}, page_content='# Huawei\\n\\n>[Huawei Technologies Co., Ltd.](https://www.huawei.com/) is a Chinese multinational \\n> digital communications technology corporation.\\n> \\n>[Huawei Cloud](https://www.huaweicloud.com/intl/en-us/product/) provides a comprehensive suite of \\n> global cloud computing services. \\n \\n\\n## Installation and Setup\\n\\nTo access the `Huawei Cloud`, you need an access token.\\n\\nYou also have to install a python library:\\n\\n```bash\\npip install -U esdk-obs-python\\n```\\n\\n\\n## Document Loader\\n\\n### Huawei OBS Directory\\n\\nSee a [usage example](/docs/integrations/document_loaders/huawei_obs_directory).\\n\\n```python\\nfrom langchain_community.document_loaders import OBSDirectoryLoader\\n```\\n\\n### Huawei OBS File\\n\\nSee a [usage example](/docs/integrations/document_loaders/huawei_obs_file).\\n\\n```python\\nfrom langchain_community.document_loaders.obs_file import OBSFileLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='# Hugging Face\\n\\nAll functionality related to the [Hugging Face Platform](https://huggingface.co/).\\n\\n## Installation\\n\\nMost of the Hugging Face integrations are available in the `langchain-huggingface` package.\\n\\n```bash\\npip install langchain-huggingface\\n```\\n\\n## Chat models\\n\\n### ChatHuggingFace\\n\\nWe can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.\\n\\nSee a [usage example](/docs/integrations/chat/huggingface).\\n\\n```python\\nfrom langchain_huggingface import ChatHuggingFace\\n```\\n\\n## LLMs\\n\\n### HuggingFaceEndpoint\\n\\n\\nSee a [usage example](/docs/integrations/llms/huggingface_endpoint).\\n\\n```python\\nfrom langchain_huggingface import HuggingFaceEndpoint\\n```\\n\\n### HuggingFacePipeline\\n\\nHugging Face models can be run locally through the `HuggingFacePipeline` class.\\n\\nSee a [usage example](/docs/integrations/llms/huggingface_pipelines).\\n\\n```python\\nfrom langchain_huggingface import HuggingFacePipeline\\n```\\n\\n## Embedding Models\\n\\n### HuggingFaceEmbeddings'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/text_embedding/huggingfacehub).\\n\\n```python\\nfrom langchain_huggingface import HuggingFaceEmbeddings\\n```\\n\\n### HuggingFaceEndpointEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/huggingfacehub).\\n\\n```python\\nfrom langchain_huggingface import HuggingFaceEndpointEmbeddings\\n```\\n\\n### HuggingFaceInferenceAPIEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/huggingfacehub).\\n\\n```python\\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\\n```\\n\\n### HuggingFaceInstructEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/instruct_embeddings).\\n\\n```python\\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\\n```\\n\\n### HuggingFaceBgeEmbeddings'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='>[BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).\\n>BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.\\n\\nSee a [usage example](/docs/integrations/text_embedding/bge_huggingface).\\n\\n```python\\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\\n```\\n\\n## Document Loaders\\n\\n### Hugging Face dataset'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='>[Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000\\n> [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages\\n> that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\\n> They used for a diverse range of tasks such as translation, automatic speech\\n> recognition, and image classification.\\n\\nWe need to install `datasets` python package.\\n\\n```bash\\npip install datasets\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/hugging_face_dataset).\\n\\n```python\\nfrom langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader\\n```\\n\\n### Hugging Face model loader\\n\\n>Load model information from `Hugging Face Hub`, including README content.\\n>\\n>This loader interfaces with the `Hugging Face Models API` to fetch \\n> and load model metadata and README files. \\n> The API allows you to search and filter models based on \\n> specific criteria such as model tags, authors, and more.'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import HuggingFaceModelLoader\\n```\\n\\n### Image captions\\n\\nIt uses the Hugging Face models to generate image captions.\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install transformers pillow\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/image_captions).\\n\\n```python\\nfrom langchain_community.document_loaders import ImageCaptionLoader\\n```\\n\\n## Tools\\n\\n### Hugging Face Hub Tools\\n\\n>[Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools)\\n> support text I/O and are loaded using the `load_huggingface_tool` function.\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install transformers huggingface_hub\\n```\\n\\nSee a [usage example](/docs/integrations/tools/huggingface_tools).\\n\\n```python\\nfrom langchain_community.agent_toolkits.load_tools import load_huggingface_tool\\n```\\n\\n### Hugging Face Text-to-Speech Model Inference.\\n\\n> It is a wrapper around `OpenAI Text-to-Speech API`.'), Document(metadata={'source': 'docs/docs/integrations/providers/huggingface.mdx', 'file_path': 'docs/docs/integrations/providers/huggingface.mdx', 'file_name': 'huggingface.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_path': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_name': 'hyperbrowser.mdx', 'file_type': '.mdx'}, page_content='# Hyperbrowser\\n\\n> [Hyperbrowser](https://hyperbrowser.ai) is a platform for running and scaling headless browsers. It lets you launch and manage browser sessions at scale and provides easy to use solutions for any webscraping needs, such as scraping a single page or crawling an entire site.\\n>\\n> Key Features:\\n>\\n> - Instant Scalability - Spin up hundreds of browser sessions in seconds without infrastructure headaches\\n> - Simple Integration - Works seamlessly with popular tools like Puppeteer and Playwright\\n> - Powerful APIs - Easy to use APIs for scraping/crawling any site, and much more\\n> - Bypass Anti-Bot Measures - Built-in stealth mode, ad blocking, automatic CAPTCHA solving, and rotating proxies\\n\\nFor more information about Hyperbrowser, please visit the [Hyperbrowser website](https://hyperbrowser.ai) or if you want to check out the docs, you can visit the [Hyperbrowser docs](https://docs.hyperbrowser.ai).\\n\\n## Installation and Setup'), Document(metadata={'source': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_path': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_name': 'hyperbrowser.mdx', 'file_type': '.mdx'}, page_content='To get started with `langchain-hyperbrowser`, you can install the package using pip:\\n\\n```bash\\npip install langchain-hyperbrowser\\n```\\n\\nAnd you should configure credentials by setting the following environment variables:\\n\\n`HYPERBROWSER_API_KEY=<your-api-key>`\\n\\nMake sure to get your API Key from https://app.hyperbrowser.ai/\\n\\n## Document Loader\\n\\nThe `HyperbrowserLoader` class in `langchain-hyperbrowser` can easily be used to load content from any single page or multiple pages as well as crawl an entire site.\\nThe content can be loaded as markdown or html.\\n\\n```python\\nfrom langchain_hyperbrowser import HyperbrowserLoader\\n\\nloader = HyperbrowserLoader(urls=\"https://example.com\")\\ndocs = loader.load()\\n\\nprint(docs[0])\\n```\\n\\n## Advanced Usage'), Document(metadata={'source': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_path': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_name': 'hyperbrowser.mdx', 'file_type': '.mdx'}, page_content='You can specify the operation to be performed by the loader. The default operation is `scrape`. For `scrape`, you can provide a single URL or a list of URLs to be scraped. For `crawl`, you can only provide a single URL. The `crawl` operation will crawl the provided page and subpages and return a document for each page.\\n\\n```python\\nloader = HyperbrowserLoader(\\n  urls=\"https://hyperbrowser.ai\", api_key=\"YOUR_API_KEY\", operation=\"crawl\"\\n)\\n```\\n\\nOptional params for the loader can also be provided in the `params` argument. For more information on the supported params, visit https://docs.hyperbrowser.ai/reference/sdks/python/scrape#start-scrape-job-and-wait or https://docs.hyperbrowser.ai/reference/sdks/python/crawl#start-crawl-job-and-wait.\\n\\n```python\\nloader = HyperbrowserLoader(\\n  urls=\"https://example.com\",\\n  api_key=\"YOUR_API_KEY\",\\n  operation=\"scrape\",\\n  params={\"scrape_options\": {\"include_tags\": [\"h1\", \"h2\", \"p\"]}}\\n)\\n```\\n\\n## Additional Resources'), Document(metadata={'source': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_path': 'docs/docs/integrations/providers/hyperbrowser.mdx', 'file_name': 'hyperbrowser.mdx', 'file_type': '.mdx'}, page_content='- [Hyperbrowser Docs](https://docs.hyperbrowser.ai/)\\n- [GitHub](https://github.com/hyperbrowserai/langchain-hyperbrowser/)\\n- [PyPi](https://pypi.org/project/langchain-hyperbrowser/)'), Document(metadata={'source': 'docs/docs/integrations/providers/ibm.mdx', 'file_path': 'docs/docs/integrations/providers/ibm.mdx', 'file_name': 'ibm.mdx', 'file_type': '.mdx'}, page_content='# IBM\\n\\nThe `LangChain` integrations related to [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) platform.\\n\\nIBM® watsonx.ai™ AI studio is part of the IBM [watsonx](https://www.ibm.com/watsonx)™ AI and data platform, bringing together new generative \\nAI capabilities powered by [foundation models](https://www.ibm.com/products/watsonx-ai/foundation-models) and traditional machine learning (ML) \\ninto a powerful studio spanning the AI lifecycle. Tune and guide models with your enterprise data to meet your needs with easy-to-use tools for \\nbuilding and refining performant prompts. With watsonx.ai, you can build AI applications in a fraction of the time and with a fraction of the data. \\nWatsonx.ai offers:'), Document(metadata={'source': 'docs/docs/integrations/providers/ibm.mdx', 'file_path': 'docs/docs/integrations/providers/ibm.mdx', 'file_name': 'ibm.mdx', 'file_type': '.mdx'}, page_content='- **Multi-model variety and flexibility:** Choose from IBM-developed, open-source and third-party models, or build your own model.\\n- **Differentiated client protection:** IBM stands behind IBM-developed models and indemnifies the client against third-party IP claims.\\n- **End-to-end AI governance:** Enterprises can scale and accelerate the impact of AI with trusted data across the business, using data wherever it resides.\\n- **Hybrid, multi-cloud deployments:** IBM provides the flexibility to integrate and deploy your AI workloads into your hybrid-cloud stack of choice.\\n\\n\\n## Installation and Setup\\n\\nInstall the integration package with\\n```bash\\npip install -qU langchain-ibm\\n```\\n\\nGet an IBM watsonx.ai api key and set it as an environment variable (`WATSONX_APIKEY`)\\n```python\\nimport os\\n\\nos.environ[\"WATSONX_APIKEY\"] = \"your IBM watsonx.ai api key\"\\n```\\n\\n## Chat Model\\n\\n### ChatWatsonx\\n\\nSee a [usage example](/docs/integrations/chat/ibm_watsonx).'), Document(metadata={'source': 'docs/docs/integrations/providers/ibm.mdx', 'file_path': 'docs/docs/integrations/providers/ibm.mdx', 'file_name': 'ibm.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_ibm import ChatWatsonx\\n```\\n\\n## LLMs\\n\\n### WatsonxLLM\\n\\nSee a [usage example](/docs/integrations/llms/ibm_watsonx).\\n\\n```python\\nfrom langchain_ibm import WatsonxLLM\\n```\\n\\n## Embedding Models\\n\\n### WatsonxEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/ibm_watsonx).\\n\\n```python\\nfrom langchain_ibm import WatsonxEmbeddings\\n```\\n\\n## Reranker\\n\\n### WatsonxRerank\\n\\nSee a [usage example](/docs/integrations/retrievers/ibm_watsonx_ranker).\\n\\n```python\\nfrom langchain_ibm import WatsonxRerank\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ieit_systems.mdx', 'file_path': 'docs/docs/integrations/providers/ieit_systems.mdx', 'file_name': 'ieit_systems.mdx', 'file_type': '.mdx'}, page_content='# IEIT Systems\\n\\n>[IEIT Systems](https://en.ieisystem.com/) is a Chinese information technology company \\n> established in 1999. It provides the IT infrastructure products, solutions, \\n> and services, innovative IT products and solutions across cloud computing, \\n> big data, and artificial intelligence.\\n\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/yuan2).\\n\\n```python\\nfrom langchain_community.llms.yuan2 import Yuan2\\n```\\n\\n## Chat models\\n\\nSee the [installation instructions](/docs/integrations/chat/yuan2/#setting-up-your-api-server).\\n\\nYuan2.0 provided an OpenAI compatible API, and ChatYuan2 is integrated into langchain by using `OpenAI client`. \\nTherefore, ensure the `openai` package is installed.\\n\\n```bash\\npip install openai\\n```\\nSee a [usage example](/docs/integrations/chat/yuan2).\\n\\n```python\\nfrom langchain_community.chat_models import ChatYuan2\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ifixit.mdx', 'file_path': 'docs/docs/integrations/providers/ifixit.mdx', 'file_name': 'ifixit.mdx', 'file_type': '.mdx'}, page_content=\"# iFixit\\n\\n>[iFixit](https://www.ifixit.com) is the largest, open repair community on the web. The site contains nearly 100k \\n> repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under `CC-BY-NC-SA 3.0`.\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/ifixit).\\n\\n```python\\nfrom langchain_community.document_loaders import IFixitLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/iflytek.mdx', 'file_path': 'docs/docs/integrations/providers/iflytek.mdx', 'file_name': 'iflytek.mdx', 'file_type': '.mdx'}, page_content='# iFlytek\\n\\n>[iFlytek](https://www.iflytek.com) is a Chinese information technology company \\n> established in 1999. It creates voice recognition software and \\n> voice-based internet/mobile products covering education, communication, \\n> music, intelligent toys industries.\\n\\n\\n## Installation and Setup\\n\\n- Get `SparkLLM` app_id, api_key and api_secret from [iFlyTek SparkLLM API Console](https://console.xfyun.cn/services/bm3) (for more info, see [iFlyTek SparkLLM Intro](https://xinghuo.xfyun.cn/sparkapi)).\\n- Install the Python package (not for the embedding models):\\n\\n```bash\\npip install websocket-client\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/sparkllm).\\n\\n```python\\nfrom langchain_community.llms import SparkLLM\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/sparkllm).\\n\\n```python\\nfrom langchain_community.chat_models import ChatSparkLLM\\n```\\n\\n## Embedding models\\n\\n```python\\nfrom langchain_community.embeddings import SparkLLMTextEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/imsdb.mdx', 'file_path': 'docs/docs/integrations/providers/imsdb.mdx', 'file_name': 'imsdb.mdx', 'file_type': '.mdx'}, page_content=\"# IMSDb\\n\\n>[IMSDb](https://imsdb.com/) is the `Internet Movie Script Database`.\\n> \\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/imsdb).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import IMSDbLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/infinispanvs.mdx', 'file_path': 'docs/docs/integrations/providers/infinispanvs.mdx', 'file_name': 'infinispanvs.mdx', 'file_type': '.mdx'}, page_content='# Infinispan VS\\n\\n> [Infinispan](https://infinispan.org) Infinispan is an open-source in-memory data grid that provides\\n> a key/value data store able to hold all types of data, from Java objects to plain text.\\n> Since version 15 Infinispan supports vector search over caches.\\n\\n## Installation and Setup\\nSee [Get Started](https://infinispan.org/get-started/) to run an Infinispan server, you may want to disable authentication\\n(not supported atm)\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/infinispanvs).\\n\\n```python\\nfrom langchain_community.vectorstores import InfinispanVS\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/infinity.mdx', 'file_path': 'docs/docs/integrations/providers/infinity.mdx', 'file_name': 'infinity.mdx', 'file_type': '.mdx'}, page_content='# Infinity\\n\\n>[Infinity](https://github.com/michaelfeil/infinity) allows the creation of text embeddings.\\n\\n## Text Embedding Model\\n\\nThere exists an infinity Embedding model, which you can access with \\n```python\\nfrom langchain_community.embeddings import InfinityEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/infinity)'), Document(metadata={'source': 'docs/docs/integrations/providers/infino.mdx', 'file_path': 'docs/docs/integrations/providers/infino.mdx', 'file_name': 'infino.mdx', 'file_type': '.mdx'}, page_content=\"# Infino\\n\\n>[Infino](https://github.com/infinohq/infino) is an open-source observability platform that stores both metrics and application logs together.\\n\\nKey features of `Infino` include:\\n- **Metrics Tracking**: Capture time taken by LLM model to handle request, errors, number of tokens, and costing indication for the particular LLM.\\n- **Data Tracking**: Log and store prompt, request, and response data for each LangChain interaction.\\n- **Graph Visualization**: Generate basic graphs over time, depicting metrics such as request duration, error occurrences, token count, and cost.\\n\\n## Installation and Setup\\n\\nFirst, you'll need to install the  `infinopy` Python package as follows:\\n\\n```bash\\npip install infinopy\\n```\\n\\nIf you already have an `Infino Server` running, then you're good to go; but if\\nyou don't, follow the next steps to start it:\"), Document(metadata={'source': 'docs/docs/integrations/providers/infino.mdx', 'file_path': 'docs/docs/integrations/providers/infino.mdx', 'file_name': 'infino.mdx', 'file_type': '.mdx'}, page_content='- Make sure you have Docker installed\\n- Run the following in your terminal:\\n    ```\\n    docker run --rm --detach --name infino-example -p 3000:3000 infinohq/infino:latest\\n    ```\\n\\n## Using Infino\\n\\nSee a [usage example of `InfinoCallbackHandler`](/docs/integrations/callbacks/infino).\\n\\n```python\\nfrom langchain.callbacks import InfinoCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='# Intel\\n\\n>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) is the interface between the 🤗 Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\\n\\n>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) is an innovative toolkit designed to accelerate GenAI/LLM everywhere with the optimal performance of Transformer-based models on various Intel platforms, including Intel Gaudi2, Intel CPU, and Intel GPU.\\n\\nThis page covers how to use optimum-intel and ITREX with LangChain.\\n\\n## Optimum-intel\\n\\nAll functionality related to the [optimum-intel](https://github.com/huggingface/optimum-intel.git) and [IPEX](https://github.com/intel/intel-extension-for-pytorch).\\n\\n### Installation\\n\\nInstall using optimum-intel and ipex using:'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install optimum[neural-compressor]\\npip install intel_extension_for_pytorch\\n```\\n\\nPlease follow the installation instructions as specified below:\\n\\n* Install optimum-intel as shown [here](https://github.com/huggingface/optimum-intel).\\n* Install IPEX as shown [here](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu).\\n\\n### Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/optimum_intel).\\nWe also offer a full tutorial notebook \"rag_with_quantized_embeddings.ipynb\" for using the embedder in a RAG pipeline in the cookbook dir.\\n\\n```python\\nfrom langchain_community.embeddings import QuantizedBiEncoderEmbeddings\\n```\\n\\n## Intel® Extension for Transformers (ITREX)\\n(ITREX) is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular, effective on 4th Intel Xeon Scalable processor Sapphire Rapids (codenamed Sapphire Rapids).'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='Quantization is a process that involves reducing the precision of these weights by representing them using a smaller number of bits. Weight-only quantization specifically focuses on quantizing the weights of the neural network while keeping other components, such as activations, in their original precision.\\n\\nAs large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computational demands of these modern architectures while maintaining the accuracy. Compared to [normal quantization](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md) like W8A8, weight only quantization is probably a better trade-off to balance the performance and the accuracy, since we will see below that the bottleneck of deploying LLMs is the memory bandwidth and normally weight only quantization could lead to better accuracy.'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='Here, we will introduce Embedding Models and Weight-only quantization for Transformers large language models with ITREX. Weight-only quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks. In the context of deep neural networks, the model parameters, also known as weights, are typically represented using floating-point numbers, which can consume a significant amount of memory and require intensive computational resources.\\n\\nAll functionality related to the [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers).\\n\\n### Installation\\n\\nInstall intel-extension-for-transformers. For system requirements and other installation tips, please refer to [Installation Guide](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)\\n\\n```bash\\npip install intel-extension-for-transformers\\n```\\nInstall other required packages.'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install -U torch onnx accelerate datasets\\n```\\n\\n### Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/itrex).\\n\\n```python\\nfrom langchain_community.embeddings import QuantizedBgeEmbeddings\\n```\\n\\n### Weight-Only Quantization with ITREX\\n\\nSee a [usage example](/docs/integrations/llms/weight_only_quantization).\\n\\n## Detail of Configuration Parameters\\n\\nHere is the detail of the `WeightOnlyQuantConfig` class.'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='#### weight_dtype (string): Weight Data Type, default is \"nf4\".\\nWe support quantize the weights to following data types for storing(weight_dtype in WeightOnlyQuantConfig):\\n* **int8**: Uses 8-bit data type.\\n* **int4_fullrange**: Uses the -8 value of int4 range compared with the normal int4 range [-7,7].\\n* **int4_clip**: Clips and retains the values within the int4 range, setting others to zero.\\n* **nf4**: Uses the normalized float 4-bit data type.\\n* **fp4_e2m1**: Uses regular float 4-bit data type. \"e2\" means that 2 bits are used for the exponent, and \"m1\" means that 1 bits are used for the mantissa.\\n\\n#### compute_dtype (string): Computing Data Type, Default is \"fp32\".\\nWhile these techniques store weights in 4 or 8 bit, the computation still happens in float32, bfloat16 or int8(compute_dtype in WeightOnlyQuantConfig):\\n* **fp32**: Uses the float32 data type to compute.\\n* **bf16**: Uses the bfloat16 data type to compute.\\n* **int8**: Uses 8-bit data type to compute.'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='#### llm_int8_skip_modules (list of module\\'s name): Modules to Skip Quantization, Default is None.\\nIt is a list of modules to be skipped quantization.\\n\\n#### scale_dtype (string): The Scale Data Type, Default is \"fp32\".\\nNow only support \"fp32\"(float32).'), Document(metadata={'source': 'docs/docs/integrations/providers/intel.mdx', 'file_path': 'docs/docs/integrations/providers/intel.mdx', 'file_name': 'intel.mdx', 'file_type': '.mdx'}, page_content='#### mse_range (boolean): Whether to Search for The Best Clip Range from Range [0.805, 1.0, 0.005], default is False.\\n#### use_double_quant (boolean): Whether to Quantize Scale, Default is False.\\nNot support yet.\\n#### double_quant_dtype (string): Reserve for Double Quantization.\\n#### double_quant_scale_dtype (string): Reserve for Double Quantization.\\n#### group_size (int): Group Size When Auantization.\\n#### scheme (string): Which Format Weight Be Quantize to. Default is \"sym\".\\n* **sym**: Symmetric.\\n* **asym**: Asymmetric.\\n#### algorithm (string): Which Algorithm to Improve the Accuracy . Default is \"RTN\"\\n* **RTN**: Round-to-nearest (RTN) is a quantification method that we can think of very intuitively.\\n* **AWQ**: Protecting only 1% of salient weights can greatly reduce quantization error. the salient weight channels are selected by observing the distribution of activation and weight per channel. The salient weights are also quantized after multiplying a big scale factor before quantization for preserving. .\\n* **TEQ**: A trainable equivalent transformation that preserves the FP32 precision in weight-only quantization.'), Document(metadata={'source': 'docs/docs/integrations/providers/iugu.mdx', 'file_path': 'docs/docs/integrations/providers/iugu.mdx', 'file_name': 'iugu.mdx', 'file_type': '.mdx'}, page_content='# Iugu\\n\\n>[Iugu](https://www.iugu.com/) is a Brazilian services and software as a service (SaaS)\\n> company. It offers payment-processing software and application programming \\n> interfaces for e-commerce websites and mobile applications.\\n \\n\\n## Installation and Setup\\n\\nThe `Iugu API` requires an access token, which can be found inside of the `Iugu` dashboard.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/iugu).\\n\\n```python\\nfrom langchain_community.document_loaders import IuguLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/jaguar.mdx', 'file_path': 'docs/docs/integrations/providers/jaguar.mdx', 'file_name': 'jaguar.mdx', 'file_type': '.mdx'}, page_content='# Jaguar\\n\\nThis page describes how to use Jaguar vector database within LangChain.\\nIt contains three sections: introduction, installation and setup, and Jaguar API.\\n\\n\\n## Introduction\\n\\nJaguar vector database has the following characteristics:\\n\\n1. It is a distributed vector database\\n2. The “ZeroMove” feature of JaguarDB enables instant horizontal scalability\\n3. Multimodal: embeddings, text, images, videos, PDFs, audio, time series, and geospatial\\n4. All-masters: allows both parallel reads and writes\\n5. Anomaly detection capabilities\\n6. RAG support: combines LLM with proprietary and real-time data\\n7. Shared metadata: sharing of metadata across multiple vector indexes\\n8. Distance metrics: Euclidean, Cosine, InnerProduct, Manhatten, Chebyshev, Hamming, Jeccard, Minkowski\\n\\n[Overview of Jaguar scalable vector database](http://www.jaguardb.com)\\n\\nYou can run JaguarDB in docker container; or download the software and run on-cloud or off-cloud.\\n\\n## Installation and Setup'), Document(metadata={'source': 'docs/docs/integrations/providers/jaguar.mdx', 'file_path': 'docs/docs/integrations/providers/jaguar.mdx', 'file_name': 'jaguar.mdx', 'file_type': '.mdx'}, page_content='- Install the JaguarDB on one host or multiple hosts\\n- Install the Jaguar HTTP Gateway server on one host\\n- Install the JaguarDB HTTP Client package\\n\\nThe steps are described in [Jaguar Documents](http://www.jaguardb.com/support.html)\\n\\nEnvironment Variables in client programs:\\n\\n    export OPENAI_API_KEY=\"......\"\\n    export JAGUAR_API_KEY=\"......\"\\n\\n  \\n## Jaguar API\\n\\nTogether with LangChain, a Jaguar client class is provided by importing it in Python:\\n\\n```python\\nfrom langchain_community.vectorstores.jaguar import Jaguar\\n```\\n\\nSupported API functions of the Jaguar class are:\\n\\n- `add_texts`\\n- `add_documents`\\n- `from_texts`\\n- `from_documents`\\n- `similarity_search`\\n- `is_anomalous`\\n- `create`\\n- `delete`\\n- `clear`\\n- `drop`\\n- `login`\\n- `logout`\\n\\n\\nFor more details of the Jaguar API, please refer to [this notebook](/docs/integrations/vectorstores/jaguar)'), Document(metadata={'source': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_path': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_name': 'javelin_ai_gateway.mdx', 'file_type': '.mdx'}, page_content=\"# Javelin AI Gateway\\n\\n[The Javelin AI Gateway](https://www.getjavelin.io) service is a high-performance, enterprise grade API Gateway for AI applications.  \\nIt is designed to streamline the usage and access of various large language model (LLM) providers, \\nsuch as OpenAI, Cohere, Anthropic and custom large language models within an organization by incorporating\\nrobust access security for all interactions with LLMs. \\n\\nJavelin offers a high-level interface that simplifies the interaction with LLMs by providing a unified endpoint \\nto handle specific LLM related requests. \\n\\nSee the Javelin AI Gateway [documentation](https://docs.getjavelin.io) for more details.  \\n[Javelin Python SDK](https://www.github.com/getjavelin/javelin-python) is an easy to use client library meant to be embedded into AI Applications\\n\\n## Installation and Setup\\n\\nInstall `javelin_sdk` to interact with Javelin AI Gateway:\\n\\n```sh\\npip install 'javelin_sdk'\\n```\\n\\nSet the Javelin's API key as an environment variable:\"), Document(metadata={'source': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_path': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_name': 'javelin_ai_gateway.mdx', 'file_type': '.mdx'}, page_content='```sh\\nexport JAVELIN_API_KEY=...\\n```\\n\\n## Completions Example\\n\\n```python\\n\\nfrom langchain.chains import LLMChain\\nfrom langchain_community.llms import JavelinAIGateway\\nfrom langchain_core.prompts import PromptTemplate\\n\\nroute_completions = \"eng_dept03\"\\n\\ngateway = JavelinAIGateway(\\n    gateway_uri=\"http://localhost:8000\",\\n    route=route_completions,\\n    model_name=\"text-davinci-003\",\\n)\\n\\nllmchain = LLMChain(llm=gateway, prompt=prompt)\\nresult = llmchain.run(\"podcast player\")\\n\\nprint(result)\\n\\n```\\n\\n## Embeddings Example\\n\\n```python\\nfrom langchain_community.embeddings import JavelinAIGatewayEmbeddings\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = JavelinAIGatewayEmbeddings(\\n    gateway_uri=\"http://localhost:8000\",\\n    route=\"embeddings\",\\n)\\n\\nprint(embeddings.embed_query(\"hello\"))\\nprint(embeddings.embed_documents([\"hello\"]))\\n```\\n\\n## Chat Example\\n```python\\nfrom langchain_community.chat_models import ChatJavelinAIGateway\\nfrom langchain_core.messages import HumanMessage, SystemMessage'), Document(metadata={'source': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_path': 'docs/docs/integrations/providers/javelin_ai_gateway.mdx', 'file_name': 'javelin_ai_gateway.mdx', 'file_type': '.mdx'}, page_content='messages = [\\n    SystemMessage(\\n        content=\"You are a helpful assistant that translates English to French.\"\\n    ),\\n    HumanMessage(\\n        content=\"Artificial Intelligence has the power to transform humanity and make the world a better place\"\\n    ),\\n]\\n\\nchat = ChatJavelinAIGateway(\\n    gateway_uri=\"http://localhost:8000\",\\n    route=\"mychatbot_route\",\\n    model_name=\"gpt-3.5-turbo\"\\n    params={\\n        \"temperature\": 0.1\\n    }\\n)\\n\\nprint(chat(messages))\\n\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/jenkins.mdx', 'file_path': 'docs/docs/integrations/providers/jenkins.mdx', 'file_name': 'jenkins.mdx', 'file_type': '.mdx'}, page_content=\"# Jenkins\\n\\n[Jenkins](https://www.jenkins.io/) is an open-source automation platform that enables\\nsoftware teams to streamline their development workflows. It's widely adopted in the\\nDevOps community as a tool for automating the building, testing, and deployment of\\napplications through CI/CD pipelines.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-jenkins\\n```\\n\\n## Tools\\n\\nSee detail on available tools [here](/docs/integrations/tools/jenkins).\"), Document(metadata={'source': 'docs/docs/integrations/providers/jina.mdx', 'file_path': 'docs/docs/integrations/providers/jina.mdx', 'file_name': 'jina.mdx', 'file_type': '.mdx'}, page_content='# Jina AI\\n\\n>[Jina AI](https://jina.ai/about-us) is a search AI company. `Jina` helps businesses and developers unlock multimodal data with a better search.\\n\\n:::caution\\nFor proper compatibility, please ensure you are using the `openai` SDK at version **0.x**.\\n:::\\n\\n## Installation and Setup\\n- Get a Jina AI API token from [here](https://jina.ai/embeddings/) and set it as an environment variable (`JINA_API_TOKEN`)\\n\\n## Chat Models\\n\\n```python\\nfrom langchain_community.chat_models import JinaChat\\n```\\n\\nSee a [usage examples](/docs/integrations/chat/jinachat).\\n\\n## Embedding Models\\n\\nYou can check the list of available models from [here](https://jina.ai/embeddings/)\\n\\n```python\\nfrom langchain_community.embeddings import JinaEmbeddings\\n```\\n\\nSee a [usage examples](/docs/integrations/text_embedding/jina).\\n\\n## Document Transformers\\n\\n### Jina Rerank\\n\\n```python\\nfrom langchain_community.document_compressors import JinaRerank\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/jina.mdx', 'file_path': 'docs/docs/integrations/providers/jina.mdx', 'file_name': 'jina.mdx', 'file_type': '.mdx'}, page_content='See a [usage examples](/docs/integrations/document_transformers/jina_rerank).'), Document(metadata={'source': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_path': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_name': 'johnsnowlabs.mdx', 'file_type': '.mdx'}, page_content='# Johnsnowlabs\\n\\nGain access to the [johnsnowlabs](https://www.johnsnowlabs.com/) ecosystem of enterprise NLP libraries\\nwith over 21.000 enterprise NLP models in over 200 languages with the open source `johnsnowlabs` library.\\nFor all 24.000+ models, see the [John Snow Labs Model Models Hub](https://nlp.johnsnowlabs.com/models)\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install johnsnowlabs\\n```\\n\\nTo [install enterprise features](https://nlp.johnsnowlabs.com/docs/en/jsl/install_licensed_quick, run:\\n```python\\n# for more details see https://nlp.johnsnowlabs.com/docs/en/jsl/install_licensed_quick\\nnlp.install()\\n```\\n\\n\\nYou can embed your queries and documents with either `gpu`,`cpu`,`apple_silicon`,`aarch` based optimized binaries.\\nBy default cpu binaries are used.\\nOnce a session is started, you must restart your notebook to switch between GPU or CPU, or changes will not take effect.'), Document(metadata={'source': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_path': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_name': 'johnsnowlabs.mdx', 'file_type': '.mdx'}, page_content='## Embed Query with CPU:\\n```python\\ndocument = \"foo bar\"\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\')\\noutput = embedding.embed_query(document)\\n```\\n\\n\\n## Embed Query with GPU:\\n\\n\\n```python\\ndocument = \"foo bar\"\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'gpu\\')\\noutput = embedding.embed_query(document)\\n```\\n\\n\\n## Embed Query with Apple Silicon (M1,M2,etc..):\\n\\n```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'apple_silicon\\')\\noutput = embedding.embed_query(document)\\n```\\n\\n## Embed Query with AARCH:\\n\\n```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'aarch\\')\\noutput = embedding.embed_query(document)\\n```\\n\\n\\n## Embed Document with CPU:\\n```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'gpu\\')\\noutput = embedding.embed_documents(documents)\\n```\\n\\n## Embed Document with GPU:'), Document(metadata={'source': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_path': 'docs/docs/integrations/providers/johnsnowlabs.mdx', 'file_name': 'johnsnowlabs.mdx', 'file_type': '.mdx'}, page_content='```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'gpu\\')\\noutput = embedding.embed_documents(documents)\\n```\\n\\n## Embed Document with Apple Silicon (M1,M2,etc..):\\n\\n```python\\n\\n```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'apple_silicon\\')\\noutput = embedding.embed_documents(documents)\\n```\\n\\n## Embed Document with AARCH:\\n\\n```python\\n\\n```python\\ndocuments = [\"foo bar\", \\'bar foo\\']\\nembedding = JohnSnowLabsEmbeddings(\\'embed_sentence.bert\\',\\'aarch\\')\\noutput = embedding.embed_documents(documents)\\n```\\n\\n\\nModels are loaded with [nlp.load](https://nlp.johnsnowlabs.com/docs/en/jsl/load_api) and spark session is started with [nlp.start()](https://nlp.johnsnowlabs.com/docs/en/jsl/start-a-sparksession) under the hood.'), Document(metadata={'source': 'docs/docs/integrations/providers/joplin.mdx', 'file_path': 'docs/docs/integrations/providers/joplin.mdx', 'file_name': 'joplin.mdx', 'file_type': '.mdx'}, page_content='# Joplin\\n\\n>[Joplin](https://joplinapp.org/) is an open-source note-taking app. It captures your thoughts \\n> and securely accesses them from any device.\\n \\n\\n## Installation and Setup\\n\\nThe `Joplin API` requires an access token. \\nYou can find installation instructions [here](https://joplinapp.org/api/references/rest_api/).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/joplin).\\n\\n```python\\nfrom langchain_community.document_loaders import JoplinLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/kdbai.mdx', 'file_path': 'docs/docs/integrations/providers/kdbai.mdx', 'file_name': 'kdbai.mdx', 'file_type': '.mdx'}, page_content='# KDB.AI\\n\\n>[KDB.AI](https://kdb.ai) is a powerful knowledge-based vector database and search engine that allows you to build scalable, reliable AI applications, using real-time data, by providing advanced search, recommendation and personalization.\\n\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install kdbai-client\\n```\\n\\n\\n## Vector store\\n\\nThere exists a wrapper around KDB.AI indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain_community.vectorstores import KDBAI\\n```\\n\\nFor a more detailed walkthrough of the KDB.AI vectorstore, see [this notebook](/docs/integrations/vectorstores/kdbai)'), Document(metadata={'source': 'docs/docs/integrations/providers/kinetica.mdx', 'file_path': 'docs/docs/integrations/providers/kinetica.mdx', 'file_name': 'kinetica.mdx', 'file_type': '.mdx'}, page_content=\"# Kinetica\\n\\n[Kinetica](https://www.kinetica.com/) is a real-time database purpose built for enabling\\nanalytics and generative AI on time-series & spatial data.\\n\\n## Chat Model\\n\\nThe Kinetica LLM wrapper uses the [Kinetica SqlAssist\\nLLM](https://docs.kinetica.com/7.2/sql-gpt/concepts/) to transform natural language into\\nSQL to simplify the process of data retrieval.\\n\\nSee [Kinetica Language To SQL Chat Model](/docs/integrations/chat/kinetica) for usage.\\n\\n```python\\nfrom langchain_community.chat_models.kinetica import ChatKinetica\\n```\\n\\n## Vector Store\\n\\nThe Kinetca vectorstore wrapper leverages Kinetica's native support for [vector\\nsimilarity search](https://docs.kinetica.com/7.2/vector_search/).\\n\\nSee [Kinetica Vectorstore API](/docs/integrations/vectorstores/kinetica) for usage.\\n\\n```python\\nfrom langchain_community.vectorstores import Kinetica\\n```\\n\\n## Document Loader\"), Document(metadata={'source': 'docs/docs/integrations/providers/kinetica.mdx', 'file_path': 'docs/docs/integrations/providers/kinetica.mdx', 'file_name': 'kinetica.mdx', 'file_type': '.mdx'}, page_content='The Kinetica Document loader can be used to load LangChain [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) from the\\n[Kinetica](https://www.kinetica.com/) database.\\n\\nSee [Kinetica Document Loader](/docs/integrations/document_loaders/kinetica) for usage\\n\\n```python\\nfrom langchain_community.document_loaders.kinetica_loader import KineticaLoader\\n```\\n\\n## Retriever\\n\\nThe Kinetica Retriever can return documents given an unstructured query.\\n\\nSee [Kinetica VectorStore based Retriever](/docs/integrations/retrievers/kinetica) for usage'), Document(metadata={'source': 'docs/docs/integrations/providers/koboldai.mdx', 'file_path': 'docs/docs/integrations/providers/koboldai.mdx', 'file_name': 'koboldai.mdx', 'file_type': '.mdx'}, page_content='# KoboldAI\\n\\n>[KoboldAI](https://koboldai.com/) is a free, open-source project that allows users to run AI models locally \\n> on their own computer. \\n> It\\'s a browser-based front-end that can be used for writing or role playing with an AI.\\n>[KoboldAI](https://github.com/KoboldAI/KoboldAI-Client) is a \"a browser-based front-end for \\n> AI-assisted writing with multiple local & remote AI models...\". \\n> It has a public and local API that can be used in LangChain.\\n\\n## Installation and Setup\\n\\nCheck out the [installation guide](https://github.com/KoboldAI/KoboldAI-Client).\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/koboldai).\\n\\n```python\\nfrom langchain_community.llms import KoboldApiLLM\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/konko.mdx', 'file_path': 'docs/docs/integrations/providers/konko.mdx', 'file_name': 'konko.mdx', 'file_type': '.mdx'}, page_content=\"# Konko\\nAll functionality related to Konko\\n\\n>[Konko AI](https://www.konko.ai/) provides a fully managed API to help application developers\\n\\n>1. **Select** the right open source or proprietary LLMs for their application\\n>2. **Build** applications faster with integrations to leading application frameworks and fully managed APIs\\n>3. **Fine tune** smaller open-source LLMs to achieve industry-leading performance at a fraction of the cost\\n>4. **Deploy production-scale APIs** that meet security, privacy, throughput, and latency SLAs without infrastructure set-up or administration using Konko AI's SOC 2 compliant, multi-cloud infrastructure\\n\\n## Installation and Setup\\n\\n1. Sign in to our web app to [create an API key](https://platform.konko.ai/settings/api-keys) to access models via our endpoints for [chat completions](https://docs.konko.ai/reference/post-chat-completions) and [completions](https://docs.konko.ai/reference/post-completions).\\n2. Enable a Python3.8+ environment\\n3. Install the SDK\"), Document(metadata={'source': 'docs/docs/integrations/providers/konko.mdx', 'file_path': 'docs/docs/integrations/providers/konko.mdx', 'file_name': 'konko.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install konko\\n```\\n\\n4. Set API Keys as environment variables(`KONKO_API_KEY`,`OPENAI_API_KEY`)\\n\\n```bash\\nexport KONKO_API_KEY={your_KONKO_API_KEY_here}\\nexport OPENAI_API_KEY={your_OPENAI_API_KEY_here} #Optional\\n```\\n\\nPlease see [the Konko docs](https://docs.konko.ai/docs/getting-started) for more details.\\n\\n\\n## LLM\\n\\n**Explore Available Models:** Start by browsing through the [available models](https://docs.konko.ai/docs/list-of-models) on Konko. Each model caters to different use cases and capabilities.\\n\\nAnother way to find the list of models running on the Konko instance is through this [endpoint](https://docs.konko.ai/reference/get-models).\\n\\nSee a usage [example](/docs/integrations/llms/konko).\\n\\n### Examples of Endpoint Usage\\n\\n- **Completion with mistralai/Mistral-7B-v0.1:**'), Document(metadata={'source': 'docs/docs/integrations/providers/konko.mdx', 'file_path': 'docs/docs/integrations/providers/konko.mdx', 'file_name': 'konko.mdx', 'file_type': '.mdx'}, page_content='```python\\n  from langchain_community.llms import Konko\\n  llm = Konko(max_tokens=800, model=\\'mistralai/Mistral-7B-v0.1\\')\\n  prompt = \"Generate a Product Description for Apple Iphone 15\"\\n  response = llm.invoke(prompt)\\n  ```\\n\\n## Chat Models\\n\\nSee a usage [example](/docs/integrations/chat/konko).\\n\\n\\n- **ChatCompletion with Mistral-7B:**\\n\\n  ```python\\n  from langchain_core.messages import HumanMessage\\n  from langchain_community.chat_models import ChatKonko\\n  chat_instance = ChatKonko(max_tokens=10, model = \\'mistralai/mistral-7b-instruct-v0.1\\')\\n  msg = HumanMessage(content=\"Hi\")\\n  chat_response = chat_instance([msg])\\n  ```\\n\\nFor further assistance, contact [support@konko.ai](mailto:support@konko.ai) or join our [Discord](https://discord.gg/TXV2s3z7RZ).'), Document(metadata={'source': 'docs/docs/integrations/providers/konlpy.mdx', 'file_path': 'docs/docs/integrations/providers/konlpy.mdx', 'file_name': 'konlpy.mdx', 'file_type': '.mdx'}, page_content='# KoNLPY\\n\\n>[KoNLPy](https://konlpy.org/) is a Python package for natural language processing (NLP) \\n> of the Korean language.\\n\\n\\n## Installation and Setup\\n\\nYou need to install the `konlpy` python package.\\n\\n```bash\\npip install konlpy\\n```\\n\\n## Text splitter\\n\\nSee a [usage example](/docs/how_to/split_by_token/#konlpy).\\n\\n```python\\nfrom langchain_text_splitters import KonlpyTextSplitter\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/kuzu.mdx', 'file_path': 'docs/docs/integrations/providers/kuzu.mdx', 'file_name': 'kuzu.mdx', 'file_type': '.mdx'}, page_content='# Kùzu\\n\\n> [Kùzu](https://kuzudb.com/) is an embeddable, scalable, extremely fast graph database.\\n> It is permissively licensed with an MIT license, and you can see its source code [here](https://github.com/kuzudb/kuzu).\\n\\n> Key characteristics of Kùzu:\\n>- Performance and scalability: Implements modern, state-of-the-art join algorithms for graphs.\\n>- Usability: Very easy to set up and get started with, as there are no servers (embedded architecture).\\n>- Interoperability: Can conveniently scan and copy data from external columnar formats, CSV, JSON and relational databases.\\n>- Structured property graph model: Implements the property graph model, with added structure.\\n>- Cypher support: Allows convenient querying of the graph in Cypher, a declarative query language.\\n\\n> Get started with Kùzu by visiting their [documentation](https://docs.kuzudb.com/).\\n\\n\\n## Installation and Setup\\n\\nInstall the Python SDK as follows:\\n\\n```bash\\npip install -U langchain-kuzu\\n```\\n\\n## Usage\\n\\n## Graphs'), Document(metadata={'source': 'docs/docs/integrations/providers/kuzu.mdx', 'file_path': 'docs/docs/integrations/providers/kuzu.mdx', 'file_name': 'kuzu.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/graphs/kuzu_db).\\n\\n```python\\nfrom langchain_kuzu.graphs.kuzu_graph import KuzuGraph\\n```\\n\\n## Chains\\n\\nSee a [usage example](/docs/integrations/graphs/kuzu_db/#creating-kuzuqachain).\\n\\n```python\\nfrom langchain_kuzu.chains.graph_qa.kuzu import KuzuQAChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/labelstudio.mdx', 'file_path': 'docs/docs/integrations/providers/labelstudio.mdx', 'file_name': 'labelstudio.mdx', 'file_type': '.mdx'}, page_content='# Label Studio\\n\\n\\n>[Label Studio](https://labelstud.io/guide/get_started) is an open-source data labeling platform that provides LangChain with flexibility when it comes to labeling data for fine-tuning large language models (LLMs). It also enables the preparation of custom training data and the collection and evaluation of responses through human feedback.\\n\\n## Installation and Setup\\n\\nSee the [Label Studio installation guide](https://labelstud.io/guide/install) for installation options.\\n\\nWe need to install the  `label-studio` and `label-studio-sdk-python` Python packages:\\n\\n```bash\\npip install label-studio label-studio-sdk\\n```\\n\\n\\n## Callbacks\\n\\nSee a [usage example](/docs/integrations/callbacks/labelstudio).\\n\\n```python\\nfrom langchain.callbacks import LabelStudioCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/lakefs.mdx', 'file_path': 'docs/docs/integrations/providers/lakefs.mdx', 'file_name': 'lakefs.mdx', 'file_type': '.mdx'}, page_content='# lakeFS\\n\\n>[lakeFS](https://docs.lakefs.io/) provides scalable version control over \\n> the data lake, and uses Git-like semantics to create and access those versions. \\n\\n## Installation and Setup\\n\\nGet the `ENDPOINT`, `LAKEFS_ACCESS_KEY`, and `LAKEFS_SECRET_KEY`.\\nYou can find installation instructions [here](https://docs.lakefs.io/quickstart/launch.html).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/lakefs).\\n\\n```python\\nfrom langchain_community.document_loaders import LakeFSLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/lancedb.mdx', 'file_path': 'docs/docs/integrations/providers/lancedb.mdx', 'file_name': 'lancedb.mdx', 'file_type': '.mdx'}, page_content='# LanceDB\\n\\nThis page covers how to use [LanceDB](https://github.com/lancedb/lancedb) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific LanceDB wrappers.\\n\\n## Installation and Setup\\n\\n- Install the Python SDK with `pip install lancedb`\\n\\n## Wrappers\\n\\n### VectorStore\\n\\nThere exists a wrapper around LanceDB databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import LanceDB\\n```\\n\\nFor a more detailed walkthrough of the LanceDB wrapper, see [this notebook](/docs/integrations/vectorstores/lancedb)'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content=\"# LangChain Decorators ✨\\n\\n~~~\\nDisclaimer: `LangChain decorators` is not created by the LangChain team and is not supported by it.\\n~~~\\n\\n>`LangChain decorators` is a layer on the top of LangChain that provides syntactic sugar 🍭 for writing custom langchain prompts and chains\\n>\\n>For Feedback, Issues, Contributions - please raise an issue here: \\n>[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)\\n\\n\\nMain principles and benefits:\\n\\n- more `pythonic` way of writing code\\n- write multiline prompts that won't break your code flow with indentation\\n- making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc.\\n- leverage all the power of 🦜🔗 LangChain ecosystem\\n- adding support for **optional parameters**\\n- easily share parameters between the prompts by binding them to one class\\n\\n\\nHere is a simple example of a code written with **LangChain Decorators ✨**\"), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='``` python\\n\\n@llm_prompt\\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\")->str:\\n    \"\"\"\\n    Write me a short header for my post about {topic} for {platform} platform. \\n    It should be for {audience} audience.\\n    (Max 15 words)\\n    \"\"\"\\n    return\\n\\n# run it naturally\\nwrite_me_short_post(topic=\"starwars\")\\n# or\\nwrite_me_short_post(topic=\"starwars\", platform=\"redit\")\\n```\\n\\n# Quick start\\n## Installation\\n```bash\\npip install langchain_decorators\\n```\\n\\n## Examples\\n\\nGood idea on how to start is to review the examples here:\\n - [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)\\n - [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)\\n\\n# Defining other parameters\\nHere we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator.\\nHere is how it works:\\n\\n1. Using **Global settings**:\\n\\n``` python\\n# define global settings for all prompty (if not set - chatGPT is the current default)\\nfrom langchain_decorators import GlobalSettings\\n\\nGlobalSettings.define_settings(\\n    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\\n    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\\n)\\n```\\n\\n2. Using predefined **prompt types**\\n\\n``` python\\n#You can change the default prompt types\\nfrom langchain_decorators import PromptTypes, PromptTypeSettings\\n\\nPromptTypes.AGENT_REASONING.llm = ChatOpenAI()\\n\\n# Or you can just define your own ones:\\nclass MyCustomPromptTypes(PromptTypes):\\n    GPT4=PromptTypeSettings(llm=ChatOpenAI(model=\"gpt-4\"))'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) \\ndef write_a_complicated_code(app_idea:str)->str:\\n    ...\\n\\n```\\n\\n3.  Define the settings **directly in the decorator**\\n\\n``` python\\nfrom langchain_openai import OpenAI\\n\\n@llm_prompt(\\n    llm=OpenAI(temperature=0.7),\\n    stop_tokens=[\"\\\\nObservation\"],\\n    ...\\n    )\\ndef creative_writer(book_title:str)->str:\\n    ...\\n```\\n\\n## Passing a memory and/or callbacks:\\n\\nTo pass any of these, just declare them in the function (or use kwargs to pass anything)\\n\\n```python\\n\\n@llm_prompt()\\nasync def write_me_short_post(topic:str, platform:str=\"twitter\", memory:SimpleMemory = None):\\n    \"\"\"\\n    {history_key}\\n    Write me a short header for my post about {topic} for {platform} platform. \\n    It should be for {audience} audience.\\n    (Max 15 words)\\n    \"\"\"\\n    pass\\n\\nawait write_me_short_post(topic=\"old movies\")\\n\\n```\\n\\n# Simplified streaming'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='If we want to leverage streaming:\\n - we need to define prompt as async function \\n - turn on the streaming on the decorator, or we can define PromptType with streaming on\\n - capture the stream using StreamingContext\\n\\nThis way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...\\n\\nThe streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream\\n\\n``` python\\n# this code example is complete and should run as it is\\n\\nfrom langchain_decorators import StreamingContext, llm_prompt'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don\\'t want to pass distribute the callback handlers)\\n# note that only async functions can be streamed (will get an error if it\\'s not)\\n@llm_prompt(capture_stream=True) \\nasync def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\\n    \"\"\"\\n    Write me a short header for my post about {topic} for {platform} platform. \\n    It should be for {audience} audience.\\n    (Max 15 words)\\n    \"\"\"\\n    pass\\n\\n# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world\\ntokens=[]\\ndef capture_stream_func(new_token:str):\\n    tokens.append(new_token)'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='# if we want to capture the stream, we need to wrap the execution into StreamingContext... \\n# this will allow us to capture the stream even if the prompt call is hidden inside higher level method\\n# only the prompts marked with capture_stream will be captured here\\nwith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):\\n    result = await run_prompt()\\n    print(\"Stream finished ... we can distinguish tokens thanks to alternating colors\")\\n\\n\\nprint(\"\\\\nWe\\'ve captured\",len(tokens),\"tokens🎉\\\\n\")\\nprint(\"Here is the result:\")\\nprint(result)\\n```\\n\\n\\n# Prompt declarations\\nBy default the prompt is is the whole function docs, unless you mark your prompt \\n\\n## Documenting your prompt\\n\\nWe can specify what part of our docs is the prompt definition, by specifying a code block with `<prompt>` language tag'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='``` python\\n@llm_prompt\\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\\n    \"\"\"\\n    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.\\n\\n    It needs to be a code block, marked as a `<prompt>` language\\n    ```<prompt>\\n    Write me a short header for my post about {topic} for {platform} platform. \\n    It should be for {audience} audience.\\n    (Max 15 words)\\n    ```\\n\\n    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\\n    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\\n    \"\"\"\\n    return \\n```\\n\\n## Chat messages prompt\\n\\nFor chat models is very useful to define prompt as a set of message templates... here is how to do it:'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='``` python\\n@llm_prompt\\ndef simulate_conversation(human_input:str, agent_role:str=\"a pirate\"):\\n    \"\"\"\\n    ## System message\\n     - note the `:system` suffix inside the <prompt:_role_> tag\\n     \\n\\n    ```<prompt:system>\\n    You are a {agent_role} hacker. You mus act like one.\\n    You reply always in code, using python or javascript code block...\\n    for example:\\n    \\n    ... do not reply with anything else.. just with code - respecting your role.\\n    ```\\n\\n    # human message \\n    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)\\n    ``` <prompt:user>\\n    Helo, who are you\\n    ```\\n    a reply:\\n    \\n\\n    ``` <prompt:assistant>\\n    \\\\``` python <<- escaping inner code block with \\\\ that should be part of the prompt\\n    def hello():\\n        print(\"Argh... hello you pesky pirate\")\\n    \\\\```\\n    ```\\n    \\n    we can also add some history using placeholder\\n    ```<prompt:placeholder>\\n    {history}\\n    ```\\n    ```<prompt:user>\\n    {human_input}\\n    ```'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\\n    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\\n    \"\"\"\\n    pass\\n\\n```\\n\\nthe roles here are model native roles (assistant, user, system for chatGPT)\\n\\n# Optional sections\\n- you can define a whole sections of your prompt that should be optional\\n- if any input in the section is missing, the whole section won\\'t be rendered\\n\\nthe syntax for this is as follows:\\n\\n``` python\\n@llm_prompt\\ndef prompt_with_optional_partials():\\n    \"\"\"\\n    this text will be rendered always, but\\n\\n    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\")   ?}'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='you can also place it in between the words\\n    this too will be rendered{? , but\\n        this  block will be rendered only if {this_value} and {this_value}\\n        is not empty?} !\\n    \"\"\"\\n```\\n\\n\\n# Output parsers\\n\\n- llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)\\n- list, dict and pydantic outputs are also supported natively (automatically)\\n\\n``` python\\n# this code example is complete and should run as it is\\n\\nfrom langchain_decorators import llm_prompt\\n\\n@llm_prompt\\ndef write_name_suggestions(company_business:str, count:int)->list:\\n    \"\"\" Write me {count} good name suggestions for company that {company_business}\\n    \"\"\"\\n    pass\\n\\nwrite_name_suggestions(company_business=\"sells cookies\", count=5)\\n```\\n\\n## More complex structures'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='for dict / pydantic you need to specify the formatting instructions... \\nthis can be tedious, that\\'s why you can let the output parser gegnerate you the instructions based on the model (pydantic)\\n\\n``` python\\nfrom langchain_decorators import llm_prompt\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass TheOutputStructureWeExpect(BaseModel):\\n    name:str = Field (description=\"The name of the company\")\\n    headline:str = Field( description=\"The description of the company (for landing page)\")\\n    employees:list[str] = Field(description=\"5-8 fake employee names with their positions\")\\n\\n@llm_prompt()\\ndef fake_company_generator(company_business:str)->TheOutputStructureWeExpect:\\n    \"\"\" Generate a fake company that {company_business}\\n    {FORMAT_INSTRUCTIONS}\\n    \"\"\"\\n    return\\n\\ncompany = fake_company_generator(company_business=\"sells cookies\")'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='# print the result nicely formatted\\nprint(\"Company name: \",company.name)\\nprint(\"company headline: \",company.headline)\\nprint(\"company employees: \",company.employees)\\n\\n```\\n\\n\\n# Binding the prompt to an object\\n\\n``` python\\nfrom pydantic import BaseModel\\nfrom langchain_decorators import llm_prompt\\n\\nclass AssistantPersonality(BaseModel):\\n    assistant_name:str\\n    assistant_role:str\\n    field:str\\n\\n    @property\\n    def a_property(self):\\n        return \"whatever\"\\n\\n    def hello_world(self, function_kwarg:str=None):\\n        \"\"\"\\n        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method\\n        \"\"\"\\n\\n    \\n    @llm_prompt\\n    def introduce_your_self(self)->str:\\n        \"\"\"\\n        ```\\xa0<prompt:system>\\n        You are an assistant named {assistant_name}. \\n        Your role is to act as {assistant_role}\\n        ```\\n        ```<prompt:user>\\n        Introduce your self (in less than 20 words)\\n        ```\\n        \"\"\"'), Document(metadata={'source': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_path': 'docs/docs/integrations/providers/langchain_decorators.mdx', 'file_name': 'langchain_decorators.mdx', 'file_type': '.mdx'}, page_content='personality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\")\\n\\nprint(personality.introduce_your_self(personality))\\n```\\n\\n\\n# More examples:\\n\\n- these and few more examples are also available in the [colab notebook here](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)\\n- including the [ReAct Agent re-implementation](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) using purely langchain decorators'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content='# LangFair: Use-Case Level LLM Bias and Fairness Assessments\\n\\nLangFair is a comprehensive Python library designed for conducting bias and fairness assessments of large language model (LLM) use cases. The LangFair [repository](https://github.com/cvs-health/langfair) includes a comprehensive framework for [choosing bias and fairness metrics](https://github.com/cvs-health/langfair/tree/main#-choosing-bias-and-fairness-metrics-for-an-llm-use-case), along with [demo notebooks](https://github.com/cvs-health/langfair/tree/main/examples) and a [technical playbook](https://arxiv.org/abs/2407.10853) that discusses LLM bias and fairness risks, evaluation metrics, and best practices. \\n\\nExplore our [documentation site](https://cvs-health.github.io/langfair/) for detailed instructions on using LangFair.'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content='## ⚡ Quickstart Guide\\n### (Optional) Create a virtual environment for using LangFair\\nWe recommend creating a new virtual environment using venv before installing LangFair. To do so, please follow instructions [here](https://docs.python.org/3/library/venv.html).\\n\\n### Installing LangFair\\nThe latest version can be installed from PyPI:\\n\\n```bash\\npip install langfair\\n```\\n\\n### Usage Examples\\nBelow are code samples illustrating how to use LangFair to assess bias and fairness risks in text generation and summarization use cases. The below examples assume the user has already defined a list of prompts from their use case, `prompts`.'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content='##### Generate LLM responses\\nTo generate responses, we can use LangFair\\'s `ResponseGenerator` class. First, we must create a `langchain` LLM object. Below we use `ChatVertexAI`, but **any of [LangChain’s LLM classes](https://js.langchain.com/docs/integrations/chat/) may be used instead**. Note that `InMemoryRateLimiter` is to used to avoid rate limit errors.\\n```python\\nfrom langchain_google_vertexai import ChatVertexAI\\nfrom langchain_core.rate_limiters import InMemoryRateLimiter\\nrate_limiter = InMemoryRateLimiter(\\n    requests_per_second=4.5, check_every_n_seconds=0.5, max_bucket_size=280,  \\n)\\nllm = ChatVertexAI(\\n    model_name=\"gemini-pro\", temperature=0.3, rate_limiter=rate_limiter\\n)\\n```\\nWe can use `ResponseGenerator.generate_responses` to generate 25 responses for each prompt, as is convention for toxicity evaluation.\\n```python\\nfrom langfair.generator import ResponseGenerator\\nrg = ResponseGenerator(langchain_llm=llm)\\ngenerations = await rg.generate_responses(prompts=prompts, count=25)\\nresponses = generations[\"data\"][\"response\"]\\nduplicated_prompts = generations[\"data\"][\"prompt\"] # so prompts correspond to responses\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content='##### Compute toxicity metrics\\nToxicity metrics can be computed with `ToxicityMetrics`. Note that use of `torch.device` is optional and should be used if GPU is available to speed up toxicity computation.\\n```python\\n# import torch # uncomment if GPU is available\\n# device = torch.device(\"cuda\") # uncomment if GPU is available\\nfrom langfair.metrics.toxicity import ToxicityMetrics\\ntm = ToxicityMetrics(\\n    # device=device, # uncomment if GPU is available,\\n)\\ntox_result = tm.evaluate(\\n    prompts=duplicated_prompts, \\n    responses=responses, \\n    return_data=True\\n)\\ntox_result[\\'metrics\\']\\n# # Output is below\\n# {\\'Toxic Fraction\\': 0.0004,\\n# \\'Expected Maximum Toxicity\\': 0.013845130120171235,\\n# \\'Toxicity Probability\\': 0.01}\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content='##### Compute stereotype metrics\\nStereotype metrics can be computed with `StereotypeMetrics`.\\n```python\\nfrom langfair.metrics.stereotype import StereotypeMetrics\\nsm = StereotypeMetrics()\\nstereo_result = sm.evaluate(responses=responses, categories=[\"gender\"])\\nstereo_result[\\'metrics\\']\\n# # Output is below\\n# {\\'Stereotype Association\\': 0.3172750176745329,\\n# \\'Cooccurrence Bias\\': 0.44766333654278373,\\n# \\'Stereotype Fraction - gender\\': 0.08}\\n```\\n\\n##### Generate counterfactual responses and compute metrics\\nWe can generate counterfactual responses with `CounterfactualGenerator`.\\n```python\\nfrom langfair.generator.counterfactual import CounterfactualGenerator\\ncg = CounterfactualGenerator(langchain_llm=llm)\\ncf_generations = await cg.generate_responses(\\n    prompts=prompts, attribute=\\'gender\\', count=25\\n)\\nmale_responses = cf_generations[\\'data\\'][\\'male_response\\']\\nfemale_responses = cf_generations[\\'data\\'][\\'female_response\\']\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content=\"Counterfactual metrics can be easily computed with `CounterfactualMetrics`.\\n```python\\nfrom langfair.metrics.counterfactual import CounterfactualMetrics\\ncm = CounterfactualMetrics()\\ncf_result = cm.evaluate(\\n    texts1=male_responses, \\n    texts2=female_responses,\\n    attribute='gender'\\n)\\ncf_result['metrics']\\n# # Output is below\\n# {'Cosine Similarity': 0.8318708,\\n# 'RougeL Similarity': 0.5195852482361165,\\n# 'Bleu Similarity': 0.3278433712872481,\\n# 'Sentiment Bias': 0.0009947145187601957}\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/langfair.mdx', 'file_path': 'docs/docs/integrations/providers/langfair.mdx', 'file_name': 'langfair.mdx', 'file_type': '.mdx'}, page_content=\"##### Alternative approach: Semi-automated evaluation with `AutoEval`\\nTo streamline assessments for text generation and summarization use cases, the `AutoEval` class conducts a multi-step process that completes all of the aforementioned steps with two lines of code.\\n```python\\nfrom langfair.auto import AutoEval\\nauto_object = AutoEval(\\n    prompts=prompts, \\n    langchain_llm=llm,\\n    # toxicity_device=device # uncomment if GPU is available\\n)\\nresults = await auto_object.evaluate()\\nresults['metrics']\\n# # Output is below\\n# {'Toxicity': {'Toxic Fraction': 0.0004,\\n#   'Expected Maximum Toxicity': 0.013845130120171235,\\n#   'Toxicity Probability': 0.01},\\n#  'Stereotype': {'Stereotype Association': 0.3172750176745329,\\n#   'Cooccurrence Bias': 0.44766333654278373,\\n#   'Stereotype Fraction - gender': 0.08,\\n#   'Expected Maximum Stereotype - gender': 0.60355167388916,\\n#   'Stereotype Probability - gender': 0.27036},\\n#  'Counterfactual': {'male-female': {'Cosine Similarity': 0.8318708,\\n#    'RougeL Similarity': 0.5195852482361165,\\n#    'Bleu Similarity': 0.3278433712872481,\\n#    'Sentiment Bias': 0.0009947145187601957}}}\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/lantern.mdx', 'file_path': 'docs/docs/integrations/providers/lantern.mdx', 'file_name': 'lantern.mdx', 'file_type': '.mdx'}, page_content='# Lantern\\n\\nThis page covers how to use the [Lantern](https://github.com/lanterndata/lantern) within LangChain\\nIt is broken into two parts: setup, and then references to specific Lantern wrappers.\\n\\n## Setup\\n1. The first step is to create a database with the `lantern` extension installed.\\n\\n    Follow the steps at [Lantern Installation Guide](https://github.com/lanterndata/lantern#-quick-install) to install the database and the extension. The docker image is the easiest way to get started.\\n\\n## Wrappers\\n\\n### VectorStore\\n\\nThere exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import Lantern\\n```\\n\\n### Usage\\n\\nFor a more detailed walkthrough of the Lantern Wrapper, see [this notebook](/docs/integrations/vectorstores/lantern)'), Document(metadata={'source': 'docs/docs/integrations/providers/linkup.mdx', 'file_path': 'docs/docs/integrations/providers/linkup.mdx', 'file_name': 'linkup.mdx', 'file_type': '.mdx'}, page_content='# Linkup\\n\\n> [Linkup](https://www.linkup.so/) provides an API to connect LLMs to the web and the Linkup Premium Partner sources.\\n\\n## Installation and Setup\\n\\nTo use the Linkup provider, you first need a valid API key, which you can find by signing-up [here](https://app.linkup.so/sign-up).\\nYou will also need the `langchain-linkup` package, which you can install using pip:\\n\\n```bash\\npip install langchain-linkup\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/linkup_search).\\n\\n```python\\nfrom langchain_linkup import LinkupSearchRetriever\\n\\nretriever = LinkupSearchRetriever(\\n    depth=\"deep\",  # \"standard\" or \"deep\"\\n    linkup_api_key=None,  # API key can be passed here or set as the LINKUP_API_KEY environment variable\\n)\\n```\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/linkup_search).\\n\\n```python\\nfrom langchain_linkup import LinkupSearchTool'), Document(metadata={'source': 'docs/docs/integrations/providers/linkup.mdx', 'file_path': 'docs/docs/integrations/providers/linkup.mdx', 'file_name': 'linkup.mdx', 'file_type': '.mdx'}, page_content='tool = LinkupSearchTool(\\n    depth=\"deep\",  # \"standard\" or \"deep\"\\n    output_type=\"searchResults\",  # \"searchResults\", \"sourcedAnswer\" or \"structured\"\\n    linkup_api_key=None,  # API key can be passed here or set as the LINKUP_API_KEY environment variable\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/llama_index.mdx', 'file_path': 'docs/docs/integrations/providers/llama_index.mdx', 'file_name': 'llama_index.mdx', 'file_type': '.mdx'}, page_content='# LlamaIndex\\n\\n>[LlamaIndex](https://www.llamaindex.ai/) is the leading data framework for building LLM applications\\n\\n\\n## Installation and Setup\\n\\nYou need to install the `llama-index` python package.\\n\\n```bash\\npip install llama-index\\n```\\n\\nSee the [installation instructions](https://docs.llamaindex.ai/en/stable/getting_started/installation/).\\n\\n## Retrievers\\n\\n### LlamaIndexRetriever\\n\\n>It is used for the question-answering with sources over an LlamaIndex data structure.\\n\\n```python\\nfrom langchain_community.retrievers.llama_index import LlamaIndexRetriever\\n```\\n\\n### LlamaIndexGraphRetriever\\n\\n>It is used for question-answering with sources over an LlamaIndex graph data structure.\\n\\n```python\\nfrom langchain_community.retrievers.llama_index import LlamaIndexGraphRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/llamacpp.mdx', 'file_path': 'docs/docs/integrations/providers/llamacpp.mdx', 'file_name': 'llamacpp.mdx', 'file_type': '.mdx'}, page_content='# Llama.cpp\\n\\n>[llama.cpp python](https://github.com/abetlen/llama-cpp-python) library is a simple Python bindings for `@ggerganov`\\n>[llama.cpp](https://github.com/ggerganov/llama.cpp).\\n>\\n>This package provides:\\n>\\n> - Low-level access to C API via ctypes interface.\\n> - High-level Python API for text completion\\n>   - `OpenAI`-like API\\n>   - `LangChain` compatibility\\n>   - `LlamaIndex` compatibility\\n> - OpenAI compatible web server\\n>   - Local Copilot replacement\\n>   - Function Calling support\\n>   - Vision API support\\n>   - Multiple Models\\n\\n## Installation and Setup\\n\\n- Install the Python package\\n  ```bash\\n  pip install llama-cpp-python\\n  ````\\n- Download one of the [supported models](https://github.com/ggerganov/llama.cpp#description) and convert them to the llama.cpp format per the [instructions](https://github.com/ggerganov/llama.cpp)\\n\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/llamacpp).\\n\\n```python\\nfrom langchain_community.chat_models import ChatLlamaCpp\\n```\\n\\n## LLMs'), Document(metadata={'source': 'docs/docs/integrations/providers/llamacpp.mdx', 'file_path': 'docs/docs/integrations/providers/llamacpp.mdx', 'file_name': 'llamacpp.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/llms/llamacpp).\\n\\n```python\\nfrom langchain_community.llms import LlamaCpp\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/llamacpp).\\n\\n```python\\nfrom langchain_community.embeddings import LlamaCppEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/llamaedge.mdx', 'file_path': 'docs/docs/integrations/providers/llamaedge.mdx', 'file_name': 'llamaedge.mdx', 'file_type': '.mdx'}, page_content='# LlamaEdge\\n\\n>[LlamaEdge](https://llamaedge.com/docs/intro/) is the easiest & fastest way to run customized \\n> and fine-tuned LLMs locally or on the edge.\\n>\\n>* Lightweight inference apps. `LlamaEdge` is in MBs instead of GBs\\n>* Native and GPU accelerated performance\\n>* Supports many GPU and hardware accelerators\\n>* Supports many optimized inference libraries\\n>* Wide selection of AI / LLM models\\n\\n## Installation and Setup\\n\\nSee the [installation instructions](https://llamaedge.com/docs/user-guide/quick-start-command).\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/llama_edge).\\n\\n```python\\nfrom langchain_community.chat_models.llama_edge import LlamaEdgeChatService\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/llamafile.mdx', 'file_path': 'docs/docs/integrations/providers/llamafile.mdx', 'file_name': 'llamafile.mdx', 'file_type': '.mdx'}, page_content='# llamafile\\n\\n>[llamafile](https://github.com/Mozilla-Ocho/llamafile) lets you distribute and run LLMs \\n> with a single file.\\n\\n>`llamafile` makes open LLMs much more accessible to both developers and end users. \\n> `llamafile` is doing that by combining [llama.cpp](https://github.com/ggerganov/llama.cpp) with \\n> [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into one framework that collapses \\n> all the complexity of LLMs down to a single-file executable (called a \"llamafile\") \\n> that runs locally on most computers, with no installation.\\n\\n\\n## Installation and Setup\\n\\nSee the [installation instructions](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart).\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/llamafile).\\n\\n```python\\nfrom langchain_community.llms.llamafile import Llamafile\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/llamafile).\\n\\n```python\\nfrom langchain_community.embeddings import LlamafileEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/llmonitor.mdx', 'file_path': 'docs/docs/integrations/providers/llmonitor.mdx', 'file_name': 'llmonitor.mdx', 'file_type': '.mdx'}, page_content='# LLMonitor\\n\\n>[LLMonitor](https://llmonitor.com?utm_source=langchain&utm_medium=py&utm_campaign=docs) is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools.\\n\\n## Installation and Setup\\n\\nCreate an account on [llmonitor.com](https://llmonitor.com?utm_source=langchain&utm_medium=py&utm_campaign=docs), then copy your new app\\'s `tracking id`.\\n\\nOnce you have it, set it as an environment variable by running:\\n\\n```bash\\nexport LLMONITOR_APP_ID=\"...\"\\n```\\n\\n\\n## Callbacks\\n\\nSee a [usage example](/docs/integrations/callbacks/llmonitor).\\n\\n```python\\nfrom langchain.callbacks import LLMonitorCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/localai.mdx', 'file_path': 'docs/docs/integrations/providers/localai.mdx', 'file_name': 'localai.mdx', 'file_type': '.mdx'}, page_content='# LocalAI\\n\\n>[LocalAI](https://localai.io/) is the free, Open Source OpenAI alternative. \\n> `LocalAI` act as a drop-in replacement REST API that’s compatible with OpenAI API \\n> specifications for local inferencing. It allows you to run LLMs, generate images, \\n> audio (and not only) locally or on-prem with consumer grade hardware, \\n> supporting multiple model families and architectures.\\n\\n:::caution\\nFor proper compatibility, please ensure you are using the `openai` SDK at version **0.x**.\\n:::\\n\\n:::info\\n`langchain-localai` is a 3rd party integration package for LocalAI. It provides a simple way to use LocalAI services in Langchain.\\nThe source code is available on [Github](https://github.com/mkhludnev/langchain-localai)\\n:::\\n\\n## Installation and Setup\\n\\nWe have to install several python packages: \\n\\n```bash\\npip install tenacity openai\\n```\\n\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/localai).\\n\\n```python\\nfrom langchain_localai import LocalAIEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/log10.mdx', 'file_path': 'docs/docs/integrations/providers/log10.mdx', 'file_name': 'log10.mdx', 'file_type': '.mdx'}, page_content='# Log10\\n\\nThis page covers how to use the [Log10](https://log10.io) within LangChain.\\n\\n## What is Log10?\\n\\nLog10 is an [open-source](https://github.com/log10-io/log10) proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls.\\n\\n## Quick start\\n\\n1. Create your free account at [log10.io](https://log10.io)\\n2. Add your `LOG10_TOKEN` and `LOG10_ORG_ID` from the Settings and Organization tabs respectively as environment variables.\\n3. Also add `LOG10_URL=https://log10.io` and your usual LLM API key: for e.g. `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` to your environment\\n\\n## How to enable Log10 data management for Langchain\\n\\nIntegration with log10 is a simple one-line `log10_callback` integration as shown below:\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.messages import HumanMessage\\n\\nfrom log10.langchain import Log10Callback\\nfrom log10.llm import Log10Config'), Document(metadata={'source': 'docs/docs/integrations/providers/log10.mdx', 'file_path': 'docs/docs/integrations/providers/log10.mdx', 'file_name': 'log10.mdx', 'file_type': '.mdx'}, page_content='log10_callback = Log10Callback(log10_config=Log10Config())\\n\\nmessages = [\\n    HumanMessage(content=\"You are a ping pong machine\"),\\n    HumanMessage(content=\"Ping?\"),\\n]\\n\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", callbacks=[log10_callback])\\n```\\n\\n[Log10 + Langchain + Logs docs](https://github.com/log10-io/log10/blob/main/logging.md#langchain-logger)\\n\\n[More details + screenshots](https://log10.io/docs/observability/logs) including instructions for self-hosting logs\\n\\n## How to use tags with Log10\\n\\n```python\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.chat_models import ChatAnthropic\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.messages import HumanMessage\\n\\nfrom log10.langchain import Log10Callback\\nfrom log10.llm import Log10Config\\n\\nlog10_callback = Log10Callback(log10_config=Log10Config())\\n\\nmessages = [\\n    HumanMessage(content=\"You are a ping pong machine\"),\\n    HumanMessage(content=\"Ping?\"),\\n]'), Document(metadata={'source': 'docs/docs/integrations/providers/log10.mdx', 'file_path': 'docs/docs/integrations/providers/log10.mdx', 'file_name': 'log10.mdx', 'file_type': '.mdx'}, page_content='llm = ChatOpenAI(model=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])\\nprint(completion)\\n\\nllm = ChatAnthropic(model=\"claude-2\", callbacks=[log10_callback], temperature=0.7, tags=[\"baz\"])\\nllm.predict_messages(messages)\\nprint(completion)\\n\\nllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", callbacks=[log10_callback], temperature=0.5)\\ncompletion = llm.predict(\"You are a ping pong machine.\\\\nPing?\\\\n\")\\nprint(completion)\\n```\\n\\nYou can also intermix direct OpenAI calls and Langchain LLM calls:\\n\\n```python\\nimport os\\nfrom log10.load import log10, log10_session\\nimport openai\\nfrom langchain_openai import OpenAI\\n\\nlog10(openai)'), Document(metadata={'source': 'docs/docs/integrations/providers/log10.mdx', 'file_path': 'docs/docs/integrations/providers/log10.mdx', 'file_name': 'log10.mdx', 'file_type': '.mdx'}, page_content='with log10_session(tags=[\"foo\", \"bar\"]):\\n    # Log a direct OpenAI call\\n    response = openai.Completion.create(\\n        model=\"text-ada-001\",\\n        prompt=\"Where is the Eiffel Tower?\",\\n        temperature=0,\\n        max_tokens=1024,\\n        top_p=1,\\n        frequency_penalty=0,\\n        presence_penalty=0,\\n    )\\n    print(response)\\n\\n    # Log a call via Langchain\\n    llm = OpenAI(model_name=\"text-ada-001\", temperature=0.5)\\n    response = llm.predict(\"You are a ping pong machine.\\\\nPing?\\\\n\")\\n    print(response)\\n```\\n\\n## How to debug Langchain calls\\n\\n[Example of debugging](https://log10.io/docs/observability/prompt_chain_debugging)\\n\\n[More Langchain examples](https://github.com/log10-io/log10/tree/main/examples#langchain)'), Document(metadata={'source': 'docs/docs/integrations/providers/maritalk.mdx', 'file_path': 'docs/docs/integrations/providers/maritalk.mdx', 'file_name': 'maritalk.mdx', 'file_type': '.mdx'}, page_content='# MariTalk\\n\\n>[MariTalk](https://www.maritaca.ai/en) is an LLM-based chatbot trained to meet the needs of Brazil.\\n\\n## Installation and Setup\\n\\nYou have to get the MariTalk API key.\\n\\nYou also need to install the `httpx` Python package.\\n\\n```bash\\npip install httpx\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/maritalk).\\n\\n```python\\nfrom langchain_community.chat_models import ChatMaritalk\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mediawikidump.mdx', 'file_path': 'docs/docs/integrations/providers/mediawikidump.mdx', 'file_name': 'mediawikidump.mdx', 'file_type': '.mdx'}, page_content='# MediaWikiDump\\n\\n>[MediaWiki XML Dumps](https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps) contain the content of a wiki \\n> (wiki pages with all their revisions), without the site-related data. A XML dump does not create a full backup \\n> of the wiki database, the dump does not contain user accounts, images, edit logs, etc.\\n\\n\\n## Installation and Setup\\n\\nWe need to install several python packages.\\n\\nThe `mediawiki-utilities` supports XML schema 0.11 in unmerged branches.\\n```bash\\npip install -qU git+https://github.com/mediawiki-utilities/python-mwtypes@updates_schema_0.11\\n```\\n\\nThe `mediawiki-utilities mwxml` has a bug, fix PR pending.\\n\\n```bash\\npip install -qU git+https://github.com/gdedrouas/python-mwxml@xml_format_0.11\\npip install -qU mwparserfromhell\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/mediawikidump).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import MWDumpLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/meilisearch.mdx', 'file_path': 'docs/docs/integrations/providers/meilisearch.mdx', 'file_name': 'meilisearch.mdx', 'file_type': '.mdx'}, page_content='# Meilisearch\\n\\n> [Meilisearch](https://meilisearch.com) is an open-source, lightning-fast, and hyper\\n> relevant search engine. \\n> It comes with great defaults to help developers build snappy search experiences. \\n>\\n> You can [self-host Meilisearch](https://www.meilisearch.com/docs/learn/getting_started/installation#local-installation) \\n> or run on [Meilisearch Cloud](https://www.meilisearch.com/pricing).\\n>\\n>`Meilisearch v1.3` supports vector search.\\n\\n## Installation and Setup\\n\\nSee a [usage example](/docs/integrations/vectorstores/meilisearch) for detail configuration instructions.\\n\\n\\nWe need to install `meilisearch` python package.\\n\\n```bash\\npip install meilisearch\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/meilisearch).\\n\\n```python\\nfrom langchain_community.vectorstores import Meilisearch\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/memcached.mdx', 'file_path': 'docs/docs/integrations/providers/memcached.mdx', 'file_name': 'memcached.mdx', 'file_type': '.mdx'}, page_content='# Memcached\\n\\n> [Memcached](https://www.memcached.org/) is a free & open source, high-performance, distributed memory object caching system,\\n> generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load.\\n\\nThis page covers how to use Memcached with langchain, using [pymemcache](https://github.com/pinterest/pymemcache) as\\na client to connect to an already running Memcached instance.\\n\\n## Installation and Setup\\n```bash\\npip install pymemcache\\n```\\n\\n## LLM Cache\\n\\nTo integrate a Memcached Cache into your application:\\n```python3\\nfrom langchain.globals import set_llm_cache\\nfrom langchain_openai import OpenAI\\n\\nfrom langchain_community.cache import MemcachedCache\\nfrom pymemcache.client.base import Client\\n\\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)\\nset_llm_cache(MemcachedCache(Client(\\'localhost\\')))\\n\\n# The first time, it is not yet in cache, so it should take longer\\nllm.invoke(\"Which city is the most crowded city in the USA?\")'), Document(metadata={'source': 'docs/docs/integrations/providers/memcached.mdx', 'file_path': 'docs/docs/integrations/providers/memcached.mdx', 'file_name': 'memcached.mdx', 'file_type': '.mdx'}, page_content='# The second time it is, so it goes faster\\nllm.invoke(\"Which city is the most crowded city in the USA?\")\\n```\\n\\nLearn more in the [example notebook](/docs/integrations/llm_caching#memcached-cache)'), Document(metadata={'source': 'docs/docs/integrations/providers/metal.mdx', 'file_path': 'docs/docs/integrations/providers/metal.mdx', 'file_name': 'metal.mdx', 'file_type': '.mdx'}, page_content='# Metal\\n\\nThis page covers how to use [Metal](https://getmetal.io) within LangChain.\\n\\n## What is Metal?\\n\\nMetal is a  managed retrieval & memory platform built for production. Easily index your data into `Metal` and run semantic search and retrieval on it.\\n\\n![Screenshot of the Metal dashboard showing the Browse Index feature with sample data.](/img/MetalDash.png \"Metal Dashboard Interface\")\\n\\n## Quick start\\n\\nGet started by [creating a Metal account](https://app.getmetal.io/signup).\\n\\nThen, you can easily take advantage of the `MetalRetriever` class to start retrieving your data for semantic search, prompting context, etc. This class takes a `Metal` instance and a dictionary of parameters to pass to the Metal API.\\n\\n```python\\nfrom langchain.retrievers import MetalRetriever\\nfrom metal_sdk.metal import Metal\\n\\n\\nmetal = Metal(\"API_KEY\", \"CLIENT_ID\", \"INDEX_ID\");\\nretriever = MetalRetriever(metal, params={\"limit\": 2})\\n\\ndocs = retriever.invoke(\"search term\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='---\\nkeywords: [azure]\\n---\\n\\n# Microsoft\\n\\nAll functionality related to `Microsoft Azure` and other `Microsoft` products.\\n\\n## Chat Models\\n\\n### Azure OpenAI\\n\\n>[Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from `OpenAI` including the `GPT-3`, `Codex` and `Embeddings model` series for content generation, summarization, semantic search, and natural language to code translation.\\n\\n```bash\\npip install langchain-openai\\n```\\n\\nSet the environment variables to get access to the `Azure OpenAI` service.\\n\\n```python\\nimport os\\n\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\"\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\\n```\\n\\nSee a [usage example](/docs/integrations/chat/azure_chat_openai)\\n\\n\\n```python\\nfrom langchain_openai import AzureChatOpenAI\\n```\\n\\n### Azure ML Chat Online Endpoint\\n\\nSee the documentation [here](/docs/integrations/chat/azureml_chat_endpoint) for accessing chat\\nmodels hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/).\\n\\n\\n## LLMs\\n\\n### Azure ML'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/llms/azure_ml).\\n\\n```python\\nfrom langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\\n```\\n\\n### Azure OpenAI\\n\\nSee a [usage example](/docs/integrations/llms/azure_openai).\\n\\n```python\\nfrom langchain_openai import AzureOpenAI\\n```\\n\\n## Embedding Models\\n### Azure OpenAI\\n\\nSee a [usage example](/docs/integrations/text_embedding/azureopenai)\\n\\n```python\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n```\\n\\n## Document loaders\\n\\n### Azure AI Data\\n\\n>[Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets \\n> to cloud storage and register existing data assets from the following sources:\\n>\\n>- `Microsoft OneLake`\\n>- `Azure Blob Storage`\\n>- `Azure Data Lake gen 2`\\n\\nFirst, you need to install several python packages.\\n\\n```bash\\npip install azureml-fsspec, azure-ai-generative\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/azure_ai_data).'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain.document_loaders import AzureAIDataLoader\\n```\\n\\n\\n### Azure AI Document Intelligence\\n\\n>[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known\\n> as `Azure Form Recognizer`) is machine-learning\\n> based service that extracts texts (including handwriting), tables, document structures, \\n> and key-value-pairs\\n> from digital or scanned PDFs, images, Office and HTML files.\\n>\\n> Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\\n\\nFirst, you need to install a python package.\\n\\n```bash\\npip install azure-ai-documentintelligence\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/azure_document_intelligence).\\n\\n```python\\nfrom langchain.document_loaders import AzureAIDocumentIntelligenceLoader\\n```\\n\\n\\n### Azure Blob Storage'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\">[Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.\\n\\n>[Azure Files](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction) offers fully managed\\n> file shares in the cloud that are accessible via the industry standard Server Message Block (`SMB`) protocol,\\n> Network File System (`NFS`) protocol, and `Azure Files REST API`. `Azure Files` are based on the `Azure Blob Storage`.\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='`Azure Blob Storage` is designed for:\\n- Serving images or documents directly to a browser.\\n- Storing files for distributed access.\\n- Streaming video and audio.\\n- Writing to log files.\\n- Storing data for backup and restore, disaster recovery, and archiving.\\n- Storing data for analysis by an on-premises or Azure-hosted service.\\n\\n```bash\\npip install azure-storage-blob\\n```\\n\\nSee a [usage example for the Azure Blob Storage](/docs/integrations/document_loaders/azure_blob_storage_container).\\n\\n```python\\nfrom langchain_community.document_loaders import AzureBlobStorageContainerLoader\\n```\\n\\nSee a [usage example for the Azure Files](/docs/integrations/document_loaders/azure_blob_storage_file).\\n\\n```python\\nfrom langchain_community.document_loaders import AzureBlobStorageFileLoader\\n```\\n\\n\\n### Microsoft OneDrive\\n\\n>[Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.\\n\\nFirst, you need to install a python package.'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install o365\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_onedrive).\\n\\n```python\\nfrom langchain_community.document_loaders import OneDriveLoader\\n```\\n\\n### Microsoft OneDrive File\\n\\n>[Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.\\n\\nFirst, you need to install a python package.\\n\\n```bash\\npip install o365\\n```\\n\\n```python\\nfrom langchain_community.document_loaders import OneDriveFileLoader\\n```\\n\\n\\n### Microsoft Word\\n\\n>[Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_word).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredWordDocumentLoader\\n```\\n\\n\\n### Microsoft Excel'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) is a spreadsheet editor developed by \\n> Microsoft for Windows, macOS, Android, iOS and iPadOS. \\n> It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming \\n> language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.\\n\\nThe `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files. \\nThe page content will be the raw text of the Excel file. If you use the loader in `\"elements\"` mode, an HTML \\nrepresentation of the Excel file will be available in the document metadata under the `text_as_html` key.\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_excel).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredExcelLoader\\n```\\n\\n\\n### Microsoft SharePoint'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\">[Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system \\n> that uses workflow applications, “list” databases, and other web parts and security features to \\n> empower business teams to work together developed by Microsoft.\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_sharepoint).\\n\\n```python\\nfrom langchain_community.document_loaders.sharepoint import SharePointLoader\\n```\\n\\n\\n### Microsoft PowerPoint\\n\\n>[Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_powerpoint).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredPowerPointLoader\\n```\\n\\n### Microsoft OneNote\\n\\nFirst, let's install dependencies:\\n\\n```bash\\npip install bs4 msal\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_onenote).\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\"```python\\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\\n```\\n\\n### Playwright URL Loader\\n\\n>[Playwright](https://github.com/microsoft/playwright) is an open-source automation tool \\n> developed by `Microsoft` that allows you to programmatically control and automate \\n> web browsers. It is designed for end-to-end testing, scraping, and automating \\n> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.\\n\\n\\nFirst, let's install dependencies:\\n\\n```bash\\npip install playwright unstructured\\n```\\n\\nSee a [usage example](/docs/integrations/document_loaders/url/#playwright-url-loader).\\n\\n```python\\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\\n```\\n\\n## Vector Stores\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\"### Azure Cosmos DB\\nAI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode. \\n\\nBelow are two available Azure Cosmos DB APIs that can provide vector store functionalities.\\n\\n#### Azure Cosmos DB for MongoDB (vCore)\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\">[Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support.\\n> You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.\\n> Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.\\n\\n##### Installation and Setup\\n\\nSee [detail configuration instructions](/docs/integrations/vectorstores/azure_cosmos_db).\\n\\nWe need to install `pymongo` python package.\\n\\n```bash\\npip install pymongo\\n```\\n\\n##### Deploy Azure Cosmos DB on Microsoft Azure\\n\\nAzure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.\\n\\n[Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today.\\n\\nSee a [usage example](/docs/integrations/vectorstores/azure_cosmos_db).\\n\\n```python\\nfrom langchain_community.vectorstores import AzureCosmosDBVectorSearch\\n```\\n\\n#### Azure Cosmos DB NoSQL'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview.\\nThis feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors\\ndirectly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,\\nbut also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,\\nas the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the\\nefficiency of vector-based operations.\\n\\n##### Installation and Setup\\n\\nSee [detail configuration instructions](/docs/integrations/vectorstores/azure_cosmos_db_no_sql).\\n\\nWe need to install `azure-cosmos` python package.\\n\\n```bash\\npip install azure-cosmos\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='##### Deploy Azure Cosmos DB on Microsoft Azure\\n\\nAzure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available\\nin every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.\\n\\n[Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today.\\n\\nSee a [usage example](/docs/integrations/vectorstores/azure_cosmos_db_no_sql).\\n\\n```python\\nfrom langchain_community.vectorstores import AzureCosmosDBNoSQLVectorSearch\\n```\\n\\n### Azure Database for PostgreSQL'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\">[Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\\n\\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL. \\n\\nSee a [usage example](/docs/integrations/memory/postgres_chat_message_history/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\"Since Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/docs/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.\\n\\n### Azure SQL Database\\n\\n>[Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\\n\\nBy leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='##### Installation and Setup\\n\\nSee [detail configuration instructions](/docs/integrations/vectorstores/sqlserver).\\n\\nWe need to install the `langchain-sqlserver` python package.\\n\\n```bash\\n!pip install langchain-sqlserver==0.1.1\\n```\\n\\n##### Deploy Azure SQL DB on Microsoft Azure\\n\\n[Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today.\\n\\nSee a [usage example](/docs/integrations/vectorstores/sqlserver).\\n\\n```python\\nfrom langchain_sqlserver import SQLServer_VectorStore\\n```\\n\\n### Azure AI Search\\n\\n[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service\\nthat gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid\\nqueries at scale. See [here](/docs/integrations/vectorstores/azuresearch) for usage examples.\\n\\n```python\\nfrom langchain_community.vectorstores.azuresearch import AzureSearch\\n```\\n\\n## Retrievers\\n\\n### Azure AI Search'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\">Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:\\n>- A search engine for full text search over a search index containing user-owned content\\n>- Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation\\n>- Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more\\n>- Programmability through REST APIs and client libraries in Azure SDKs\\n>- Azure integration at the data layer, machine learning layer, and AI (AI Services)\\n\\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).\\n\\nSee a [usage example](/docs/integrations/retrievers/azure_ai_search).\\n\\n```python\\nfrom langchain_community.retrievers import AzureAISearchRetriever\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content=\"## Vector Store\\n### Azure Database for PostgreSQL\\n>[Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\\n\\nSee [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL. \\n\\nYou need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/docs/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.\"), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal. \\n\\n\\n## Tools\\n\\n### Azure Container Apps dynamic sessions\\n\\nWe need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.\\nSee the instructions [here](/docs/integrations/tools/azure_dynamic_sessions/#setup).\\n\\nWe need to install a python package.\\n\\n```bash\\npip install langchain-azure-dynamic-sessions\\n```\\n\\nSee a [usage example](/docs/integrations/tools/azure_dynamic_sessions).\\n\\n```python\\nfrom langchain_azure_dynamic_sessions import SessionsPythonREPLTool\\n```\\n\\n### Bing Search\\n\\nFollow the documentation [here](/docs/integrations/tools/bing_search) to get a detail explanations and instructions of this tool.'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='The environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.\\n\\n```python\\nfrom langchain_community.tools.bing_search import BingSearchResults\\nfrom langchain_community.utilities import BingSearchAPIWrapper\\n\\napi_wrapper = BingSearchAPIWrapper()\\ntool = BingSearchResults(api_wrapper=api_wrapper)\\n```\\n\\n## Toolkits\\n\\n### Azure AI Services\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\\n```\\n\\nSee a [usage example](/docs/integrations/tools/azure_ai_services).\\n\\n```python\\nfrom langchain_community.agent_toolkits import azure_ai_services\\n```\\n\\n#### Azure AI Services individual tools\\n\\nThe `azure_ai_services` toolkit includes the following tools:'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='- Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.image_analysis.AzureAiServicesImageAnalysisTool.html)\\n- Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.document_intelligence.AzureAiServicesDocumentIntelligenceTool.html)\\n- Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.speech_to_text.AzureAiServicesSpeechToTextTool.html)\\n- Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_to_speech.AzureAiServicesTextToSpeechTool.html)\\n- Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_analytics_for_health.AzureAiServicesTextAnalyticsForHealthTool.html)'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='### Azure Cognitive Services\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\\n```\\n\\nSee a [usage example](/docs/integrations/tools/azure_cognitive_services).\\n\\n```python\\nfrom langchain_community.agent_toolkits import AzureCognitiveServicesToolkit\\n```\\n\\n#### Azure AI Services individual tools\\n\\nThe `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:\\n- `AzureCogsFormRecognizerTool`: Form Recognizer API\\n- `AzureCogsImageAnalysisTool`: Image Analysis API\\n- `AzureCogsSpeech2TextTool`: Speech2Text API\\n- `AzureCogsText2SpeechTool`: Text2Speech API\\n- `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API\\n\\n```python\\nfrom langchain_community.tools.azure_cognitive_services import (\\n    AzureCogsFormRecognizerTool,\\n    AzureCogsImageAnalysisTool,\\n    AzureCogsSpeech2TextTool,\\n    AzureCogsText2SpeechTool,\\n    AzureCogsTextAnalyticsHealthTool,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='### Microsoft Office 365 email and calendar\\n\\nWe need to install `O365` python package.\\n\\n```bash\\npip install O365\\n```\\n\\n\\nSee a [usage example](/docs/integrations/tools/office365).\\n\\n```python\\nfrom langchain_community.agent_toolkits import O365Toolkit\\n```\\n\\n#### Office 365 individual tools\\n\\nYou can use individual tools from the Office 365 Toolkit:\\n- `O365CreateDraftMessage`: creating a draft email in Office 365\\n- `O365SearchEmails`: searching email messages in Office 365\\n- `O365SearchEvents`: searching calendar events in Office 365\\n- `O365SendEvent`: sending calendar events in Office 365\\n- `O365SendMessage`: sending an email in Office 365\\n\\n```python\\nfrom langchain_community.tools.office365 import O365CreateDraftMessage\\nfrom langchain_community.tools.office365 import O365SearchEmails\\nfrom langchain_community.tools.office365 import O365SearchEvents\\nfrom langchain_community.tools.office365 import O365SendEvent\\nfrom langchain_community.tools.office365 import O365SendMessage\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='### Microsoft Azure PowerBI\\n\\nWe need to install `azure-identity` python package.\\n\\n```bash\\npip install azure-identity\\n```\\n\\nSee a [usage example](/docs/integrations/tools/powerbi).\\n\\n```python\\nfrom langchain_community.agent_toolkits import PowerBIToolkit\\nfrom langchain_community.utilities.powerbi import PowerBIDataset\\n```\\n\\n#### PowerBI individual tools\\n\\nYou can use individual tools from the Azure PowerBI Toolkit:\\n- `InfoPowerBITool`: getting metadata about a PowerBI Dataset\\n- `ListPowerBITool`: getting tables names\\n- `QueryPowerBITool`: querying a PowerBI Dataset\\n\\n```python\\nfrom langchain_community.tools.powerbi.tool import InfoPowerBITool\\nfrom langchain_community.tools.powerbi.tool import ListPowerBITool\\nfrom langchain_community.tools.powerbi.tool import QueryPowerBITool\\n```\\n\\n\\n### PlayWright Browser Toolkit'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Playwright](https://github.com/microsoft/playwright) is an open-source automation tool \\n> developed by `Microsoft` that allows you to programmatically control and automate \\n> web browsers. It is designed for end-to-end testing, scraping, and automating \\n> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install playwright lxml\\n```\\n\\nSee a [usage example](/docs/integrations/tools/playwright).\\n\\n```python\\nfrom langchain_community.agent_toolkits import PlayWrightBrowserToolkit\\n```\\n\\n#### PlayWright Browser individual tools\\n\\nYou can use individual tools from the PlayWright Browser Toolkit.'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.tools.playwright import ClickTool\\nfrom langchain_community.tools.playwright import CurrentWebPageTool\\nfrom langchain_community.tools.playwright import ExtractHyperlinksTool\\nfrom langchain_community.tools.playwright import ExtractTextTool\\nfrom langchain_community.tools.playwright import GetElementsTool\\nfrom langchain_community.tools.playwright import NavigateTool\\nfrom langchain_community.tools.playwright import NavigateBackTool\\n```\\n\\n## Graphs\\n\\n### Azure Cosmos DB for Apache Gremlin\\n\\nWe need to install a python package.\\n\\n```bash\\npip install gremlinpython\\n```\\n\\nSee a [usage example](/docs/integrations/graphs/azure_cosmosdb_gremlin).\\n\\n```python\\nfrom langchain_community.graphs import GremlinGraph\\nfrom langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\\n```\\n\\n## Utilities\\n\\n### Bing Search API'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='>[Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`, \\n> is a web search engine owned and operated by `Microsoft`.\\n\\nSee a [usage example](/docs/integrations/tools/bing_search).\\n\\n```python\\nfrom langchain_community.utilities import BingSearchAPIWrapper\\n```\\n\\n## More\\n\\n### Microsoft Presidio\\n\\n>[Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium ‘protection, garrison’) \\n> helps to ensure sensitive data is properly managed and governed. It provides fast identification and \\n> anonymization modules for private entities in text and images such as credit card numbers, names, \\n> locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\\n\\nFirst, you need to install several python packages and download a `SpaCy` model.\\n\\n```bash\\npip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\\npython -m spacy download en_core_web_lg\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/microsoft.mdx', 'file_path': 'docs/docs/integrations/providers/microsoft.mdx', 'file_name': 'microsoft.mdx', 'file_type': '.mdx'}, page_content='See [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio_data_anonymization).\\n\\n```python\\nfrom langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/milvus.mdx', 'file_path': 'docs/docs/integrations/providers/milvus.mdx', 'file_name': 'milvus.mdx', 'file_type': '.mdx'}, page_content='# Milvus\\n\\n>[Milvus](https://milvus.io/docs/overview.md) is a database that stores, indexes, and manages\\n> massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\\n\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install langchain-milvus\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/milvus).\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_milvus import Milvus\\n```\\n\\n## Retrievers\\n\\nSee a [usage example](/docs/integrations/retrievers/milvus_hybrid_search).\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\\nfrom langchain_milvus.utils.sparse import BM25SparseEmbedding\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mindsdb.mdx', 'file_path': 'docs/docs/integrations/providers/mindsdb.mdx', 'file_name': 'mindsdb.mdx', 'file_type': '.mdx'}, page_content=\"# MindsDB\\n\\nMindsDB is the platform for customizing AI from enterprise data. With MindsDB and it's nearly 200 integrations to [data sources](https://docs.mindsdb.com/integrations/data-overview) and [AI/ML frameworks](https://docs.mindsdb.com/integrations/ai-overview), any developer can use their enterprise data to customize AI for their purpose, faster and more securely.\\n\\nWith MindsDB, you can connect any data source to any AI/ML model to implement and automate AI-powered applications. Deploy, serve, and fine-tune models in real-time, utilizing data from databases, vector stores, or applications. Do all that using universal tools developers already know.\\n\\nMindsDB integrates with LangChain, enabling users to:\\n\\n\\n- Deploy models available via LangChain within MindsDB, making them accessible to numerous data sources.\\n- Fine-tune models available via LangChain within MindsDB using real-time and dynamic data.\\n- Automate AI workflows with LangChain and MindsDB.\"), Document(metadata={'source': 'docs/docs/integrations/providers/mindsdb.mdx', 'file_path': 'docs/docs/integrations/providers/mindsdb.mdx', 'file_name': 'mindsdb.mdx', 'file_type': '.mdx'}, page_content='Follow [our docs](https://docs.mindsdb.com/integrations/ai-engines/langchain) to learn more about MindsDB’s integration with LangChain and see examples.'), Document(metadata={'source': 'docs/docs/integrations/providers/minimax.mdx', 'file_path': 'docs/docs/integrations/providers/minimax.mdx', 'file_name': 'minimax.mdx', 'file_type': '.mdx'}, page_content='# Minimax\\n\\n>[Minimax](https://api.minimax.chat) is a Chinese startup that provides natural language processing models\\n> for companies and individuals.\\n\\n## Installation and Setup\\nGet a [Minimax api key](https://api.minimax.chat/user-center/basic-information/interface-key) and set it as an environment variable (`MINIMAX_API_KEY`)\\nGet a [Minimax group id](https://api.minimax.chat/user-center/basic-information) and set it as an environment variable (`MINIMAX_GROUP_ID`)\\n\\n\\n## LLM\\n\\nThere exists a Minimax LLM wrapper, which you can access with\\nSee a [usage example](/docs/integrations/llms/minimax).\\n\\n```python\\nfrom langchain_community.llms import Minimax\\n```\\n\\n## Chat Models\\n\\nSee a [usage example](/docs/integrations/chat/minimax)\\n\\n```python\\nfrom langchain_community.chat_models import MiniMaxChat\\n```\\n\\n## Text Embedding Model\\n\\nThere exists a Minimax Embedding model, which you can access with\\n```python\\nfrom langchain_community.embeddings import MiniMaxEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mistralai.mdx', 'file_path': 'docs/docs/integrations/providers/mistralai.mdx', 'file_name': 'mistralai.mdx', 'file_type': '.mdx'}, page_content='# MistralAI\\n\\n>[Mistral AI](https://docs.mistral.ai/api/) is a platform that offers hosting for their powerful open source models.\\n\\n\\n## Installation and Setup\\n\\nA valid [API key](https://console.mistral.ai/users/api-keys/) is needed to communicate with the API.\\n\\nYou will also need the `langchain-mistralai` package:\\n\\n```bash\\npip install langchain-mistralai\\n```\\n\\n## Chat models\\n\\n### ChatMistralAI\\n\\nSee a [usage example](/docs/integrations/chat/mistralai).\\n\\n```python\\nfrom langchain_mistralai.chat_models import ChatMistralAI\\n```\\n\\n## Embedding models\\n\\n### MistralAIEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/mistralai).\\n\\n```python\\nfrom langchain_mistralai import MistralAIEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mlflow.mdx', 'file_path': 'docs/docs/integrations/providers/mlflow.mdx', 'file_name': 'mlflow.mdx', 'file_type': '.mdx'}, page_content=\"# MLflow AI Gateway for LLMs\\n\\n>[The MLflow AI Gateway for LLMs](https://www.mlflow.org/docs/latest/llms/deployments/index.html) is a powerful tool designed to streamline the usage and management of various large\\n> language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface\\n> that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests.\\n\\n## Installation and Setup\\n\\nInstall `mlflow` with MLflow GenAI dependencies:\\n\\n```sh\\npip install 'mlflow[genai]'\\n```\\n\\nSet the OpenAI API key as an environment variable:\\n\\n```sh\\nexport OPENAI_API_KEY=...\\n```\\n\\nCreate a configuration file:\\n\\n```yaml\\nendpoints:\\n  - name: completions\\n    endpoint_type: llm/v1/completions\\n    model:\\n      provider: openai\\n      name: text-davinci-003\\n      config:\\n        openai_api_key: $OPENAI_API_KEY\"), Document(metadata={'source': 'docs/docs/integrations/providers/mlflow.mdx', 'file_path': 'docs/docs/integrations/providers/mlflow.mdx', 'file_name': 'mlflow.mdx', 'file_type': '.mdx'}, page_content='- name: embeddings\\n    endpoint_type: llm/v1/embeddings\\n    model:\\n      provider: openai\\n      name: text-embedding-ada-002\\n      config:\\n        openai_api_key: $OPENAI_API_KEY\\n```\\n\\nStart the gateway server:\\n\\n```sh\\nmlflow gateway start --config-path /path/to/config.yaml\\n```\\n\\n## Example provided by `MLflow`\\n\\n>The `mlflow.langchain` module provides an API for logging and loading `LangChain` models.\\n> This module exports multivariate LangChain models in the langchain flavor and univariate LangChain\\n> models in the pyfunc flavor.\\n\\nSee the [API documentation and examples](https://www.mlflow.org/docs/latest/llms/langchain/index.html) for more information.\\n\\n## Completions Example\\n\\n```python\\nimport mlflow\\nfrom langchain.chains import LLMChain, PromptTemplate\\nfrom langchain_community.llms import Mlflow\\n\\nllm = Mlflow(\\n    target_uri=\"http://127.0.0.1:5000\",\\n    endpoint=\"completions\",\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/mlflow.mdx', 'file_path': 'docs/docs/integrations/providers/mlflow.mdx', 'file_name': 'mlflow.mdx', 'file_type': '.mdx'}, page_content='llm_chain = LLMChain(\\n    llm=Mlflow,\\n    prompt=PromptTemplate(\\n        input_variables=[\"adjective\"],\\n        template=\"Tell me a {adjective} joke\",\\n    ),\\n)\\nresult = llm_chain.run(adjective=\"funny\")\\nprint(result)\\n\\nwith mlflow.start_run():\\n    model_info = mlflow.langchain.log_model(chain, \"model\")\\n\\nmodel = mlflow.pyfunc.load_model(model_info.model_uri)\\nprint(model.predict([{\"adjective\": \"funny\"}]))\\n```\\n\\n## Embeddings Example\\n\\n```python\\nfrom langchain_community.embeddings import MlflowEmbeddings\\n\\nembeddings = MlflowEmbeddings(\\n    target_uri=\"http://127.0.0.1:5000\",\\n    endpoint=\"embeddings\",\\n)\\n\\nprint(embeddings.embed_query(\"hello\"))\\nprint(embeddings.embed_documents([\"hello\"]))\\n```\\n\\n## Chat Example\\n\\n```python\\nfrom langchain_community.chat_models import ChatMlflow\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\nchat = ChatMlflow(\\n    target_uri=\"http://127.0.0.1:5000\",\\n    endpoint=\"chat\",\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/mlflow.mdx', 'file_path': 'docs/docs/integrations/providers/mlflow.mdx', 'file_name': 'mlflow.mdx', 'file_type': '.mdx'}, page_content='messages = [\\n    SystemMessage(\\n        content=\"You are a helpful assistant that translates English to French.\"\\n    ),\\n    HumanMessage(\\n        content=\"Translate this sentence from English to French: I love programming.\"\\n    ),\\n]\\nprint(chat(messages))\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mlx.mdx', 'file_path': 'docs/docs/integrations/providers/mlx.mdx', 'file_name': 'mlx.mdx', 'file_type': '.mdx'}, page_content='# MLX\\n\\n>[MLX](https://ml-explore.github.io/mlx/build/html/index.html) is a `NumPy`-like array framework \\n> designed for efficient and flexible machine learning on `Apple` silicon, \\n> brought to you by `Apple machine learning research`.\\n\\n\\n## Installation and Setup\\n\\nInstall several Python packages:\\n\\n```bash\\npip install mlx-lm transformers huggingface_hub\\n````\\n\\n\\n## Chat models\\n\\n\\nSee a [usage example](/docs/integrations/chat/mlx).\\n\\n```python\\nfrom langchain_community.chat_models.mlx import ChatMLX\\n```\\n\\n## LLMs\\n\\n### MLX Local Pipelines\\n\\nSee a [usage example](/docs/integrations/llms/mlx_pipelines).\\n\\n```python\\nfrom langchain_community.llms.mlx_pipeline import MLXPipeline\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/modal.mdx', 'file_path': 'docs/docs/integrations/providers/modal.mdx', 'file_name': 'modal.mdx', 'file_type': '.mdx'}, page_content='# Modal\\n\\nThis page covers how to use the Modal ecosystem to run LangChain custom LLMs.\\nIt is broken into two parts: \\n\\n1. Modal installation and web endpoint deployment\\n2. Using deployed web endpoint with `LLM` wrapper class.\\n\\n## Installation and Setup\\n\\n- Install with `pip install modal`\\n- Run `modal token new`\\n\\n## Define your Modal Functions and Webhooks\\n\\nYou must include a prompt. There is a rigid response structure:\\n\\n```python\\nclass Item(BaseModel):\\n    prompt: str\\n\\n@stub.function()\\n@modal.web_endpoint(method=\"POST\")\\ndef get_text(item: Item):\\n    return {\"prompt\": run_gpt2.call(item.prompt)}\\n```\\n\\nThe following is an example with the GPT2 model:\\n\\n```python\\nfrom pydantic import BaseModel\\n\\nimport modal\\n\\nCACHE_PATH = \"/root/model_cache\"\\n\\nclass Item(BaseModel):\\n    prompt: str\\n\\nstub = modal.Stub(name=\"example-get-started-with-langchain\")'), Document(metadata={'source': 'docs/docs/integrations/providers/modal.mdx', 'file_path': 'docs/docs/integrations/providers/modal.mdx', 'file_name': 'modal.mdx', 'file_type': '.mdx'}, page_content='def download_model():\\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\\n    tokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\n    model = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\n    tokenizer.save_pretrained(CACHE_PATH)\\n    model.save_pretrained(CACHE_PATH)\\n\\n# Define a container image for the LLM function below, which\\n# downloads and stores the GPT-2 model.\\nimage = modal.Image.debian_slim().pip_install(\\n    \"tokenizers\", \"transformers\", \"torch\", \"accelerate\"\\n).run_function(download_model)\\n\\n@stub.function(\\n    gpu=\"any\",\\n    image=image,\\n    retries=3,\\n)\\ndef run_gpt2(text: str):\\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\\n    tokenizer = GPT2Tokenizer.from_pretrained(CACHE_PATH)\\n    model = GPT2LMHeadModel.from_pretrained(CACHE_PATH)\\n    encoded_input = tokenizer(text, return_tensors=\\'pt\\').input_ids\\n    output = model.generate(encoded_input, max_length=50, do_sample=True)\\n    return tokenizer.decode(output[0], skip_special_tokens=True)'), Document(metadata={'source': 'docs/docs/integrations/providers/modal.mdx', 'file_path': 'docs/docs/integrations/providers/modal.mdx', 'file_name': 'modal.mdx', 'file_type': '.mdx'}, page_content='@stub.function()\\n@modal.web_endpoint(method=\"POST\")\\ndef get_text(item: Item):\\n    return {\"prompt\": run_gpt2.call(item.prompt)}\\n```\\n\\n### Deploy the web endpoint\\n\\nDeploy the web endpoint to Modal cloud with the [`modal deploy`](https://modal.com/docs/reference/cli/deploy) CLI command.\\nYour web endpoint will acquire a persistent URL under the `modal.run` domain.\\n\\n## LLM wrapper around Modal web endpoint\\n\\nThe  `Modal` LLM wrapper class which will accept your deployed web endpoint\\'s URL.\\n\\n```python\\nfrom langchain_community.llms import Modal\\n\\nendpoint_url = \"https://ecorp--custom-llm-endpoint.modal.run\"  # REPLACE ME with your deployed Modal web endpoint\\'s URL\\n\\nllm = Modal(endpoint_url=endpoint_url)\\nllm_chain = LLMChain(prompt=prompt, llm=llm)\\n\\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\\n\\nllm_chain.run(question)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/modelscope.mdx', 'file_path': 'docs/docs/integrations/providers/modelscope.mdx', 'file_name': 'modelscope.mdx', 'file_type': '.mdx'}, page_content='# ModelScope\\n\\n>[ModelScope](https://www.modelscope.cn/home) is a big repository of the models and datasets.\\n\\nThis page covers how to use the modelscope ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific modelscope wrappers.\\n\\n## Installation\\n\\n```bash\\npip install -U langchain-modelscope-integration\\n```\\n\\nHead to [ModelScope](https://modelscope.cn/) to sign up to ModelScope and generate an [SDK token](https://modelscope.cn/my/myaccesstoken). Once you\\'ve done this set the `MODELSCOPE_SDK_TOKEN` environment variable:\\n\\n```bash\\nexport MODELSCOPE_SDK_TOKEN=<your_sdk_token>\\n```\\n\\n## Chat Models\\n\\n`ModelScopeChatEndpoint` class exposes chat models from ModelScope. See available models [here](https://www.modelscope.cn/docs/model-service/API-Inference/intro).\\n\\n```python\\nfrom langchain_modelscope import ModelScopeChatEndpoint\\n\\nllm = ModelScopeChatEndpoint(model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\\nllm.invoke(\"Sing a ballad of LangChain.\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/modelscope.mdx', 'file_path': 'docs/docs/integrations/providers/modelscope.mdx', 'file_name': 'modelscope.mdx', 'file_type': '.mdx'}, page_content='## Embeddings\\n\\n`ModelScopeEmbeddings` class exposes embeddings from ModelScope.\\n\\n```python\\nfrom langchain_modelscope import ModelScopeEmbeddings\\n\\nembeddings = ModelScopeEmbeddings(model_id=\"damo/nlp_corom_sentence-embedding_english-base\")\\nembeddings.embed_query(\"What is the meaning of life?\")\\n```\\n\\n## LLMs\\n`ModelScopeLLM` class exposes LLMs from ModelScope.\\n\\n```python\\nfrom langchain_modelscope import ModelScopeLLM\\n\\nllm = ModelScopeLLM(model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\\nllm.invoke(\"The meaning of life is\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/modern_treasury.mdx', 'file_path': 'docs/docs/integrations/providers/modern_treasury.mdx', 'file_name': 'modern_treasury.mdx', 'file_type': '.mdx'}, page_content=\"# Modern Treasury\\n\\n>[Modern Treasury](https://www.moderntreasury.com/) simplifies complex payment operations. It is a unified platform to power products and processes that move money.\\n>- Connect to banks and payment systems\\n>- Track transactions and balances in real-time\\n>- Automate payment operations for scale\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/modern_treasury).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import ModernTreasuryLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/momento.mdx', 'file_path': 'docs/docs/integrations/providers/momento.mdx', 'file_name': 'momento.mdx', 'file_type': '.mdx'}, page_content=\"# Momento\\n\\n> [Momento Cache](https://docs.momentohq.com/) is the world's first truly serverless caching service, offering instant elasticity, scale-to-zero\\n> capability, and blazing-fast performance.\\n>\\n> [Momento Vector Index](https://docs.momentohq.com/vector-index) stands out as the most productive, easiest-to-use, fully serverless vector index.\\n>\\n> For both services, simply grab the SDK, obtain an API key, input a few lines into your code, and you're set to go. Together, they provide a comprehensive solution for your LLM data needs.\\n\\nThis page covers how to use the [Momento](https://gomomento.com) ecosystem within LangChain.\\n\\n## Installation and Setup\\n\\n- Sign up for a free account [here](https://console.gomomento.com/) to get an API key\\n- Install the Momento Python SDK with `pip install momento`\\n\\n## Cache\\n\\nUse Momento as a serverless, distributed, low-latency cache for LLM prompts and responses. The standard cache is the primary use case for Momento users in any environment.\"), Document(metadata={'source': 'docs/docs/integrations/providers/momento.mdx', 'file_path': 'docs/docs/integrations/providers/momento.mdx', 'file_name': 'momento.mdx', 'file_type': '.mdx'}, page_content='To integrate Momento Cache into your application:\\n\\n```python\\nfrom langchain.cache import MomentoCache\\n```\\n\\nThen, set it up with the following code:\\n\\n```python\\nfrom datetime import timedelta\\nfrom momento import CacheClient, Configurations, CredentialProvider\\nfrom langchain.globals import set_llm_cache\\n\\n# Instantiate the Momento client\\ncache_client = CacheClient(\\n    Configurations.Laptop.v1(),\\n    CredentialProvider.from_environment_variable(\"MOMENTO_API_KEY\"),\\n    default_ttl=timedelta(days=1))\\n\\n# Choose a Momento cache name of your choice\\ncache_name = \"langchain\"\\n\\n# Instantiate the LLM cache\\nset_llm_cache(MomentoCache(cache_client, cache_name))\\n```\\n\\n## Memory\\n\\nMomento can be used as a distributed memory store for LLMs.\\n\\nSee [this notebook](/docs/integrations/memory/momento_chat_message_history) for a walkthrough of how to use Momento as a memory store for chat message history.\\n\\n```python\\nfrom langchain.memory import MomentoChatMessageHistory\\n```\\n\\n## Vector Store'), Document(metadata={'source': 'docs/docs/integrations/providers/momento.mdx', 'file_path': 'docs/docs/integrations/providers/momento.mdx', 'file_name': 'momento.mdx', 'file_type': '.mdx'}, page_content='Momento Vector Index (MVI) can be used as a vector store.\\n\\nSee [this notebook](/docs/integrations/vectorstores/momento_vector_index) for a walkthrough of how to use MVI as a vector store.\\n\\n```python\\nfrom langchain_community.vectorstores import MomentoVectorIndex\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mongodb.mdx', 'file_path': 'docs/docs/integrations/providers/mongodb.mdx', 'file_name': 'mongodb.mdx', 'file_type': '.mdx'}, page_content='# MongoDB\\n\\n>[MongoDB](https://www.mongodb.com/) is a NoSQL, document-oriented \\n> database that supports JSON-like documents with a dynamic schema.\\n \\n**NOTE:** \\n- See other `MongoDB` integrations on the [MongoDB Atlas page](/docs/integrations/providers/mongodb_atlas).\\n\\n## Installation and Setup\\n\\nInstall the Python package:\\n\\n```bash\\npip install langchain-mongodb\\n```\\n\\n## Message Histories\\n\\nSee a [usage example](/docs/integrations/memory/mongodb_chat_message_history).\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_path': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_name': 'mongodb_atlas.mdx', 'file_type': '.mdx'}, page_content='# MongoDB Atlas\\n\\n>[MongoDB Atlas](https://www.mongodb.com/docs/atlas/) is a fully-managed cloud\\n> database available in AWS, Azure, and GCP.  It now has support for native \\n> Vector Search on the MongoDB document data.\\n\\n## Installation and Setup\\n\\nSee [detail configuration instructions](/docs/integrations/vectorstores/mongodb_atlas).\\n\\nWe need to install `langchain-mongodb` python package.\\n\\n```bash\\npip install langchain-mongodb\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/mongodb_atlas).\\n\\n```python\\nfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n```\\n\\n## Retrievers\\n\\n### Full Text Search Retriever\\n\\n>`Hybrid Search Retriever` performs full-text searches using \\n> Lucene’s standard (`BM25`) analyzer.\\n\\n```python\\nfrom langchain_mongodb.retrievers import MongoDBAtlasFullTextSearchRetriever\\n```\\n\\n### Hybrid Search Retriever'), Document(metadata={'source': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_path': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_name': 'mongodb_atlas.mdx', 'file_type': '.mdx'}, page_content='>`Hybrid Search Retriever` combines vector and full-text searches weighting \\n> them the via `Reciprocal Rank Fusion` (`RRF`) algorithm.\\n \\n```python\\nfrom langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever\\n```\\n\\n## Model Caches\\n\\n### MongoDBCache\\n\\nAn abstraction to store a simple cache in MongoDB. This does not use Semantic Caching, nor does it require an index to be made on the collection before generation.\\n\\nTo import this cache:\\n```python\\nfrom langchain_mongodb.cache import MongoDBCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain_core.globals import set_llm_cache\\n\\n# use any embedding provider...\\nfrom tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings\\n\\nmongodb_atlas_uri = \"<YOUR_CONNECTION_STRING>\"\\nCOLLECTION_NAME=\"<YOUR_CACHE_COLLECTION_NAME>\"\\nDATABASE_NAME=\"<YOUR_DATABASE_NAME>\"\\n\\nset_llm_cache(MongoDBCache(\\n    connection_string=mongodb_atlas_uri,\\n    collection_name=COLLECTION_NAME,\\n    database_name=DATABASE_NAME,\\n))\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_path': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_name': 'mongodb_atlas.mdx', 'file_type': '.mdx'}, page_content='### MongoDBAtlasSemanticCache\\nSemantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends MongoDBAtlas as both a cache and a vectorstore.\\nThe MongoDBAtlasSemanticCache inherits from `MongoDBAtlasVectorSearch` and needs an Atlas Vector Search Index defined to work. Please look at the [usage example](/docs/integrations/vectorstores/mongodb_atlas) on how to set up the index.\\n\\nTo import this cache:\\n```python\\nfrom langchain_mongodb.cache import MongoDBAtlasSemanticCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain_core.globals import set_llm_cache\\n\\n# use any embedding provider...\\nfrom tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings\\n\\nmongodb_atlas_uri = \"<YOUR_CONNECTION_STRING>\"\\nCOLLECTION_NAME=\"<YOUR_CACHE_COLLECTION_NAME>\"\\nDATABASE_NAME=\"<YOUR_DATABASE_NAME>\"'), Document(metadata={'source': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_path': 'docs/docs/integrations/providers/mongodb_atlas.mdx', 'file_name': 'mongodb_atlas.mdx', 'file_type': '.mdx'}, page_content='set_llm_cache(MongoDBAtlasSemanticCache(\\n    embedding=FakeEmbeddings(),\\n    connection_string=mongodb_atlas_uri,\\n    collection_name=COLLECTION_NAME,\\n    database_name=DATABASE_NAME,\\n))\\n```\\n``'), Document(metadata={'source': 'docs/docs/integrations/providers/motherduck.mdx', 'file_path': 'docs/docs/integrations/providers/motherduck.mdx', 'file_name': 'motherduck.mdx', 'file_type': '.mdx'}, page_content='# Motherduck\\n\\n>[Motherduck](https://motherduck.com/) is a managed DuckDB-in-the-cloud service.\\n\\n## Installation and Setup\\n\\nFirst, you need to install `duckdb` python package.\\n\\n```bash\\npip install duckdb\\n```\\n\\nYou will also need to sign up for an account at [Motherduck](https://motherduck.com/)\\n\\nAfter that, you should set up a connection string - we mostly integrate with Motherduck through SQLAlchemy.\\nThe connection string is likely in the form:\\n\\n```\\ntoken=\"...\"\\n\\nconn_str = f\"duckdb:///md:{token}@my_db\"\\n```\\n\\n## SQLChain\\n\\nYou can use the SQLChain to query data in your Motherduck instance in natural language.\\n\\n```\\nfrom langchain_openai import OpenAI\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_experimental.sql import SQLDatabaseChain\\ndb = SQLDatabase.from_uri(conn_str)\\ndb_chain = SQLDatabaseChain.from_llm(OpenAI(temperature=0), db, verbose=True)\\n```\\n\\nFrom here, see the [SQL Chain](/docs/how_to#qa-over-sql--csv) documentation on how to use.\\n\\n\\n## LLMCache'), Document(metadata={'source': 'docs/docs/integrations/providers/motherduck.mdx', 'file_path': 'docs/docs/integrations/providers/motherduck.mdx', 'file_name': 'motherduck.mdx', 'file_type': '.mdx'}, page_content='You can also easily use Motherduck to cache LLM requests.\\nOnce again this is done through the SQLAlchemy wrapper.\\n\\n```\\nimport sqlalchemy\\nfrom langchain.globals import set_llm_cache\\neng = sqlalchemy.create_engine(conn_str)\\nset_llm_cache(SQLAlchemyCache(engine=eng))\\n```\\n\\nFrom here, see the [LLM Caching](/docs/integrations/llm_caching) documentation on how to use.'), Document(metadata={'source': 'docs/docs/integrations/providers/motorhead.mdx', 'file_path': 'docs/docs/integrations/providers/motorhead.mdx', 'file_name': 'motorhead.mdx', 'file_type': '.mdx'}, page_content='# Motörhead\\n\\n>[Motörhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\\n\\n## Installation and Setup\\n\\nSee instructions at [Motörhead](https://github.com/getmetal/motorhead) for running the server locally.\\n\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/motorhead_memory).\\n\\n```python\\nfrom langchain_community.memory import MotorheadMemory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/myscale.mdx', 'file_path': 'docs/docs/integrations/providers/myscale.mdx', 'file_name': 'myscale.mdx', 'file_type': '.mdx'}, page_content=\"# MyScale\\n\\nThis page covers how to use MyScale vector database within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific MyScale wrappers.\\n\\nWith MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale's cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets.\\n\\n## Introduction\\n\\n[Overview to MyScale and High performance vector search](https://docs.myscale.com/en/overview/)\\n\\nYou can now register on our SaaS and [start a cluster now!](https://docs.myscale.com/en/quickstart/)\\n\\nIf you are also interested in how we managed to integrate SQL and vector, please refer to [this document](https://docs.myscale.com/en/vector-reference/) for further syntax reference.\"), Document(metadata={'source': 'docs/docs/integrations/providers/myscale.mdx', 'file_path': 'docs/docs/integrations/providers/myscale.mdx', 'file_name': 'myscale.mdx', 'file_type': '.mdx'}, page_content=\"We also deliver with live demo on huggingface! Please checkout our [huggingface space](https://huggingface.co/myscale)! They search millions of vector within a blink!\\n\\n## Installation and Setup\\n- Install the Python SDK with `pip install clickhouse-connect`\\n\\n### Setting up environments\\n\\nThere are two ways to set up parameters for myscale index.\\n\\n1. Environment Variables\\n\\n    Before you run the app, please set the environment variable with `export`:\\n    `export MYSCALE_HOST='<your-endpoints-url>' MYSCALE_PORT=<your-endpoints-port> MYSCALE_USERNAME=<your-username> MYSCALE_PASSWORD=<your-password> ...`\\n\\n    You can easily find your account, password and other info on our SaaS. For details please refer to [this document](https://docs.myscale.com/en/cluster-management/)\\n    Every attributes under `MyScaleSettings` can be set with prefix `MYSCALE_` and is case insensitive.\\n\\n2. Create `MyScaleSettings` object with parameters\"), Document(metadata={'source': 'docs/docs/integrations/providers/myscale.mdx', 'file_path': 'docs/docs/integrations/providers/myscale.mdx', 'file_name': 'myscale.mdx', 'file_type': '.mdx'}, page_content='```python\\n    from langchain_community.vectorstores import MyScale, MyScaleSettings\\n    config = MyScaleSettings(host=\"<your-backend-url>\", port=8443, ...)\\n    index = MyScale(embedding_function, config)\\n    index.add_documents(...)\\n    ```\\n  \\n## Wrappers\\nsupported functions:\\n- `add_texts`\\n- `add_documents`\\n- `from_texts`\\n- `from_documents`\\n- `similarity_search`\\n- `asimilarity_search`\\n- `similarity_search_by_vector`\\n- `asimilarity_search_by_vector`\\n- `similarity_search_with_relevance_scores`\\n- `delete`\\n\\n### VectorStore\\n\\nThere exists a wrapper around MyScale database, allowing you to use it as a vectorstore,\\nwhether for semantic search or similar example retrieval.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import MyScale\\n```\\n\\nFor a more detailed walkthrough of the MyScale wrapper, see [this notebook](/docs/integrations/vectorstores/myscale)'), Document(metadata={'source': 'docs/docs/integrations/providers/naver.mdx', 'file_path': 'docs/docs/integrations/providers/naver.mdx', 'file_name': 'naver.mdx', 'file_type': '.mdx'}, page_content='# NAVER\\n\\nAll functionality related to `Naver`, including HyperCLOVA X models, particularly those accessible through `Naver Cloud` [CLOVA Studio](https://clovastudio.ncloud.com/).\\n\\n> [Naver](https://navercorp.com/) is a global technology company with cutting-edge technologies and a diverse business portfolio including search, commerce, fintech, content, cloud, and AI.\\n\\n> [Naver Cloud](https://www.navercloudcorp.com/lang/en/) is the cloud computing arm of Naver, a leading cloud service provider offering a comprehensive suite of cloud services to businesses through its [Naver Cloud Platform (NCP)](https://www.ncloud.com/).\\n\\nPlease refer to [NCP User Guide](https://guide.ncloud-docs.com/docs/clovastudio-overview) for more detailed instructions (also in Korean).\\n\\n## Installation and Setup'), Document(metadata={'source': 'docs/docs/integrations/providers/naver.mdx', 'file_path': 'docs/docs/integrations/providers/naver.mdx', 'file_name': 'naver.mdx', 'file_type': '.mdx'}, page_content=\"- Get a CLOVA Studio API Key by [issuing it](https://api.ncloud-docs.com/docs/ai-naver-clovastudio-summary#API%ED%82%A4) and set it as an environment variable (`NCP_CLOVASTUDIO_API_KEY`).\\n    - If you are using a legacy API Key (that doesn't start with `nv-*` prefix), you might need to get an additional API Key by [creating your app](https://guide.ncloud-docs.com/docs/en/clovastudio-playground01#create-test-app) and set it as `NCP_APIGW_API_KEY`.\\n- Install the integration Python package with:\\n\\n```bash\\npip install -U langchain-community\\n```\\n\\n## Chat models\\n\\n### ChatClovaX \\n\\nSee a [usage example](/docs/integrations/chat/naver).\\n\\n```python\\nfrom langchain_community.chat_models import ChatClovaX\\n```\\n\\n## Embedding models\\n\\n### ClovaXEmbeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/naver).\\n\\n```python\\nfrom langchain_community.embeddings import ClovaXEmbeddings\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/neo4j.mdx', 'file_path': 'docs/docs/integrations/providers/neo4j.mdx', 'file_name': 'neo4j.mdx', 'file_type': '.mdx'}, page_content='# Neo4j\\n\\n>What is `Neo4j`?\\n\\n>- Neo4j is an `open-source database management system` that specializes in graph database technology.\\n>- Neo4j allows you to represent and store data in nodes and edges, making it ideal for handling connected data and relationships.\\n>- Neo4j provides a `Cypher Query Language`, making it easy to interact with and query your graph data.\\n>- With Neo4j, you can achieve high-performance `graph traversals and queries`, suitable for production-level systems.\\n\\n>Get started with Neo4j by visiting [their website](https://neo4j.com/).\\n\\n## Installation and Setup\\n\\n- Install the Python SDK with `pip install neo4j langchain-neo4j`\\n\\n\\n## VectorStore\\n\\nThe Neo4j vector index is used as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain_neo4j import Neo4jVector\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/neo4jvector)\\n\\n## GraphCypherQAChain'), Document(metadata={'source': 'docs/docs/integrations/providers/neo4j.mdx', 'file_path': 'docs/docs/integrations/providers/neo4j.mdx', 'file_name': 'neo4j.mdx', 'file_type': '.mdx'}, page_content=\"There exists a wrapper around Neo4j graph database that allows you to generate Cypher statements based on the user input\\nand use them to retrieve relevant information from the database.\\n\\n```python\\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\\n```\\n\\nSee a [usage example](/docs/integrations/graphs/neo4j_cypher)\\n\\n## Constructing a knowledge graph from text\\n\\nText data often contain rich relationships and insights that can be useful for various analytics, recommendation engines, or knowledge management applications.\\nDiffbot's NLP API allows for the extraction of entities, relationships, and semantic meaning from unstructured text data.\\nBy coupling Diffbot's NLP API with Neo4j, a graph database, you can create powerful, dynamic graph structures based on the information extracted from text.\\nThese graph structures are fully queryable and can be integrated into various applications.\"), Document(metadata={'source': 'docs/docs/integrations/providers/neo4j.mdx', 'file_path': 'docs/docs/integrations/providers/neo4j.mdx', 'file_name': 'neo4j.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_neo4j import Neo4jGraph\\nfrom langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\\n```\\n\\nSee a [usage example](/docs/integrations/graphs/diffbot)\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/neo4j_chat_message_history).\\n\\n```python\\nfrom langchain_neo4j import Neo4jChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/nlpcloud.mdx', 'file_path': 'docs/docs/integrations/providers/nlpcloud.mdx', 'file_name': 'nlpcloud.mdx', 'file_type': '.mdx'}, page_content='# NLPCloud\\n\\n>[NLP Cloud](https://docs.nlpcloud.com/#introduction) is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data. \\n\\n\\n## Installation and Setup\\n\\n- Install the `nlpcloud` package.\\n\\n```bash\\npip install nlpcloud\\n```\\n\\n- Get an NLPCloud api key and set it as an environment variable (`NLPCLOUD_API_KEY`)\\n\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/nlpcloud).\\n\\n```python\\nfrom langchain_community.llms import NLPCloud\\n```\\n\\n## Text Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/nlp_cloud)\\n\\n```python\\nfrom langchain_community.embeddings import NLPCloudEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/nomic.mdx', 'file_path': 'docs/docs/integrations/providers/nomic.mdx', 'file_name': 'nomic.mdx', 'file_type': '.mdx'}, page_content='# Nomic\\n\\n>[Nomic](https://www.nomic.ai/) builds tools that enable everyone to interact with AI scale datasets and run AI models on consumer computers.\\n>\\n>`Nomic` currently offers two products:\\n>\\n>- `Atlas`: the Visual Data Engine\\n>- `GPT4All`: the Open Source Edge Language Model Ecosystem\\n\\nThe Nomic integration exists in two partner packages: [langchain-nomic](https://pypi.org/project/langchain-nomic/)\\nand in [langchain-community](https://pypi.org/project/langchain-community/). \\n\\n## Installation\\n\\nYou can install them with:\\n\\n```bash\\npip install -U langchain-nomic\\npip install -U langchain-community\\n```\\n\\n## LLMs\\n\\n### GPT4All\\n\\nSee [a usage example](/docs/integrations/llms/gpt4all).\\n\\n```python\\nfrom langchain_community.llms import GPT4All\\n```\\n\\n## Embedding models\\n\\n### NomicEmbeddings\\n\\nSee [a usage example](/docs/integrations/text_embedding/nomic).\\n\\n```python\\nfrom langchain_nomic import NomicEmbeddings\\n```\\n\\n### GPT4All\\n\\nSee [a usage example](/docs/integrations/text_embedding/gpt4all).'), Document(metadata={'source': 'docs/docs/integrations/providers/nomic.mdx', 'file_path': 'docs/docs/integrations/providers/nomic.mdx', 'file_name': 'nomic.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.embeddings import GPT4AllEmbeddings\\n```\\n\\n## Vector store\\n\\n### Atlas\\n\\nSee [a usage example and installation instructions](/docs/integrations/vectorstores/atlas).\\n\\n```python\\nfrom langchain_community.vectorstores import AtlasDB\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/notion.mdx', 'file_path': 'docs/docs/integrations/providers/notion.mdx', 'file_name': 'notion.mdx', 'file_type': '.mdx'}, page_content='# Notion DB\\n\\n>[Notion](https://www.notion.so/) is a collaboration platform with modified Markdown support that integrates kanban \\n> boards, tasks, wikis and databases. It is an all-in-one workspace for notetaking, knowledge and data management, \\n> and project and task management.\\n\\n## Installation and Setup\\n\\nAll instructions are in examples below.\\n\\n## Document Loader\\n\\nWe have two different loaders: `NotionDirectoryLoader` and `NotionDBLoader`.\\n\\nSee [usage examples here](/docs/integrations/document_loaders/notion).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import NotionDirectoryLoader, NotionDBLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/nuclia.mdx', 'file_path': 'docs/docs/integrations/providers/nuclia.mdx', 'file_name': 'nuclia.mdx', 'file_type': '.mdx'}, page_content='# Nuclia\\n\\n>[Nuclia](https://nuclia.com) automatically indexes your unstructured data from any internal\\n> and external source, providing optimized search results and generative answers. \\n> It can handle video and audio transcription, image content extraction, and document parsing.\\n\\n## Installation and Setup\\n\\nWe need to install the `nucliadb-protos` package to use the `Nuclia Understanding API`\\n\\n```bash\\npip install nucliadb-protos\\n```\\n\\nWe need to have a `Nuclia account`. \\nWe can create one for free at [https://nuclia.cloud](https://nuclia.cloud), \\nand then [create a NUA key](https://docs.nuclia.dev/docs/docs/using/understanding/intro).\\n\\n\\n## Document Transformer\\n\\n### Nuclia\\n\\n>`Nuclia Understanding API` document transformer splits text into paragraphs and sentences, \\n> identifies entities, provides a summary of the text and generates embeddings for all the sentences.'), Document(metadata={'source': 'docs/docs/integrations/providers/nuclia.mdx', 'file_path': 'docs/docs/integrations/providers/nuclia.mdx', 'file_name': 'nuclia.mdx', 'file_type': '.mdx'}, page_content='To use the Nuclia document transformer, we need to instantiate a `NucliaUnderstandingAPI`\\ntool with `enable_ml` set to `True`:\\n\\n```python\\nfrom langchain_community.tools.nuclia import NucliaUnderstandingAPI\\n\\nnua = NucliaUnderstandingAPI(enable_ml=True)\\n```\\n\\nSee a [usage example](/docs/integrations/document_transformers/nuclia_transformer).\\n\\n```python\\nfrom langchain_community.document_transformers.nuclia_text_transform import NucliaTextTransformer\\n```\\n\\n## Document Loaders\\n\\n### Nuclea loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/nuclia).\\n\\n```python\\nfrom langchain_community.document_loaders.nuclia import NucliaLoader\\n```\\n\\n## Vector store\\n\\n### NucliaDB\\n\\nWe need to install a python package:\\n\\n```bash\\npip install nuclia\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/nucliadb).\\n\\n```python\\nfrom langchain_community.vectorstores.nucliadb import NucliaDB\\n```\\n\\n## Tools\\n\\n### Nuclia Understanding\\n\\nSee a [usage example](/docs/integrations/tools/nuclia).'), Document(metadata={'source': 'docs/docs/integrations/providers/nuclia.mdx', 'file_path': 'docs/docs/integrations/providers/nuclia.mdx', 'file_name': 'nuclia.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.tools.nuclia import NucliaUnderstandingAPI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/nvidia.mdx', 'file_path': 'docs/docs/integrations/providers/nvidia.mdx', 'file_name': 'nvidia.mdx', 'file_type': '.mdx'}, page_content='# NVIDIA\\nThe `langchain-nvidia-ai-endpoints` package contains LangChain integrations building applications with models on \\nNVIDIA NIM inference microservice. NIM supports models across domains like chat, embedding, and re-ranking models \\nfrom the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA \\naccelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single \\ncommand on NVIDIA accelerated infrastructure.\\n\\nNVIDIA hosted deployments of NIMs are available to test on the [NVIDIA API catalog](https://build.nvidia.com/). After testing, \\nNIMs can be exported from NVIDIA’s API catalog using the NVIDIA AI Enterprise license and run on-premises or in the cloud, \\ngiving enterprises ownership and full control of their IP and AI application.'), Document(metadata={'source': 'docs/docs/integrations/providers/nvidia.mdx', 'file_path': 'docs/docs/integrations/providers/nvidia.mdx', 'file_name': 'nvidia.mdx', 'file_type': '.mdx'}, page_content='NIMs are packaged as container images on a per model basis and are distributed as NGC container images through the NVIDIA NGC Catalog. \\nAt their core, NIMs provide easy, consistent, and familiar APIs for running inference on an AI model.\\n\\nBelow is an example on how to use some common functionality surrounding text-generative and embedding models.\\n\\n## Installation\\n\\n```python\\npip install -U --quiet langchain-nvidia-ai-endpoints\\n```\\n\\n## Setup\\n\\n**To get started:**\\n\\n1. Create a free account with [NVIDIA](https://build.nvidia.com/), which hosts NVIDIA AI Foundation models.\\n\\n2. Click on your model of choice.\\n\\n3. Under Input select the Python tab, and click `Get API Key`. Then click `Generate Key`.\\n\\n4. Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\\n\\n```python\\nimport getpass\\nimport os'), Document(metadata={'source': 'docs/docs/integrations/providers/nvidia.mdx', 'file_path': 'docs/docs/integrations/providers/nvidia.mdx', 'file_name': 'nvidia.mdx', 'file_type': '.mdx'}, page_content='if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\\n    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\\n    assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\\n    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\\n```\\n## Working with NVIDIA API Catalog\\n\\n```python\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\n\\nllm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\\nresult = llm.invoke(\"Write a ballad about LangChain.\")\\nprint(result.content)\\n```\\n\\nUsing the API, you can query live endpoints available on the\\xa0NVIDIA API Catalog\\xa0to get quick results from a DGX-hosted cloud compute environment. All models are source-accessible and can be deployed on your own compute cluster using NVIDIA NIM which is part of NVIDIA AI Enterprise, shown in the next section [Working with NVIDIA NIMs](#working-with-nvidia-nims).'), Document(metadata={'source': 'docs/docs/integrations/providers/nvidia.mdx', 'file_path': 'docs/docs/integrations/providers/nvidia.mdx', 'file_name': 'nvidia.mdx', 'file_type': '.mdx'}, page_content='## Working with NVIDIA NIMs\\nWhen ready to deploy, you can self-host models with NVIDIA NIM—which is included with the NVIDIA AI Enterprise software license—and run them anywhere, giving you ownership of your customizations and full control of your intellectual property (IP) and AI applications.\\n\\n[Learn more about NIMs](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)\\n\\n```python\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank\\n\\n# connect to a chat NIM running at localhost:8000, specifying a model\\nllm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")\\n\\n# connect to an embedding NIM running at localhost:8080\\nembedder = NVIDIAEmbeddings(base_url=\"http://localhost:8080/v1\")\\n\\n# connect to a reranking NIM running at localhost:2016\\nranker = NVIDIARerank(base_url=\"http://localhost:2016/v1\")\\n```\\n\\n## Using NVIDIA AI Foundation Endpoints'), Document(metadata={'source': 'docs/docs/integrations/providers/nvidia.mdx', 'file_path': 'docs/docs/integrations/providers/nvidia.mdx', 'file_name': 'nvidia.mdx', 'file_type': '.mdx'}, page_content='A selection of NVIDIA AI Foundation models are supported directly in LangChain with familiar APIs.\\n\\nThe active models which are supported can be found [in API Catalog](https://build.nvidia.com/).\\n\\n**The following may be useful examples to help you get started:**\\n- **[`ChatNVIDIA` Model](/docs/integrations/chat/nvidia_ai_endpoints).**\\n- **[`NVIDIAEmbeddings` Model for RAG Workflows](/docs/integrations/text_embedding/nvidia_ai_endpoints).**'), Document(metadata={'source': 'docs/docs/integrations/providers/obsidian.mdx', 'file_path': 'docs/docs/integrations/providers/obsidian.mdx', 'file_name': 'obsidian.mdx', 'file_type': '.mdx'}, page_content='# Obsidian\\n\\n>[Obsidian](https://obsidian.md/) is a powerful and extensible knowledge base\\nthat works on top of your local folder of plain text files.\\n\\n## Installation and Setup\\n\\nAll instructions are in examples below.\\n\\n## Document Loader\\n\\n\\nSee a [usage example](/docs/integrations/document_loaders/obsidian).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import ObsidianLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/oceanbase.mdx', 'file_path': 'docs/docs/integrations/providers/oceanbase.mdx', 'file_name': 'oceanbase.mdx', 'file_type': '.mdx'}, page_content='# OceanBase\\n\\n[OceanBase Database](https://github.com/oceanbase/oceanbase) is a distributed relational database. \\nIt is developed entirely by Ant Group. The OceanBase Database is built on a common server cluster. \\nBased on the Paxos protocol and its distributed structure, the OceanBase Database provides high availability and linear scalability.\\n\\nOceanBase currently has the ability to store vectors. Users can easily perform the following operations with SQL:\\n\\n- Create a table containing vector type fields;\\n- Create a vector index table based on the HNSW algorithm;\\n- Perform vector approximate nearest neighbor queries;\\n- ...\\n\\n## Installation\\n\\n```bash\\npip install -U langchain-oceanbase\\n```\\n\\nWe recommend using Docker to deploy OceanBase:\\n\\n```shell\\ndocker run --name=ob433 -e MODE=slim -p 2881:2881 -d oceanbase/oceanbase-ce:4.3.3.0-100000132024100711\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/oceanbase.mdx', 'file_path': 'docs/docs/integrations/providers/oceanbase.mdx', 'file_name': 'oceanbase.mdx', 'file_type': '.mdx'}, page_content='[More methods to deploy OceanBase cluster](https://github.com/oceanbase/oceanbase-doc/blob/V4.3.1/en-US/400.deploy/500.deploy-oceanbase-database-community-edition/100.deployment-overview.md)\\n\\n### Usage\\n\\nFor a more detailed walkthrough of the OceanBase Wrapper, see [this notebook](https://github.com/oceanbase/langchain-oceanbase/blob/main/docs/vectorstores.ipynb)'), Document(metadata={'source': 'docs/docs/integrations/providers/oci.mdx', 'file_path': 'docs/docs/integrations/providers/oci.mdx', 'file_name': 'oci.mdx', 'file_type': '.mdx'}, page_content='# Oracle Cloud Infrastructure (OCI)\\n\\nThe `LangChain` integrations related to [Oracle Cloud Infrastructure](https://www.oracle.com/artificial-intelligence/).\\n\\n## OCI Generative AI\\n> Oracle Cloud Infrastructure (OCI) [Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/home.htm) is a fully managed service that provides a set of state-of-the-art,\\n> customizable large language models (LLMs) that cover a wide range of use cases, and which are available through a single API.\\n> Using the OCI Generative AI service you can access ready-to-use pretrained models, or create and host your own fine-tuned\\n> custom models based on your own data on dedicated AI clusters. \\n\\nTo use, you should have the latest `oci` python SDK and the langchain_community package installed.\\n\\n```bash\\npip install -U oci langchain-community\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/oci.mdx', 'file_path': 'docs/docs/integrations/providers/oci.mdx', 'file_name': 'oci.mdx', 'file_type': '.mdx'}, page_content='See [chat](/docs/integrations/llms/oci_generative_ai), [complete](/docs/integrations/chat/oci_generative_ai), and [embedding](/docs/integrations/text_embedding/oci_generative_ai) usage examples.\\n\\n```python\\nfrom langchain_community.chat_models import ChatOCIGenAI\\n\\nfrom langchain_community.llms import OCIGenAI\\n\\nfrom langchain_community.embeddings import OCIGenAIEmbeddings\\n```\\n\\n## OCI Data Science Model Deployment Endpoint\\n\\n> [OCI Data Science](https://docs.oracle.com/en-us/iaas/data-science/using/home.htm) is a\\n> fully managed and serverless platform for data science teams. Using the OCI Data Science\\n> platform you can build, train, and manage machine learning models, and then deploy them\\n> as an OCI Model Deployment Endpoint using the\\n> [OCI Data Science Model Deployment Service](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-about.htm).\\n\\nTo use, you should have the latest `oracle-ads` python SDK installed.\\n\\n```bash\\npip install -U oracle-ads\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/oci.mdx', 'file_path': 'docs/docs/integrations/providers/oci.mdx', 'file_name': 'oci.mdx', 'file_type': '.mdx'}, page_content='See [chat](/docs/integrations/chat/oci_data_science) and [complete](/docs/integrations/llms/oci_model_deployment_endpoint) usage examples.\\n\\n\\n```python\\nfrom langchain_community.chat_models import ChatOCIModelDeployment\\n\\nfrom langchain_community.llms import OCIModelDeploymentLLM\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/octoai.mdx', 'file_path': 'docs/docs/integrations/providers/octoai.mdx', 'file_name': 'octoai.mdx', 'file_type': '.mdx'}, page_content='# OctoAI\\n\\n>[OctoAI](https://docs.octoai.cloud/docs) offers easy access to efficient compute \\n> and enables users to integrate their choice of AI models into applications. \\n> The `OctoAI` compute service helps you run, tune, and scale AI applications easily.\\n\\n\\n## Installation and Setup\\n\\n- Install the `openai` Python package:\\n  ```bash\\n  pip install openai\\n  ````\\n- Register on `OctoAI` and get an API Token from [your OctoAI account page](https://octoai.cloud/settings).\\n\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/octoai).\\n\\n```python\\nfrom langchain_community.chat_models import ChatOctoAI\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/octoai).\\n\\n```python\\nfrom langchain_community.llms.octoai_endpoint import OctoAIEndpoint\\n```\\n\\n## Embedding models\\n\\n```python\\nfrom langchain_community.embeddings.octoai_embeddings import OctoAIEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ollama.mdx', 'file_path': 'docs/docs/integrations/providers/ollama.mdx', 'file_name': 'ollama.mdx', 'file_type': '.mdx'}, page_content='# Ollama\\n\\n>[Ollama](https://ollama.com/) allows you to run open-source large language models, \\n> such as [Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/), locally.\\n>\\n>`Ollama` bundles model weights, configuration, and data into a single package, defined by a Modelfile. \\n>It optimizes setup and configuration details, including GPU usage.\\n>For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.ai/library).\\n\\nSee [this guide](/docs/how_to/local_llms) for more details\\non how to use `Ollama` with LangChain.\\n\\n## Installation and Setup\\n### Ollama installation\\nFollow [these instructions](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) \\nto set up and run a local Ollama instance.\\n\\nOllama will start as a background service automatically, if this is disabled, run:\\n\\n```bash\\n# export OLLAMA_HOST=127.0.0.1 # environment variable to set ollama host\\n# export OLLAMA_PORT=11434 # environment variable to set the ollama port\\nollama serve\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/ollama.mdx', 'file_path': 'docs/docs/integrations/providers/ollama.mdx', 'file_name': 'ollama.mdx', 'file_type': '.mdx'}, page_content=\"After starting ollama, run `ollama pull <model_checkpoint>` to download a model \\nfrom the [Ollama model library](https://ollama.ai/library).\\n\\n```bash\\nollama pull llama3.1\\n```\\n\\nWe're now ready to install the `langchain-ollama` partner package and run a model.\\n\\n### Ollama LangChain partner package install\\nInstall the integration package with:\\n```bash\\npip install langchain-ollama\\n```\\n## LLM\\n\\n```python\\nfrom langchain_ollama.llms import OllamaLLM\\n```\\n\\nSee the notebook example [here](/docs/integrations/llms/ollama).\\n\\n## Chat Models\\n\\n### Chat Ollama\\n\\n```python\\nfrom langchain_ollama.chat_models import ChatOllama\\n```\\n\\nSee the notebook example [here](/docs/integrations/chat/ollama).\"), Document(metadata={'source': 'docs/docs/integrations/providers/ollama.mdx', 'file_path': 'docs/docs/integrations/providers/ollama.mdx', 'file_name': 'ollama.mdx', 'file_type': '.mdx'}, page_content='### Ollama tool calling\\n[Ollama tool calling](https://ollama.com/blog/tool-support) uses the\\nOpenAI compatible web server specification, and can be used with\\nthe default `BaseChatModel.bind_tools()` methods\\nas described [here](/docs/how_to/tool_calling/).\\nMake sure to select an ollama model that supports [tool calling](https://ollama.com/search?&c=tools).\\n\\n## Embedding models\\n\\n```python\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n```\\n\\nSee the notebook example [here](/docs/integrations/text_embedding/ollama).'), Document(metadata={'source': 'docs/docs/integrations/providers/ontotext_graphdb.mdx', 'file_path': 'docs/docs/integrations/providers/ontotext_graphdb.mdx', 'file_name': 'ontotext_graphdb.mdx', 'file_type': '.mdx'}, page_content='# Ontotext GraphDB\\n\\n>[Ontotext GraphDB](https://graphdb.ontotext.com/) is a graph database and knowledge discovery tool compliant with RDF and SPARQL.\\n\\n## Dependencies\\n\\nInstall the [rdflib](https://github.com/RDFLib/rdflib) package with\\n```bash\\npip install rdflib==7.0.0\\n```\\n\\n## Graph QA Chain\\n\\nConnect your GraphDB Database with a chat model to get insights on your data.\\n\\nSee the notebook example [here](/docs/integrations/graphs/ontotext).\\n\\n```python\\nfrom langchain_community.graphs import OntotextGraphDBGraph\\nfrom langchain.chains import OntotextGraphDBQAChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/openai.mdx', 'file_path': 'docs/docs/integrations/providers/openai.mdx', 'file_name': 'openai.mdx', 'file_type': '.mdx'}, page_content='---\\nkeywords: [openai]\\n---\\n\\n# OpenAI\\n\\nAll functionality related to OpenAI\\n\\n>[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory \\n> consisting of the non-profit `OpenAI Incorporated`\\n> and its for-profit subsidiary corporation `OpenAI Limited Partnership`. \\n> `OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI. \\n> `OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\\n\\n>The [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\\n> \\n>[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\\n\\n## Installation and Setup\\n\\nInstall the integration package with\\n```bash\\npip install langchain-openai\\n```\\n\\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\\n\\n## Chat model'), Document(metadata={'source': 'docs/docs/integrations/providers/openai.mdx', 'file_path': 'docs/docs/integrations/providers/openai.mdx', 'file_name': 'openai.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/chat/openai).\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\n```\\n\\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\\n```python\\nfrom langchain_openai import AzureChatOpenAI\\n```\\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai).\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/openai).\\n\\n```python\\nfrom langchain_openai import OpenAI\\n```\\n\\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\\n```python\\nfrom langchain_openai import AzureOpenAI\\n```\\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai).\\n\\n## Embedding Model\\n\\nSee a [usage example](/docs/integrations/text_embedding/openai)\\n\\n```python\\nfrom langchain_openai import OpenAIEmbeddings\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader).'), Document(metadata={'source': 'docs/docs/integrations/providers/openai.mdx', 'file_path': 'docs/docs/integrations/providers/openai.mdx', 'file_name': 'openai.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin).\\n\\n```python\\nfrom langchain.retrievers import ChatGPTPluginRetriever\\n```\\n\\n## Tools\\n\\n### Dall-E Image Generator\\n\\n>[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI` \\n> using deep learning methodologies to generate digital images from natural language descriptions, \\n> called \"prompts\".\\n\\n\\nSee a [usage example](/docs/integrations/tools/dalle_image_generator).\\n\\n```python\\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\\n```\\n\\n## Adapter\\n\\nSee a [usage example](/docs/integrations/adapters/openai).\\n\\n```python\\nfrom langchain.adapters import openai as lc_openai\\n```\\n\\n## Tokenizer\\n\\nThere are several places you can use the `tiktoken` tokenizer. By default, it is used to count tokens\\nfor OpenAI LLMs.'), Document(metadata={'source': 'docs/docs/integrations/providers/openai.mdx', 'file_path': 'docs/docs/integrations/providers/openai.mdx', 'file_name': 'openai.mdx', 'file_type': '.mdx'}, page_content='You can also use it to count tokens when splitting documents with \\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\nCharacterTextSplitter.from_tiktoken_encoder(...)\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/how_to/split_by_token/#tiktoken)\\n\\n## Chain\\n\\nSee a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/moderation).\\n\\n```python\\nfrom langchain.chains import OpenAIModerationChain\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/openllm.mdx', 'file_path': 'docs/docs/integrations/providers/openllm.mdx', 'file_name': 'openllm.mdx', 'file_type': '.mdx'}, page_content=\"---\\nkeywords: [openllm]\\n---\\n\\n# OpenLLM\\n\\nOpenLLM lets developers run any **open-source LLMs** as **OpenAI-compatible API** endpoints with **a single command**.\\n\\n- 🔬 Build for fast and production usages\\n- 🚂 Support llama3, qwen2, gemma, etc, and many **quantized** versions [full list](https://github.com/bentoml/openllm-models)\\n- ⛓️ OpenAI-compatible API\\n- 💬\\xa0Built-in ChatGPT like UI\\n- 🔥 Accelerated LLM decoding with state-of-the-art inference backends\\n- 🌥️ Ready for enterprise-grade cloud deployment (Kubernetes, Docker and BentoCloud)\\n\\n## Installation and Setup\\n\\nInstall the OpenLLM package via PyPI:\\n\\n```bash\\npip install openllm\\n```\\n\\n## LLM\\n\\nOpenLLM supports a wide range of open-source LLMs as well as serving users' own\\nfine-tuned LLMs. Use `openllm model` command to see all available models that\\nare pre-optimized for OpenLLM.\\n\\n## Wrappers\\n\\nThere is a OpenLLM Wrapper which supports interacting with running server with OpenLLM:\\n\\n```python\\nfrom langchain_community.llms import OpenLLM\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/openllm.mdx', 'file_path': 'docs/docs/integrations/providers/openllm.mdx', 'file_name': 'openllm.mdx', 'file_type': '.mdx'}, page_content='### Wrapper for OpenLLM server\\n\\nThis wrapper supports interacting with OpenLLM\\'s OpenAI-compatible endpoint.\\n\\nTo run a model, do:\\n\\n```bash\\nopenllm hello\\n```\\n\\nWrapper usage:\\n\\n```python\\nfrom langchain_community.llms import OpenLLM\\n\\nllm = OpenLLM(base_url=\"http://localhost:3000/v1\", api_key=\"na\")\\n\\nllm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\")\\n```\\n\\n### Usage\\n\\nFor a more detailed walkthrough of the OpenLLM Wrapper, see the\\n[example notebook](/docs/integrations/llms/openllm)'), Document(metadata={'source': 'docs/docs/integrations/providers/opensearch.mdx', 'file_path': 'docs/docs/integrations/providers/opensearch.mdx', 'file_name': 'opensearch.mdx', 'file_type': '.mdx'}, page_content='# OpenSearch\\n\\nThis page covers how to use the OpenSearch ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers.\\n\\n## Installation and Setup\\n- Install the Python package with `pip install opensearch-py`\\n## Wrappers\\n\\n### VectorStore\\n\\nThere exists a wrapper around OpenSearch vector databases, allowing you to use it as a vectorstore \\nfor semantic search using approximate vector search powered by lucene, nmslib and faiss engines \\nor using painless scripting and script scoring functions for bruteforce vector search.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\\n```\\n\\nFor a more detailed walkthrough of the OpenSearch wrapper, see [this notebook](/docs/integrations/vectorstores/opensearch)'), Document(metadata={'source': 'docs/docs/integrations/providers/openweathermap.mdx', 'file_path': 'docs/docs/integrations/providers/openweathermap.mdx', 'file_name': 'openweathermap.mdx', 'file_type': '.mdx'}, page_content='# OpenWeatherMap\\n\\n>[OpenWeatherMap](https://openweathermap.org/api/) provides all essential weather data for a specific location:\\n>- Current weather\\n>- Minute forecast for 1 hour\\n>- Hourly forecast for 48 hours\\n>- Daily forecast for 8 days\\n>- National weather alerts\\n>- Historical weather data for 40+ years back\\n\\nThis page covers how to use the `OpenWeatherMap API` within LangChain.\\n\\n## Installation and Setup\\n\\n- Install requirements with\\n```bash\\npip install pyowm\\n```\\n- Go to OpenWeatherMap and sign up for an account to get your API key [here](https://openweathermap.org/api/)\\n- Set your API key as `OPENWEATHERMAP_API_KEY` environment variable\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a OpenWeatherMapAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/openweathermap).\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/openweathermap.mdx', 'file_path': 'docs/docs/integrations/providers/openweathermap.mdx', 'file_name': 'openweathermap.mdx', 'file_type': '.mdx'}, page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"openweathermap-api\"])\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/oracleai.mdx', 'file_path': 'docs/docs/integrations/providers/oracleai.mdx', 'file_name': 'oracleai.mdx', 'file_type': '.mdx'}, page_content=\"# OracleAI Vector Search\\n\\nOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.\\nOne of the biggest benefits of Oracle AI Vector Search is that semantic search on unstructured data can be combined with relational search on business data in one single system.\\nThis is not only powerful but also significantly more effective because you don't need to add a specialized vector database, eliminating the pain of data fragmentation between multiple systems.\\n\\nIn addition, your vectors can benefit from all of Oracle Database’s most powerful features, like the following:\"), Document(metadata={'source': 'docs/docs/integrations/providers/oracleai.mdx', 'file_path': 'docs/docs/integrations/providers/oracleai.mdx', 'file_name': 'oracleai.mdx', 'file_type': '.mdx'}, page_content='* [Partitioning Support](https://www.oracle.com/database/technologies/partitioning.html)\\n * [Real Application Clusters scalability](https://www.oracle.com/database/real-application-clusters/)\\n * [Exadata smart scans](https://www.oracle.com/database/technologies/exadata/software/smartscan/)\\n * [Shard processing across geographically distributed databases](https://www.oracle.com/database/distributed-database/)\\n * [Transactions](https://docs.oracle.com/en/database/oracle/oracle-database/23/cncpt/transactions.html)\\n * [Parallel SQL](https://docs.oracle.com/en/database/oracle/oracle-database/21/vldbg/parallel-exec-intro.html#GUID-D28717E4-0F77-44F5-BB4E-234C31D4E4BA)\\n * [Disaster recovery](https://www.oracle.com/database/data-guard/)\\n * [Security](https://www.oracle.com/security/database-security/)\\n * [Oracle Machine Learning](https://www.oracle.com/artificial-intelligence/database-machine-learning/)\\n * [Oracle Graph Database](https://www.oracle.com/database/integrated-graph-database/)\\n * [Oracle Spatial and Graph](https://www.oracle.com/database/spatial/)\\n * [Oracle Blockchain](https://docs.oracle.com/en/database/oracle/oracle-database/23/arpls/dbms_blockchain_table.html#GUID-B469E277-978E-4378-A8C1-26D3FF96C9A6)\\n * [JSON](https://docs.oracle.com/en/database/oracle/oracle-database/23/adjsn/json-in-oracle-database.html)'), Document(metadata={'source': 'docs/docs/integrations/providers/oracleai.mdx', 'file_path': 'docs/docs/integrations/providers/oracleai.mdx', 'file_name': 'oracleai.mdx', 'file_type': '.mdx'}, page_content='## Document Loaders\\n\\nPlease check the [usage example](/docs/integrations/document_loaders/oracleai).\\n\\n```python\\nfrom langchain_community.document_loaders.oracleai import OracleDocLoader\\n```\\n\\n## Text Splitter\\n\\nPlease check the [usage example](/docs/integrations/document_loaders/oracleai).\\n\\n```python\\nfrom langchain_community.document_loaders.oracleai import OracleTextSplitter\\n```\\n\\n## Embeddings\\n\\nPlease check the [usage example](/docs/integrations/text_embedding/oracleai).\\n\\n```python\\nfrom langchain_community.embeddings.oracleai import OracleEmbeddings\\n```\\n\\n## Summary\\n\\nPlease check the [usage example](/docs/integrations/tools/oracleai).\\n\\n```python\\nfrom langchain_community.utilities.oracleai import OracleSummary\\n```\\n\\n## Vector Store\\n\\nPlease check the [usage example](/docs/integrations/vectorstores/oracle).\\n\\n```python\\nfrom langchain_community.vectorstores.oraclevs import OracleVS\\n```\\n\\n## End to End Demo'), Document(metadata={'source': 'docs/docs/integrations/providers/oracleai.mdx', 'file_path': 'docs/docs/integrations/providers/oracleai.mdx', 'file_name': 'oracleai.mdx', 'file_type': '.mdx'}, page_content='Please check the [Oracle AI Vector Search End-to-End Demo Guide](https://github.com/langchain-ai/langchain/blob/master/cookbook/oracleai_demo.ipynb).'), Document(metadata={'source': 'docs/docs/integrations/providers/outline.mdx', 'file_path': 'docs/docs/integrations/providers/outline.mdx', 'file_name': 'outline.mdx', 'file_type': '.mdx'}, page_content='# Outline\\n\\n> [Outline](https://www.getoutline.com/) is an open-source collaborative knowledge base platform designed for team information sharing.\\n\\n## Setup\\n\\nYou first need to [create an api key](https://www.getoutline.com/developers#section/Authentication) for your Outline instance. Then you need to set the following environment variables:\\n\\n```python\\nimport os\\n\\nos.environ[\"OUTLINE_API_KEY\"] = \"xxx\"\\nos.environ[\"OUTLINE_INSTANCE_URL\"] = \"https://app.getoutline.com\"\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/outline).\\n\\n```python\\nfrom langchain.retrievers import OutlineRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content=\"# Outlines\\n\\n>[Outlines](https://github.com/dottxt-ai/outlines) is a Python library for constrained language generation. It provides a unified interface to various language models and allows for structured generation using techniques like regex matching, type constraints, JSON schemas, and context-free grammars.\\n\\nOutlines supports multiple backends, including:\\n- Hugging Face Transformers\\n- llama.cpp\\n- vLLM\\n- MLX\\n\\nThis integration allows you to use Outlines models with LangChain, providing both LLM and chat model interfaces.\\n\\n## Installation and Setup\\n\\nTo use Outlines with LangChain, you'll need to install the Outlines library:\\n\\n```bash\\npip install outlines\\n```\\n\\nDepending on the backend you choose, you may need to install additional dependencies:\\n\\n- For Transformers: `pip install transformers torch datasets`\\n- For llama.cpp: `pip install llama-cpp-python`\\n- For vLLM: `pip install vllm`\\n- For MLX: `pip install mlx`\\n\\n## LLM\"), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='To use Outlines as an LLM in LangChain, you can use the `Outlines` class:\\n\\n```python\\nfrom langchain_community.llms import Outlines\\n```\\n\\n## Chat Models\\n\\nTo use Outlines as a chat model in LangChain, you can use the `ChatOutlines` class:\\n\\n```python\\nfrom langchain_community.chat_models import ChatOutlines\\n```\\n\\n## Model Configuration\\n\\nBoth `Outlines` and `ChatOutlines` classes share similar configuration options:\\n\\n```python\\nmodel = Outlines(\\n    model=\"meta-llama/Llama-2-7b-chat-hf\",  # Model identifier\\n    backend=\"transformers\",  # Backend to use (transformers, llamacpp, vllm, or mlxlm)\\n    max_tokens=256,  # Maximum number of tokens to generate\\n    stop=[\"\\\\n\"],  # Optional list of stop strings\\n    streaming=True,  # Whether to stream the output\\n    # Additional parameters for structured generation:\\n    regex=None,\\n    type_constraints=None,\\n    json_schema=None,\\n    grammar=None,\\n    # Additional model parameters:\\n    model_kwargs={\"temperature\": 0.7}\\n)\\n```\\n\\n### Model Identifier'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='The `model` parameter can be:\\n- A Hugging Face model name (e.g., \"meta-llama/Llama-2-7b-chat-hf\")\\n- A local path to a model\\n- For GGUF models, the format is \"repo_id/file_name\" (e.g., \"TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\")\\n\\n### Backend Options\\n\\nThe `backend` parameter specifies which backend to use:\\n- `\"transformers\"`: For Hugging Face Transformers models (default)\\n- `\"llamacpp\"`: For GGUF models using llama.cpp\\n- `\"transformers_vision\"`: For vision-language models (e.g., LLaVA)\\n- `\"vllm\"`: For models using the vLLM library\\n- `\"mlxlm\"`: For models using the MLX framework\\n\\n### Structured Generation\\n\\nOutlines provides several methods for structured generation:\\n\\n1. **Regex Matching**:\\n   ```python\\n   model = Outlines(\\n       model=\"meta-llama/Llama-2-7b-chat-hf\",\\n       regex=r\"((25[0-5]|2[0-4]\\\\d|[01]?\\\\d\\\\d?)\\\\.){3}(25[0-5]|2[0-4]\\\\d|[01]?\\\\d\\\\d?)\"\\n   )\\n   ```\\n   This will ensure the generated text matches the specified regex pattern (in this case, a valid IP address).'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='2. **Type Constraints**:\\n   ```python\\n   model = Outlines(\\n       model=\"meta-llama/Llama-2-7b-chat-hf\",\\n       type_constraints=int\\n   )\\n   ```\\n   This restricts the output to valid Python types (int, float, bool, datetime.date, datetime.time, datetime.datetime).\\n\\n3. **JSON Schema**:\\n   ```python\\n   from pydantic import BaseModel\\n\\n   class Person(BaseModel):\\n       name: str\\n       age: int\\n\\n   model = Outlines(\\n       model=\"meta-llama/Llama-2-7b-chat-hf\",\\n       json_schema=Person\\n   )\\n   ```\\n   This ensures the generated output adheres to the specified JSON schema or Pydantic model.'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='4. **Context-Free Grammar**:\\n   ```python\\n   model = Outlines(\\n       model=\"meta-llama/Llama-2-7b-chat-hf\",\\n       grammar=\"\"\"\\n           ?start: expression\\n           ?expression: term ((\"+\" | \"-\") term)*\\n           ?term: factor ((\"*\" | \"/\") factor)*\\n           ?factor: NUMBER | \"-\" factor | \"(\" expression \")\"\\n           %import common.NUMBER\\n       \"\"\"\\n   )\\n   ```\\n   This generates text that adheres to the specified context-free grammar in EBNF format.\\n\\n## Usage Examples\\n\\n### LLM Example\\n\\n```python\\nfrom langchain_community.llms import Outlines\\n\\nllm = Outlines(model=\"meta-llama/Llama-2-7b-chat-hf\", max_tokens=100)\\nresult = llm.invoke(\"Tell me a short story about a robot.\")\\nprint(result)\\n```\\n\\n### Chat Model Example\\n\\n```python\\nfrom langchain_community.chat_models import ChatOutlines\\nfrom langchain_core.messages import HumanMessage, SystemMessage'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='chat = ChatOutlines(model=\"meta-llama/Llama-2-7b-chat-hf\", max_tokens=100)\\nmessages = [\\n    SystemMessage(content=\"You are a helpful AI assistant.\"),\\n    HumanMessage(content=\"What\\'s the capital of France?\")\\n]\\nresult = chat.invoke(messages)\\nprint(result.content)\\n```\\n\\n### Streaming Example\\n\\n```python\\nfrom langchain_community.chat_models import ChatOutlines\\nfrom langchain_core.messages import HumanMessage\\n\\nchat = ChatOutlines(model=\"meta-llama/Llama-2-7b-chat-hf\", streaming=True)\\nfor chunk in chat.stream(\"Tell me a joke about programming.\"):\\n    print(chunk.content, end=\"\", flush=True)\\nprint()\\n```\\n\\n### Structured Output Example\\n\\n```python\\nfrom langchain_community.llms import Outlines\\nfrom pydantic import BaseModel\\n\\nclass MovieReview(BaseModel):\\n    title: str\\n    rating: int\\n    summary: str\\n\\nllm = Outlines(\\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\\n    json_schema=MovieReview\\n)\\nresult = llm.invoke(\"Write a short review for the movie \\'Inception\\'.\")\\nprint(result)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/outlines.mdx', 'file_path': 'docs/docs/integrations/providers/outlines.mdx', 'file_name': 'outlines.mdx', 'file_type': '.mdx'}, page_content='## Additional Features\\n\\n### Tokenizer Access\\n\\nYou can access the underlying tokenizer for the model:\\n\\n```python\\ntokenizer = llm.tokenizer\\nencoded = tokenizer.encode(\"Hello, world!\")\\ndecoded = tokenizer.decode(encoded)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/pandas.mdx', 'file_path': 'docs/docs/integrations/providers/pandas.mdx', 'file_name': 'pandas.mdx', 'file_type': '.mdx'}, page_content='# Pandas\\n\\n>[pandas](https://pandas.pydata.org) is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\\nbuilt on top of the `Python` programming language.\\n\\n## Installation and Setup\\n\\nInstall the `pandas` package using `pip`:\\n\\n```bash\\npip install pandas\\n```\\n\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/pandas_dataframe).\\n\\n```python\\nfrom langchain_community.document_loaders import DataFrameLoader\\n```\\n\\n## Toolkit\\n\\nSee a [usage example](/docs/integrations/tools/pandas).\\n\\n```python\\nfrom langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_path': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_name': 'payman-tool.mdx', 'file_type': '.mdx'}, page_content='# PaymanAI\\n\\nPaymanAI provides functionality to send and receive payments (fiat and crypto) on behalf of an AI Agent. To get started:\\n\\n1. **Sign up** at app.paymanai.com to create an AI Agent and obtain your **API Key**.\\n2. **Set** environment variables (`PAYMAN_API_SECRET` for your API Key, `PAYMAN_ENVIRONMENT` for sandbox or production).\\n\\nThis notebook gives a quick overview of integrating PaymanAI into LangChain as a tool. For complete reference, see the API documentation.\\n\\n## Overview\\n\\nThe PaymanAI integration is part of the `langchain-community` (or your custom) package. It allows you to:\\n\\n- Send payments (`send_payment`) to crypto addresses or bank accounts.\\n- Search for payees (`search_payees`).\\n- Add new payees (`add_payee`).\\n- Request money from customers with a hosted checkout link (`ask_for_money`).\\n- Check agent or customer balances (`get_balance`).\\n\\nThese can be wrapped as **LangChain Tools** for an LLM-based agent to call them automatically.\\n\\n### Integration details'), Document(metadata={'source': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_path': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_name': 'payman-tool.mdx', 'file_type': '.mdx'}, page_content='| Class | Package | Serializable | JS support | Package latest |\\n| :--- | :--- | :---: | :---: | :--- |\\n| PaymanAI | `langchain-payman-tool` | ❌ | ❌ | [PyPI Version] |\\n\\nIf you\\'re simply calling the PaymanAI SDK, you can do it directly or via the **Tool** interface in LangChain.\\n\\n## Setup\\n\\n1. **Install** the PaymanAI tool package:\\n\\n```bash\\npip install langchain-payman-tool\\n```\\n\\n2. **Install** the PaymanAI SDK:\\n```bash\\npip install paymanai\\n```\\n\\n3. **Set** environment variables:\\n```bash\\nexport PAYMAN_API_SECRET=\"YOUR_SECRET_KEY\"\\nexport PAYMAN_ENVIRONMENT=\"sandbox\"\\n```\\n\\nYour `PAYMAN_API_SECRET` should be the secret key from app.paymanai.com. The `PAYMAN_ENVIRONMENT` can be `sandbox` or `production` depending on your usage.\\n\\n## Instantiation\\n\\nHere is an example of instantiating a PaymanAI tool. If you have multiple Payman methods, you can create multiple tools.\\n\\n```python\\nfrom langchain_payman_tool.tool import PaymanAI'), Document(metadata={'source': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_path': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_name': 'payman-tool.mdx', 'file_type': '.mdx'}, page_content='# Instantiate the PaymanAI tool (example)\\ntool = PaymanAI(\\n    name=\"send_payment\",\\n    description=\"Send a payment to a specified payee.\",\\n)\\n```\\n\\n## Invocation\\n\\n### Invoke directly with args\\n\\nYou can call `tool.invoke(...)` and pass a dictionary matching the tool\\'s expected fields. For example:\\n\\n```python\\nresponse = tool.invoke({\\n    \"amount_decimal\": 10.00,\\n    \"payment_destination_id\": \"abc123\",\\n    \"customer_id\": \"cust_001\",\\n    \"memo\": \"Payment for invoice #XYZ\"\\n})\\n```\\n\\n### Invoke with ToolCall\\n\\nWhen used inside an AI workflow, the LLM might produce a `ToolCall` dict. You can simulate it as follows:\\n\\n```python\\nmodel_generated_tool_call = {\\n    \"args\": {\\n        \"amount_decimal\": 10.00,\\n        \"payment_destination_id\": \"abc123\"\\n    },\\n    \"id\": \"1\",\\n    \"name\": tool.name,\\n    \"type\": \"tool_call\",\\n}\\ntool.invoke(model_generated_tool_call)\\n```\\n\\n## Using the Tool in a Chain or Agent\\n\\nYou can bind a PaymanAI tool to a LangChain agent or chain that supports tool-calling.'), Document(metadata={'source': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_path': 'docs/docs/integrations/providers/payman-tool.mdx', 'file_name': 'payman-tool.mdx', 'file_type': '.mdx'}, page_content='## Quick Start Summary\\n\\n1. **Sign up** at app.paymanai.com to get your **API Key**.\\n2. **Install** dependencies:\\n   ```bash\\n   pip install paymanai langchain-payman-tool\\n   ```\\n3. **Export** environment variables:\\n   ```bash\\n   export PAYMAN_API_SECRET=\"YOUR_SECRET_KEY\"\\n   export PAYMAN_ENVIRONMENT=\"sandbox\"\\n   ```\\n4. **Instantiate** a PaymanAI tool, passing your desired name/description.\\n5. **Call** the tool with `.invoke(...)` or integrate it into a chain or agent.'), Document(metadata={'source': 'docs/docs/integrations/providers/permit.mdx', 'file_path': 'docs/docs/integrations/providers/permit.mdx', 'file_name': 'permit.mdx', 'file_type': '.mdx'}, page_content='# Permit\\n\\n[Permit.io](https://permit.io/) offers fine-grained access control and policy\\nenforcement. With LangChain, you can integrate Permit checks to ensure only authorized\\nusers can access or retrieve data in your LLM applications.\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-permit\\npip install permit\\n```\\n\\nSet environment variables for your Permit PDP and credentials:\\n\\n```python\\nexport PERMIT_API_KEY=\"your_permit_api_key\"\\nexport PERMIT_PDP_URL=\"http://localhost:7766\"   # or your real PDP endpoint\\n```\\n\\nMake sure your PDP is running and configured. See\\n[Permit Docs](https://docs.permit.io/sdk/python/quickstart-python/#2-setup-your-pdp-policy-decision-point-container)\\nfor policy setup.\\n\\n## Tools\\n\\nSee detail on available tools [here](/docs/integrations/tools/permit).\\n\\n## Retrievers\\n\\nSee detail on available retrievers [here](/docs/integrations/retrievers/permit).'), Document(metadata={'source': 'docs/docs/integrations/providers/perplexity.mdx', 'file_path': 'docs/docs/integrations/providers/perplexity.mdx', 'file_name': 'perplexity.mdx', 'file_type': '.mdx'}, page_content='# Perplexity\\n\\n>[Perplexity](https://www.perplexity.ai/pro) is the most powerful way to search \\n> the internet with unlimited Pro Search, upgraded AI models, unlimited file upload, \\n> image generation, and API credits.\\n>\\n> You can check a [list of available models](https://docs.perplexity.ai/docs/model-cards).\\n\\n## Installation and Setup\\n\\nInstall a Python package:\\n\\n```bash\\npip install openai\\n````\\n\\nGet your API key from [here](https://docs.perplexity.ai/docs/getting-started).\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/perplexity).\\n\\n```python\\nfrom langchain_community.chat_models import ChatPerplexity\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/petals.mdx', 'file_path': 'docs/docs/integrations/providers/petals.mdx', 'file_name': 'petals.mdx', 'file_type': '.mdx'}, page_content='# Petals\\n\\nThis page covers how to use the Petals ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Petals wrappers.\\n\\n## Installation and Setup\\n- Install with `pip install petals`\\n- Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`)\\n\\n## Wrappers\\n\\n### LLM\\n\\nThere exists an Petals LLM wrapper, which you can access with \\n```python\\nfrom langchain_community.llms import Petals\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/pg_embedding.mdx', 'file_path': 'docs/docs/integrations/providers/pg_embedding.mdx', 'file_name': 'pg_embedding.mdx', 'file_type': '.mdx'}, page_content='# Postgres Embedding\\n\\n> [pg_embedding](https://github.com/neondatabase/pg_embedding) is an open-source package for\\n> vector similarity search using `Postgres` and the `Hierarchical Navigable Small Worlds`\\n> algorithm for approximate nearest neighbor search.\\n\\n## Installation and Setup\\n\\nWe need to install several python packages.\\n\\n```bash\\npip install psycopg2-binary\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/pgembedding).\\n\\n```python\\nfrom langchain_community.vectorstores import PGEmbedding\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/pgvector.mdx', 'file_path': 'docs/docs/integrations/providers/pgvector.mdx', 'file_name': 'pgvector.mdx', 'file_type': '.mdx'}, page_content='# PGVector\\n\\nThis page covers how to use the Postgres [PGVector](https://github.com/pgvector/pgvector) ecosystem within LangChain\\nIt is broken into two parts: installation and setup, and then references to specific PGVector wrappers.\\n\\n## Installation\\n- Install the Python package with `pip install pgvector`\\n\\n\\n## Setup\\n1. The first step is to create a database with the `pgvector` extension installed.\\n\\n    Follow the steps at [PGVector Installation Steps](https://github.com/pgvector/pgvector#installation) to install the database and the extension. The docker image is the easiest way to get started.\\n\\n## Wrappers\\n\\n### VectorStore\\n\\nThere exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores.pgvector import PGVector\\n```\\n\\n### Usage'), Document(metadata={'source': 'docs/docs/integrations/providers/pgvector.mdx', 'file_path': 'docs/docs/integrations/providers/pgvector.mdx', 'file_name': 'pgvector.mdx', 'file_type': '.mdx'}, page_content='For a more detailed walkthrough of the PGVector Wrapper, see [this notebook](/docs/integrations/vectorstores/pgvector)'), Document(metadata={'source': 'docs/docs/integrations/providers/pinecone.mdx', 'file_path': 'docs/docs/integrations/providers/pinecone.mdx', 'file_name': 'pinecone.mdx', 'file_type': '.mdx'}, page_content='---\\nkeywords: [pinecone]\\n---\\n\\n# Pinecone\\n\\n>[Pinecone](https://docs.pinecone.io/docs/overview) is a vector database with broad functionality.\\n\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install langchain-pinecone\\n```\\n\\n\\n## Vector store\\n\\nThere exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain_pinecone import PineconeVectorStore\\n```\\n\\nFor a more detailed walkthrough of the Pinecone vectorstore, see [this notebook](/docs/integrations/vectorstores/pinecone)\\n\\n## Retrievers\\n\\n### Pinecone Hybrid Search\\n\\n```bash\\npip install pinecone pinecone-text\\n```\\n\\n```python\\nfrom langchain_community.retrievers import (\\n    PineconeHybridSearchRetriever,\\n)\\n```\\n\\nFor more detailed information, see [this notebook](/docs/integrations/retrievers/pinecone_hybrid_search).\\n\\n\\n### Self Query retriever\\n\\nPinecone vector store can be used as a retriever for self-querying.'), Document(metadata={'source': 'docs/docs/integrations/providers/pinecone.mdx', 'file_path': 'docs/docs/integrations/providers/pinecone.mdx', 'file_name': 'pinecone.mdx', 'file_type': '.mdx'}, page_content='For more detailed information, see [this notebook](/docs/integrations/retrievers/self_query/pinecone).'), Document(metadata={'source': 'docs/docs/integrations/providers/pipelineai.mdx', 'file_path': 'docs/docs/integrations/providers/pipelineai.mdx', 'file_name': 'pipelineai.mdx', 'file_type': '.mdx'}, page_content='# PipelineAI\\n\\nThis page covers how to use the PipelineAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific PipelineAI wrappers.\\n\\n## Installation and Setup\\n\\n- Install with `pip install pipeline-ai`\\n- Get a Pipeline Cloud api key and set it as an environment variable (`PIPELINE_API_KEY`)\\n\\n## Wrappers\\n\\n### LLM\\n\\nThere exists a PipelineAI LLM wrapper, which you can access with\\n\\n```python\\nfrom langchain_community.llms import PipelineAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_path': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_name': 'predictionguard.mdx', 'file_type': '.mdx'}, page_content='# Prediction Guard\\n\\nThis page covers how to use the Prediction Guard ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers.\\n\\nThis integration is maintained in the [langchain-predictionguard](https://github.com/predictionguard/langchain-predictionguard)\\npackage.\\n\\n## Installation and Setup\\n\\n- Install the PredictionGuard Langchain partner package:\\n```\\npip install langchain-predictionguard\\n```\\n\\n- Get a Prediction Guard API key (as described [here](https://docs.predictionguard.com/)) and set it as an environment variable (`PREDICTIONGUARD_API_KEY`)'), Document(metadata={'source': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_path': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_name': 'predictionguard.mdx', 'file_type': '.mdx'}, page_content='## Prediction Guard Langchain Integrations\\n|API|Description|Endpoint Docs| Import                                                  | Example Usage                                                                 |\\n|---|---|---|---------------------------------------------------------|-------------------------------------------------------------------------------|\\n|Chat|Build Chat Bots|[Chat](https://docs.predictionguard.com/api-reference/api-reference/chat-completions)| `from langchain_predictionguard import ChatPredictionGuard` | [ChatPredictionGuard.ipynb](/docs/integrations/chat/predictionguard)             |\\n|Completions|Generate Text|[Completions](https://docs.predictionguard.com/api-reference/api-reference/completions)| `from langchain_predictionguard import PredictionGuard` | [PredictionGuard.ipynb](/docs/integrations/llms/predictionguard)                     |\\n|Text Embedding|Embed String to Vectores|[Embeddings](https://docs.predictionguard.com/api-reference/api-reference/embeddings)| `from langchain_predictionguard import PredictionGuardEmbeddings` | [PredictionGuardEmbeddings.ipynb](/docs/integrations/text_embedding/predictionguard) |'), Document(metadata={'source': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_path': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_name': 'predictionguard.mdx', 'file_type': '.mdx'}, page_content='## Getting Started\\n\\n## Chat Models\\n\\n### Prediction Guard Chat\\n\\nSee a [usage example](/docs/integrations/chat/predictionguard)\\n\\n```python\\nfrom langchain_predictionguard import ChatPredictionGuard\\n```\\n\\n#### Usage\\n\\n```python\\n# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.\\nchat = ChatPredictionGuard(model=\"Hermes-3-Llama-3.1-8B\")\\n\\nchat.invoke(\"Tell me a joke\")\\n```\\n\\n## Embedding Models\\n\\n### Prediction Guard Embeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/predictionguard)\\n\\n```python\\nfrom langchain_predictionguard import PredictionGuardEmbeddings\\n```\\n\\n#### Usage\\n```python\\n# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.\\nembeddings = PredictionGuardEmbeddings(model=\"bridgetower-large-itm-mlm-itc\")\\n\\ntext = \"This is an embedding example.\"\\noutput = embeddings.embed_query(text)\\n```\\n\\n## LLMs\\n\\n### Prediction Guard LLM'), Document(metadata={'source': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_path': 'docs/docs/integrations/providers/predictionguard.mdx', 'file_name': 'predictionguard.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/llms/predictionguard)\\n\\n```python\\nfrom langchain_predictionguard import PredictionGuard\\n```\\n\\n#### Usage\\n```python\\n# If predictionguard_api_key is not passed, default behavior is to use the `PREDICTIONGUARD_API_KEY` environment variable.\\nllm = PredictionGuard(model=\"Hermes-2-Pro-Llama-3-8B\")\\n\\nllm.invoke(\"Tell me a joke about bears\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/promptlayer.mdx', 'file_path': 'docs/docs/integrations/providers/promptlayer.mdx', 'file_name': 'promptlayer.mdx', 'file_type': '.mdx'}, page_content=\"# PromptLayer\\n\\n>[PromptLayer](https://docs.promptlayer.com/introduction) is a platform for prompt engineering. \\n> It also helps with the LLM observability to visualize requests, version prompts, and track usage.\\n>\\n>While `PromptLayer` does have LLMs that integrate directly with LangChain (e.g. \\n> [`PromptLayerOpenAI`](https://docs.promptlayer.com/languages/langchain)), \\n> using a callback is the recommended way to integrate `PromptLayer` with LangChain.\\n\\n## Installation and Setup\\n\\nTo work with `PromptLayer`, we have to:\\n- Create a `PromptLayer` account\\n- Create an api token and set it as an environment variable (`PROMPTLAYER_API_KEY`)\\n\\nInstall a Python package:\\n\\n```bash\\npip install promptlayer\\n```\\n\\n\\n## Callback\\n\\nSee a [usage example](/docs/integrations/callbacks/promptlayer).\\n\\n```python\\nimport promptlayer  # Don't forget this import!\\nfrom langchain.callbacks import PromptLayerCallbackHandler\\n```\\n\\n\\n## LLM\\n\\nSee a [usage example](/docs/integrations/llms/promptlayer_openai).\"), Document(metadata={'source': 'docs/docs/integrations/providers/promptlayer.mdx', 'file_path': 'docs/docs/integrations/providers/promptlayer.mdx', 'file_name': 'promptlayer.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.llms import PromptLayerOpenAI\\n```\\n\\n\\n## Chat Models\\n\\nSee a [usage example](/docs/integrations/chat/promptlayer_chatopenai).\\n\\n```python\\nfrom langchain_community.chat_models import PromptLayerChatOpenAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/psychic.mdx', 'file_path': 'docs/docs/integrations/providers/psychic.mdx', 'file_name': 'psychic.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_class_name: hidden\\n---\\n\\n# Psychic\\n\\n:::warning\\nThis provider is no longer maintained, and may not work. Use with caution.\\n:::\\n\\n>[Psychic](https://www.psychic.dev/) is a platform for integrating with SaaS tools like `Notion`, `Zendesk`, \\n> `Confluence`, and `Google Drive` via OAuth and syncing documents from these applications to your SQL or vector\\n> database. You can think of it like Plaid for unstructured data. \\n\\n## Installation and Setup\\n\\n```bash\\npip install psychicapi\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/psychic.mdx', 'file_path': 'docs/docs/integrations/providers/psychic.mdx', 'file_name': 'psychic.mdx', 'file_type': '.mdx'}, page_content='Psychic is easy to set up - you import the `react` library and configure it with your `Sidekick API` key, which you get \\nfrom the [Psychic dashboard](https://dashboard.psychic.dev/). When you connect the applications, you  \\nview these connections from the dashboard and retrieve data using the server-side libraries.\\n \\n1. Create an account in the [dashboard](https://dashboard.psychic.dev/).\\n2. Use the [react library](https://docs.psychic.dev/sidekick-link) to add the Psychic link modal to your frontend react app. You will use this to connect the SaaS apps.\\n3. Once you have created a connection, you can use the `PsychicLoader` by following the [example notebook](/docs/integrations/document_loaders/psychic)\\n\\n\\n## Advantages vs Other Document Loaders'), Document(metadata={'source': 'docs/docs/integrations/providers/psychic.mdx', 'file_path': 'docs/docs/integrations/providers/psychic.mdx', 'file_name': 'psychic.mdx', 'file_type': '.mdx'}, page_content=\"1.\\t**Universal API:** Instead of building OAuth flows and learning the APIs for every SaaS app, you integrate Psychic once and leverage our universal API to retrieve data.\\n2.\\t**Data Syncs:** Data in your customers' SaaS apps can get stale fast. With Psychic you can configure webhooks to keep your documents up to date on a daily or realtime basis.\\n3.\\t**Simplified OAuth:** Psychic handles OAuth end-to-end so that you don't have to spend time creating OAuth clients for each integration, keeping access tokens fresh, and handling OAuth redirect logic.\"), Document(metadata={'source': 'docs/docs/integrations/providers/pull-md.mdx', 'file_path': 'docs/docs/integrations/providers/pull-md.mdx', 'file_name': 'pull-md.mdx', 'file_type': '.mdx'}, page_content=\"# PullMd Loader\\n\\n>[PullMd](https://pull.md/) is a service that converts web pages into Markdown format. The `langchain-pull-md` package utilizes this service to convert URLs, especially those rendered with JavaScript frameworks like React, Angular, or Vue.js, into Markdown without the need for local rendering.\\n\\n## Installation and Setup\\n\\nTo get started with `langchain-pull-md`, you need to install the package via pip:\\n\\n```bash\\npip install langchain-pull-md\\n```\\n\\nSee the [usage example](/docs/integrations/document_loaders/pull_md) for detailed integration and usage instructions.\\n\\n## Document Loader\\n\\nThe `PullMdLoader` class in `langchain-pull-md` provides an easy way to convert URLs to Markdown. It's particularly useful for loading content from modern web applications for use within LangChain's processing capabilities.\\n\\n```python\\nfrom langchain_pull_md import PullMdLoader\\n\\n# Initialize the loader with a URL of a JavaScript-rendered webpage\\nloader = PullMdLoader(url='https://example.com')\"), Document(metadata={'source': 'docs/docs/integrations/providers/pull-md.mdx', 'file_path': 'docs/docs/integrations/providers/pull-md.mdx', 'file_name': 'pull-md.mdx', 'file_type': '.mdx'}, page_content='# Load the content as a Document\\ndocuments = loader.load()\\n\\n# Access the Markdown content\\nfor document in documents:\\n    print(document.page_content)\\n```\\n\\nThis loader supports any URL and is particularly adept at handling sites built with dynamic JavaScript, making it a versatile tool for markdown extraction in data processing workflows.\\n\\n## API Reference\\n\\nFor a comprehensive guide to all available functions and their parameters, visit the [API reference](https://github.com/chigwell/langchain-pull-md).\\n\\n## Additional Resources\\n\\n- [GitHub Repository](https://github.com/chigwell/langchain-pull-md)\\n- [PyPi Package](https://pypi.org/project/langchain-pull-md/)'), Document(metadata={'source': 'docs/docs/integrations/providers/pygmalionai.mdx', 'file_path': 'docs/docs/integrations/providers/pygmalionai.mdx', 'file_name': 'pygmalionai.mdx', 'file_type': '.mdx'}, page_content='# PygmalionAI\\n\\n>[PygmalionAI](https://pygmalion.chat/) is a company supporting the \\n> open-source models by serving the inference endpoint \\n> for the [Aphrodite Engine](https://github.com/PygmalionAI/aphrodite-engine).\\n\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install aphrodite-engine\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/aphrodite).\\n\\n```python\\nfrom langchain_community.llms import Aphrodite\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/qdrant.mdx', 'file_path': 'docs/docs/integrations/providers/qdrant.mdx', 'file_name': 'qdrant.mdx', 'file_type': '.mdx'}, page_content='# Qdrant\\n\\n>[Qdrant](https://qdrant.tech/documentation/) (read: quadrant) is a vector similarity search engine. \\n> It provides a production-ready service with a convenient API to store, search, and manage \\n> points - vectors with an additional payload. `Qdrant` is tailored to extended filtering support.\\n\\n\\n## Installation and Setup\\n\\nInstall the Python partner package:\\n\\n```bash\\npip install langchain-qdrant\\n```\\n\\n## Embedding models\\n\\n### FastEmbedSparse\\n\\n```python\\nfrom langchain_qdrant import FastEmbedSparse\\n```\\n\\n### SparseEmbeddings\\n\\n```python\\nfrom langchain_qdrant import SparseEmbeddings\\n```\\n\\n## Vector Store\\n\\nThere exists a wrapper around `Qdrant` indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_qdrant import QdrantVectorStore\\n```\\n\\nFor a more detailed walkthrough of the Qdrant wrapper, see [this notebook](/docs/integrations/vectorstores/qdrant)'), Document(metadata={'source': 'docs/docs/integrations/providers/rank_bm25.mdx', 'file_path': 'docs/docs/integrations/providers/rank_bm25.mdx', 'file_name': 'rank_bm25.mdx', 'file_type': '.mdx'}, page_content='# rank_bm25\\n\\n[rank_bm25](https://github.com/dorianbrown/rank_bm25) is an open-source collection of algorithms\\ndesigned to query documents and return the most relevant ones, commonly used for creating\\nsearch engines.\\n\\nSee its [project page](https://github.com/dorianbrown/rank_bm25) for available algorithms.\\n\\n\\n## Installation and Setup\\n\\nFirst, you need to install `rank_bm25` python package.\\n\\n```bash\\npip install rank_bm25\\n```\\n\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/bm25).\\n\\n```python\\nfrom langchain_community.retrievers import BM25Retriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/reddit.mdx', 'file_path': 'docs/docs/integrations/providers/reddit.mdx', 'file_name': 'reddit.mdx', 'file_type': '.mdx'}, page_content='# Reddit\\n\\n>[Reddit](https://www.reddit.com) is an American social news aggregation, content rating, and discussion website.\\n\\n## Installation and Setup\\n\\nFirst, you need to install a python package.\\n\\n```bash\\npip install praw\\n```\\n\\nMake a [Reddit Application](https://www.reddit.com/prefs/apps/) and initialize the loader with your Reddit API credentials.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/reddit).\\n\\n\\n```python\\nfrom langchain_community.document_loaders import RedditPostsLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='# Redis\\n\\n>[Redis (Remote Dictionary Server)](https://en.wikipedia.org/wiki/Redis) is an open-source in-memory storage, \\n> used as a distributed, in-memory key–value database, cache and message broker, with optional durability. \\n> Because it holds all data in memory and because of its design, `Redis` offers low-latency reads and writes, \\n> making it particularly suitable for use cases that require a cache. Redis is the most popular NoSQL database, \\n> and one of the most popular databases overall.\\n\\nThis page covers how to use the [Redis](https://redis.com) ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Redis wrappers.\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install redis\\n```\\n\\nTo run Redis locally, you can use Docker:\\n\\n```bash\\ndocker run --name langchain-redis -d -p 6379:6379 redis redis-server --save 60 1 --loglevel warning\\n```\\n\\nTo stop the container:\\n\\n```bash\\ndocker stop langchain-redis\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='And to start it again:\\n\\n```bash\\ndocker start langchain-redis\\n```\\n\\n### Connections\\n\\nWe need a redis url connection string to connect to the database support either a stand alone Redis server\\nor a High-Availability setup with Replication and Redis Sentinels.\\n\\n#### Redis Standalone connection url\\nFor standalone `Redis` server, the official redis connection url formats can be used as describe in the python redis modules\\n\"from_url()\" method [Redis.from_url](https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url)\\n\\nExample: `redis_url = \"redis://:secret-pass@localhost:6379/0\"`\\n\\n#### Redis Sentinel connection url\\n\\nFor [Redis sentinel setups](https://redis.io/docs/management/sentinel/) the connection scheme is \"redis+sentinel\". \\nThis is an unofficial extensions to the official IANA registered protocol schemes as long as there is no connection url\\nfor Sentinels available.\\n\\nExample: `redis_url = \"redis+sentinel://:secret-pass@sentinel-host:26379/mymaster/0\"`'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='The format is  `redis+sentinel://[[username]:[password]]@[host-or-ip]:[port]/[service-name]/[db-number]`\\nwith the default values of \"service-name = mymaster\" and \"db-number = 0\" if not set explicit.\\nThe service-name is the redis server monitoring group name as configured within the Sentinel. \\n\\nThe current url format limits the connection string to one sentinel host only (no list can be given) and\\nbooth Redis server and sentinel must have the same password set (if used).\\n\\n#### Redis Cluster connection url\\n\\nRedis cluster is not supported right now for all methods requiring a \"redis_url\" parameter.\\nThe only way to use a Redis Cluster is with LangChain classes accepting a preconfigured Redis client like `RedisCache`\\n(example below).\\n\\n## Cache\\n\\nThe Cache wrapper allows for [Redis](https://redis.io) to be used as a remote, low-latency, in-memory cache for LLM prompts and responses.'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='### Standard Cache\\nThe standard cache is the Redis bread & butter of use case in production for both [open-source](https://redis.io) and [enterprise](https://redis.com) users globally.\\n\\n```python\\nfrom langchain.cache import RedisCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain.globals import set_llm_cache\\nimport redis\\n\\nredis_client = redis.Redis.from_url(...)\\nset_llm_cache(RedisCache(redis_client))\\n```\\n\\n### Semantic Cache\\nSemantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore.\\n\\n```python\\nfrom langchain.cache import RedisSemanticCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain.globals import set_llm_cache\\nimport redis\\n\\n# use any embedding provider...\\nfrom tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings\\n\\nredis_url = \"redis://localhost:6379\"'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='set_llm_cache(RedisSemanticCache(\\n    embedding=FakeEmbeddings(),\\n    redis_url=redis_url\\n))\\n```\\n\\n## VectorStore\\n\\nThe vectorstore wrapper turns Redis into a low-latency [vector database](https://redis.com/solutions/use-cases/vector-database/) for semantic search or LLM content retrieval.\\n\\n```python\\nfrom langchain_community.vectorstores import Redis\\n```\\n\\nFor a more detailed walkthrough of the Redis vectorstore wrapper, see [this notebook](/docs/integrations/vectorstores/redis).\\n\\n## Retriever\\n\\nThe Redis vector store retriever wrapper generalizes the vectorstore class to perform \\nlow-latency document retrieval. To create the retriever, simply \\ncall `.as_retriever()` on the base vectorstore class.\\n\\n## Memory\\n\\nRedis can be used to persist LLM conversations.\\n\\n### Vector Store Retriever Memory'), Document(metadata={'source': 'docs/docs/integrations/providers/redis.mdx', 'file_path': 'docs/docs/integrations/providers/redis.mdx', 'file_name': 'redis.mdx', 'file_type': '.mdx'}, page_content='For a more detailed walkthrough of the `VectorStoreRetrieverMemory` wrapper, see [this notebook](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html).\\n\\n### Chat Message History Memory\\nFor a detailed example of Redis to cache conversation message history, see [this notebook](/docs/integrations/memory/redis_chat_message_history).'), Document(metadata={'source': 'docs/docs/integrations/providers/remembrall.mdx', 'file_path': 'docs/docs/integrations/providers/remembrall.mdx', 'file_name': 'remembrall.mdx', 'file_type': '.mdx'}, page_content='# Remembrall\\n\\n>[Remembrall](https://remembrall.dev/) is a platform that gives a language model \\n> long-term memory, retrieval augmented generation, and complete observability.\\n \\n## Installation and Setup\\n\\nTo get started, [sign in with Github on the Remembrall platform](https://remembrall.dev/login) \\nand copy your [API key from the settings page](https://remembrall.dev/dashboard/settings).\\n\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/remembrall).'), Document(metadata={'source': 'docs/docs/integrations/providers/replicate.mdx', 'file_path': 'docs/docs/integrations/providers/replicate.mdx', 'file_name': 'replicate.mdx', 'file_type': '.mdx'}, page_content='# Replicate\\nThis page covers how to run models on Replicate within LangChain.\\n\\n## Installation and Setup\\n- Create a [Replicate](https://replicate.com) account. Get your API key and set it as an environment variable (`REPLICATE_API_TOKEN`)\\n- Install the [Replicate python client](https://github.com/replicate/replicate-python) with `pip install replicate`\\n\\n## Calling a model\\n\\nFind a model on the [Replicate explore page](https://replicate.com/explore), and then paste in the model name and version in this format: `owner-name/model-name:version`\\n\\nFor example, for this [dolly model](https://replicate.com/replicate/dolly-v2-12b), click on the API tab. The model name/version would be: `\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\"`\\n\\nOnly the `model` param is required, but any other model parameters can also be passed in with the format `input={model_param: value, ...}`'), Document(metadata={'source': 'docs/docs/integrations/providers/replicate.mdx', 'file_path': 'docs/docs/integrations/providers/replicate.mdx', 'file_name': 'replicate.mdx', 'file_type': '.mdx'}, page_content='For example, if we were running stable diffusion and wanted to change the image dimensions:\\n\\n```\\nReplicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={\\'image_dimensions\\': \\'512x512\\'})\\n```\\n\\n*Note that only the first output of a model will be returned.*\\nFrom here, we can initialize our model:\\n\\n```python\\nllm = Replicate(model=\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\")\\n```\\n\\nAnd run it:\\n\\n```python\\nprompt = \"\"\"\\nAnswer the following yes/no question by reasoning step by step.\\nCan a dog drive a car?\\n\"\"\"\\nllm(prompt)\\n```\\n\\nWe can call any Replicate model (not just LLMs) using this syntax. For example, we can call [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion):\\n\\n```python\\ntext2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={\\'image_dimensions\\':\\'512x512\\'})'), Document(metadata={'source': 'docs/docs/integrations/providers/replicate.mdx', 'file_path': 'docs/docs/integrations/providers/replicate.mdx', 'file_name': 'replicate.mdx', 'file_type': '.mdx'}, page_content='image_output = text2image(\"A cat riding a motorcycle by Picasso\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/roam.mdx', 'file_path': 'docs/docs/integrations/providers/roam.mdx', 'file_name': 'roam.mdx', 'file_type': '.mdx'}, page_content=\"# Roam\\n\\n>[ROAM](https://roamresearch.com/) is a note-taking tool for networked thought, designed to create a personal knowledge base.\\n \\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/roam).\\n\\n```python\\nfrom langchain_community.document_loaders import RoamLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/robocorp.mdx', 'file_path': 'docs/docs/integrations/providers/robocorp.mdx', 'file_name': 'robocorp.mdx', 'file_type': '.mdx'}, page_content='# Sema4 (fka Robocorp)\\n\\n>[Robocorp](https://robocorp.com/) helps build and operate Python workers that run seamlessly anywhere at any scale\\n\\n\\n## Installation and Setup\\n\\nYou need to install `langchain-robocorp` python package:\\n\\n```bash\\npip install langchain-robocorp\\n```\\n\\nYou will need a running instance of `Action Server` to communicate with from your agent application. \\nSee the [Robocorp Quickstart](https://github.com/robocorp/robocorp#quickstart) on how to setup Action Server and create your Actions.\\n\\nYou can bootstrap a new project using Action Server `new` command.\\n\\n```bash\\naction-server new\\ncd ./your-project-name\\naction-server start\\n```\\n\\n## Tool\\n\\n```python\\nfrom langchain_robocorp.toolkits import ActionServerRequestTool\\n```\\n\\n## Toolkit\\n\\nSee a [usage example](/docs/integrations/tools/robocorp).\\n\\n```python\\nfrom langchain_robocorp import ActionServerToolkit\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/rockset.mdx', 'file_path': 'docs/docs/integrations/providers/rockset.mdx', 'file_name': 'rockset.mdx', 'file_type': '.mdx'}, page_content='# Rockset\\n\\n>[Rockset](https://rockset.com/product/) is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index™ on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters. \\n\\n## Installation and Setup\\n\\nMake sure you have Rockset account and go to the web console to get the API key. Details can be found on [the website](https://rockset.com/docs/rest-api/).\\n\\n```bash\\npip install rockset\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/rockset).\\n\\n```python\\nfrom langchain_community.vectorstores import Rockset \\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/rockset).\\n```python\\nfrom langchain_community.document_loaders import RocksetLoader\\n```\\n\\n## Chat Message History'), Document(metadata={'source': 'docs/docs/integrations/providers/rockset.mdx', 'file_path': 'docs/docs/integrations/providers/rockset.mdx', 'file_name': 'rockset.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/memory/rockset_chat_message_history).\\n```python\\nfrom langchain_community.chat_message_histories import RocksetChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/runhouse.mdx', 'file_path': 'docs/docs/integrations/providers/runhouse.mdx', 'file_name': 'runhouse.mdx', 'file_type': '.mdx'}, page_content=\"# Runhouse\\n\\nThis page covers how to use the [Runhouse](https://github.com/run-house/runhouse) ecosystem within LangChain.\\nIt is broken into three parts: installation and setup, LLMs, and Embeddings.\\n\\n## Installation and Setup\\n- Install the Python SDK with `pip install runhouse`\\n- If you'd like to use on-demand cluster, check your cloud credentials with `sky check`\\n\\n## Self-hosted LLMs\\nFor a basic self-hosted LLM, you can use the `SelfHostedHuggingFaceLLM` class. For more\\ncustom LLMs, you can use the `SelfHostedPipeline` parent class.\\n\\n```python\\nfrom langchain_community.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\n```\\n\\nFor a more detailed walkthrough of the Self-hosted LLMs, see [this notebook](/docs/integrations/llms/runhouse)\\n\\n## Self-hosted Embeddings\\nThere are several ways to use self-hosted embeddings with LangChain via Runhouse.\"), Document(metadata={'source': 'docs/docs/integrations/providers/runhouse.mdx', 'file_path': 'docs/docs/integrations/providers/runhouse.mdx', 'file_name': 'runhouse.mdx', 'file_type': '.mdx'}, page_content='For a basic self-hosted embedding from a Hugging Face Transformers model, you can use \\nthe `SelfHostedEmbedding` class.\\n```python\\nfrom langchain_community.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\n```\\n\\nFor a more detailed walkthrough of the Self-hosted Embeddings, see [this notebook](/docs/integrations/text_embedding/self-hosted)'), Document(metadata={'source': 'docs/docs/integrations/providers/rwkv.mdx', 'file_path': 'docs/docs/integrations/providers/rwkv.mdx', 'file_name': 'rwkv.mdx', 'file_type': '.mdx'}, page_content='# RWKV-4\\n\\nThis page covers how to use the `RWKV-4` wrapper within LangChain.\\nIt is broken into two parts: installation and setup, and then usage with an example.\\n\\n## Installation and Setup\\n- Install the Python package with `pip install rwkv`\\n- Install the tokenizer Python package with `pip install tokenizer`\\n- Download a [RWKV model](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) and place it in your desired directory\\n- Download the [tokens file](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20B_tokenizer.json)\\n\\n## Usage\\n\\n### RWKV\\n\\nTo use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer\\'s configuration.\\n```python\\nfrom langchain_community.llms import RWKV\\n\\n# Test the model\\n\\n```python\\n\\ndef generate_prompt(instruction, input=None):\\n    if input:\\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.'), Document(metadata={'source': 'docs/docs/integrations/providers/rwkv.mdx', 'file_path': 'docs/docs/integrations/providers/rwkv.mdx', 'file_name': 'rwkv.mdx', 'file_type': '.mdx'}, page_content='# Instruction:\\n{instruction}\\n\\n# Input:\\n{input}\\n\\n# Response:\\n\"\"\"\\n    else:\\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n# Instruction:\\n{instruction}\\n\\n# Response:\\n\"\"\"\\n\\n\\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\\nresponse = model.invoke(generate_prompt(\"Once upon a time, \"))\\n```\\n## Model File\\n\\nYou can find links to model file downloads at the [RWKV-4-Raven](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) repository.\\n\\n### Rwkv-4 models -> recommended VRAM\\n\\n\\n```\\nRWKV VRAM\\nModel | 8bit | bf16/fp16 | fp32\\n14B   | 16GB | 28GB      | >50GB\\n7B    | 8GB  | 14GB      | 28GB\\n3B    | 2.8GB| 6GB       | 12GB\\n1b5   | 1.3GB| 3GB       | 6GB\\n```\\n\\nSee the [rwkv pip](https://pypi.org/project/rwkv/) page for more information about strategies, including streaming and cuda support.'), Document(metadata={'source': 'docs/docs/integrations/providers/salesforce.mdx', 'file_path': 'docs/docs/integrations/providers/salesforce.mdx', 'file_name': 'salesforce.mdx', 'file_type': '.mdx'}, page_content='# Salesforce\\n\\n[Salesforce](https://www.salesforce.com/) is a cloud-based software company that\\nprovides customer relationship management (CRM) solutions and a suite of enterprise\\napplications focused on sales, customer service, marketing automation, and analytics.\\n\\n[langchain-salesforce](https://pypi.org/project/langchain-salesforce/) implements\\ntools enabling LLMs to interact with Salesforce data.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-salesforce\\n```\\n\\n## Tools\\n\\nSee detail on available tools [here](/docs/integrations/tools/salesforce/).'), Document(metadata={'source': 'docs/docs/integrations/providers/salute_devices.mdx', 'file_path': 'docs/docs/integrations/providers/salute_devices.mdx', 'file_name': 'salute_devices.mdx', 'file_type': '.mdx'}, page_content=\"# Salute Devices\\n\\nSalute Devices provides GigaChat LLM's models.\\n\\nFor more info how to get access to GigaChat [follow here](https://developers.sber.ru/docs/ru/gigachat/api/integration).\\n\\n## Installation and Setup\\n\\nGigaChat package can be installed via pip from PyPI:\\n\\n```bash\\npip install langchain-gigachat\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/gigachat).\\n\\n```python\\nfrom langchain_community.llms import GigaChat\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/gigachat).\\n\\n```python\\nfrom langchain_gigachat.chat_models import GigaChat\\n```\\n\\n## Embeddings\\n\\nSee a [usage example](/docs/integrations/text_embedding/gigachat).\\n\\n```python\\nfrom langchain_gigachat.embeddings import GigaChatEmbeddings\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/sap.mdx', 'file_path': 'docs/docs/integrations/providers/sap.mdx', 'file_name': 'sap.mdx', 'file_type': '.mdx'}, page_content=\"# SAP\\n\\n>[SAP SE(Wikipedia)](https://www.sap.com/about/company.html) is a German multinational \\n> software company. It develops enterprise software to manage business operation and \\n> customer relations. The company is the world's leading \\n> `enterprise resource planning (ERP)` software vendor.\\n\\n## Installation and Setup\\n\\nWe need to install the `hdbcli` python package.\\n\\n```bash\\npip install hdbcli\\n```\\n\\n## Vectorstore\\n\\n>[SAP HANA Cloud Vector Engine](https://www.sap.com/events/teched/news-guide/ai.html#article8) is \\n> a vector store fully integrated into the `SAP HANA Cloud` database.\\n\\nSee a [usage example](/docs/integrations/vectorstores/sap_hanavector).\\n\\n```python\\nfrom langchain_community.vectorstores.hanavector import HanaDB\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/scrapegraph.mdx', 'file_path': 'docs/docs/integrations/providers/scrapegraph.mdx', 'file_name': 'scrapegraph.mdx', 'file_type': '.mdx'}, page_content='# ScrapeGraph AI\\n\\n>[ScrapeGraph AI](https://scrapegraphai.com) is a service that provides AI-powered web scraping capabilities.\\n>It offers tools for extracting structured data, converting webpages to markdown, and processing local HTML content\\n>using natural language prompts.\\n\\n## Installation and Setup\\n\\nInstall the required packages:\\n\\n```bash\\npip install langchain-scrapegraph\\n```\\n\\nSet up your API key:\\n\\n```bash\\nexport SGAI_API_KEY=\"your-scrapegraph-api-key\"\\n```\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/scrapegraph).\\n\\nThere are four tools available:\\n\\n```python\\nfrom langchain_scrapegraph.tools import (\\n    SmartScraperTool,    # Extract structured data from websites\\n    MarkdownifyTool,     # Convert webpages to markdown\\n    LocalScraperTool,    # Process local HTML content\\n    GetCreditsTool,      # Check remaining API credits\\n)\\n```\\n\\nEach tool serves a specific purpose:'), Document(metadata={'source': 'docs/docs/integrations/providers/scrapegraph.mdx', 'file_path': 'docs/docs/integrations/providers/scrapegraph.mdx', 'file_name': 'scrapegraph.mdx', 'file_type': '.mdx'}, page_content='- `SmartScraperTool`: Extract structured data from websites given a URL, prompt and optional output schema\\n- `MarkdownifyTool`: Convert any webpage to clean markdown format\\n- `LocalScraperTool`: Extract structured data from a local HTML file given a prompt and optional output schema\\n- `GetCreditsTool`: Check your remaining ScrapeGraph AI credits'), Document(metadata={'source': 'docs/docs/integrations/providers/searchapi.mdx', 'file_path': 'docs/docs/integrations/providers/searchapi.mdx', 'file_name': 'searchapi.mdx', 'file_type': '.mdx'}, page_content='# SearchApi\\n\\nThis page covers how to use the [SearchApi](https://www.searchapi.io/) Google Search API within LangChain. SearchApi is a real-time SERP API for easy SERP scraping.\\n\\n## Setup\\n\\n- Go to [https://www.searchapi.io/](https://www.searchapi.io/) to sign up for a free account\\n- Get the api key and set it as an environment variable (`SEARCHAPI_API_KEY`)\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere is a SearchApiAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities import SearchApiAPIWrapper\\n```\\n\\nYou can use it as part of a Self Ask chain:\\n\\n```python\\nfrom langchain_community.utilities import SearchApiAPIWrapper\\nfrom langchain_openai import OpenAI\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType\\n\\nimport os\\n\\nos.environ[\"SEARCHAPI_API_KEY\"] = \"\"\\nos.environ[\\'OPENAI_API_KEY\\'] = \"\"'), Document(metadata={'source': 'docs/docs/integrations/providers/searchapi.mdx', 'file_path': 'docs/docs/integrations/providers/searchapi.mdx', 'file_name': 'searchapi.mdx', 'file_type': '.mdx'}, page_content='llm = OpenAI(temperature=0)\\nsearch = SearchApiAPIWrapper()\\ntools = [\\n    Tool(\\n        name=\"Intermediate Answer\",\\n        func=search.run,\\n        description=\"useful for when you need to ask with search\"\\n    )\\n]\\n\\nself_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\\nself_ask_with_search.run(\"Who lived longer: Plato, Socrates, or Aristotle?\")\\n```\\n\\n#### Output\\n\\n```\\n> Entering new AgentExecutor chain...\\n Yes.\\nFollow up: How old was Plato when he died?\\nIntermediate answer: eighty\\nFollow up: How old was Socrates when he died?\\nIntermediate answer: | Socrates | \\n| -------- | \\n| Born | c. 470 BC Deme Alopece, Athens | \\n| Died | 399 BC (aged approximately 71) Athens | \\n| Cause of death | Execution by forced suicide by poisoning | \\n| Spouse(s) | Xanthippe, Myrto | \\n\\nFollow up: How old was Aristotle when he died?\\nIntermediate answer: 62 years\\nSo the final answer is: Plato\\n\\n> Finished chain.\\n\\'Plato\\'\\n```\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/searchapi.mdx', 'file_path': 'docs/docs/integrations/providers/searchapi.mdx', 'file_name': 'searchapi.mdx', 'file_type': '.mdx'}, page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"searchapi\"])\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/searx.mdx', 'file_path': 'docs/docs/integrations/providers/searx.mdx', 'file_name': 'searx.mdx', 'file_type': '.mdx'}, page_content='# SearxNG Search API\\n\\nThis page covers how to use the SearxNG search API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper.\\n\\n## Installation and Setup\\n\\nWhile it is possible to utilize the wrapper in conjunction with  [public searx\\ninstances](https://searx.space/) these instances frequently do not permit API\\naccess (see note on output format below) and have limitations on the frequency\\nof requests. It is recommended to opt for a self-hosted instance instead.\\n\\n### Self Hosted Instance:\\n\\nSee [this page](https://searxng.github.io/searxng/admin/installation.html) for installation instructions.'), Document(metadata={'source': 'docs/docs/integrations/providers/searx.mdx', 'file_path': 'docs/docs/integrations/providers/searx.mdx', 'file_name': 'searx.mdx', 'file_type': '.mdx'}, page_content='When you install SearxNG, the only active output format by default is the HTML format.\\nYou need to activate the `json` format to use the API. This can be done by adding the following line to the `settings.yml` file:\\n```yaml\\nsearch:\\n    formats:\\n        - html\\n        - json\\n```\\nYou can make sure that the API is working by issuing a curl request to the API endpoint:\\n\\n`curl -kLX GET --data-urlencode q=\\'langchain\\' -d format=json http://localhost:8888`\\n\\nThis should return a JSON object with the results.\\n\\n\\n## Wrappers\\n\\n### Utility\\n\\nTo use the wrapper we need to pass the host of the SearxNG instance to the wrapper with:\\n    1. the named parameter `searx_host` when creating the instance.\\n    2. exporting the environment variable `SEARXNG_HOST`.\\n\\nYou can use the wrapper to get results from a SearxNG instance. \\n\\n```python\\nfrom langchain_community.utilities import SearxSearchWrapper\\ns = SearxSearchWrapper(searx_host=\"http://localhost:8888\")\\ns.run(\"what is a large language model?\")\\n```\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/searx.mdx', 'file_path': 'docs/docs/integrations/providers/searx.mdx', 'file_name': 'searx.mdx', 'file_type': '.mdx'}, page_content='You can also load this wrapper as a Tool (to use with an Agent).\\n\\nYou can do this with:\\n\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"searx-search\"],\\n                    searx_host=\"http://localhost:8888\",\\n                    engines=[\"github\"])\\n```\\n\\nNote that we could _optionally_ pass custom engines to use.\\n\\nIf you want to obtain results with metadata as *json* you can use:\\n```python\\ntools = load_tools([\"searx-search-results-json\"],\\n                    searx_host=\"http://localhost:8888\",\\n                    num_results=5)\\n```\\n\\n#### Quickly creating tools\\n\\nThis examples showcases a quick way to create multiple tools from the same\\nwrapper.\\n\\n```python\\nfrom langchain_community.tools.searx_search.tool import SearxSearchResults\\n\\nwrapper = SearxSearchWrapper(searx_host=\"**\")\\ngithub_tool = SearxSearchResults(name=\"Github\", wrapper=wrapper,\\n                            kwargs = {\\n                                \"engines\": [\"github\"],\\n                                })'), Document(metadata={'source': 'docs/docs/integrations/providers/searx.mdx', 'file_path': 'docs/docs/integrations/providers/searx.mdx', 'file_name': 'searx.mdx', 'file_type': '.mdx'}, page_content='arxiv_tool = SearxSearchResults(name=\"Arxiv\", wrapper=wrapper,\\n                            kwargs = {\\n                                \"engines\": [\"arxiv\"]\\n                                })\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/semadb.mdx', 'file_path': 'docs/docs/integrations/providers/semadb.mdx', 'file_name': 'semadb.mdx', 'file_type': '.mdx'}, page_content='# SemaDB\\n\\n>[SemaDB](https://semafind.com/) is a no fuss vector similarity search engine. It provides a low-cost cloud hosted version to help you build AI applications with ease.\\n\\nWith SemaDB Cloud, our hosted version, no fuss means no pod size calculations, no schema definitions, no partition settings, no parameter tuning, no search algorithm tuning, no complex installation, no complex API. It is integrated with [RapidAPI](https://rapidapi.com/semafind-semadb/api/semadb) providing transparent billing, automatic sharding and an interactive API playground.\\n\\n## Installation\\n\\nNone required, get started directly with SemaDB Cloud at [RapidAPI](https://rapidapi.com/semafind-semadb/api/semadb).\\n\\n## Vector Store\\n\\nThere is a basic wrapper around `SemaDB` collections allowing you to use it as a vectorstore.\\n\\n```python\\nfrom langchain_community.vectorstores import SemaDB\\n```\\n\\nYou can follow a tutorial on how to use the wrapper in [this notebook](/docs/integrations/vectorstores/semadb).'), Document(metadata={'source': 'docs/docs/integrations/providers/serpapi.mdx', 'file_path': 'docs/docs/integrations/providers/serpapi.mdx', 'file_name': 'serpapi.mdx', 'file_type': '.mdx'}, page_content='# SerpAPI\\n\\nThis page covers how to use the SerpAPI search APIs within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper.\\n\\n## Installation and Setup\\n- Install requirements with `pip install google-search-results`\\n- Get a SerpAPI api key and either set it as an environment variable (`SERPAPI_API_KEY`)\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a SerpAPI utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities import SerpAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/serpapi).\\n\\n### Tool\\n\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"serpapi\"])\\n```\\n\\nFor more information on this, see [this page](/docs/how_to/tools_builtin)'), Document(metadata={'source': 'docs/docs/integrations/providers/singlestoredb.mdx', 'file_path': 'docs/docs/integrations/providers/singlestoredb.mdx', 'file_name': 'singlestoredb.mdx', 'file_type': '.mdx'}, page_content='# SingleStoreDB\\n\\n>[SingleStoreDB](https://singlestore.com/) is a high-performance distributed SQL database that supports deployment both in the [cloud](https://www.singlestore.com/cloud/) and on-premises. It provides vector storage, and vector functions including [dot_product](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/dot_product.html) and [euclidean_distance](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/euclidean_distance.html), thereby supporting AI applications that require text similarity matching. \\n\\n## Installation and Setup\\n\\nThere are several ways to establish a [connection](https://singlestoredb-python.labs.singlestore.com/generated/singlestoredb.connect.html) to the database. You can either set up environment variables or pass named parameters to the `SingleStoreDB constructor`. \\nAlternatively, you may provide these parameters to the `from_documents` and `from_texts` methods.'), Document(metadata={'source': 'docs/docs/integrations/providers/singlestoredb.mdx', 'file_path': 'docs/docs/integrations/providers/singlestoredb.mdx', 'file_name': 'singlestoredb.mdx', 'file_type': '.mdx'}, page_content='```bash\\npip install singlestoredb\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/singlestoredb).\\n\\n```python\\nfrom langchain_community.vectorstores import SingleStoreDB\\n```\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/singlestoredb_chat_message_history).\\n\\n```python\\nfrom langchain.memory import SingleStoreDBChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/sklearn.mdx', 'file_path': 'docs/docs/integrations/providers/sklearn.mdx', 'file_name': 'sklearn.mdx', 'file_type': '.mdx'}, page_content='# scikit-learn\\n\\n>[scikit-learn](https://scikit-learn.org/stable/) is an open-source collection of machine learning algorithms, \\n> including some implementations of the [k nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html). `SKLearnVectorStore` wraps this implementation and adds the possibility to persist the vector store in json, bson (binary json) or Apache Parquet format.\\n\\n## Installation and Setup\\n\\n- Install the Python package with `pip install scikit-learn`\\n\\n\\n## Vector Store\\n\\n`SKLearnVectorStore` provides a simple wrapper around the nearest neighbor implementation in the\\nscikit-learn package, allowing you to use it as a vectorstore.\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import SKLearnVectorStore\\n```\\n\\nFor a more detailed walkthrough of the SKLearnVectorStore wrapper, see [this notebook](/docs/integrations/vectorstores/sklearn).\\n\\n\\n## Retriever'), Document(metadata={'source': 'docs/docs/integrations/providers/sklearn.mdx', 'file_path': 'docs/docs/integrations/providers/sklearn.mdx', 'file_name': 'sklearn.mdx', 'file_type': '.mdx'}, page_content='`Support vector machines (SVMs)` are the supervised learning \\nmethods used for classification, regression and outliers detection.\\n\\nSee a [usage example](/docs/integrations/retrievers/svm).\\n\\n```python\\nfrom langchain_community.retrievers import SVMRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/slack.mdx', 'file_path': 'docs/docs/integrations/providers/slack.mdx', 'file_name': 'slack.mdx', 'file_type': '.mdx'}, page_content=\"# Slack\\n\\n>[Slack](https://slack.com/) is an instant messaging program.\\n \\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n\\n## Document loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/slack).\\n\\n```python\\nfrom langchain_community.document_loaders import SlackDirectoryLoader\\n```\\n\\n## Toolkit\\n\\nSee a [usage example](/docs/integrations/tools/slack).\\n\\n```python\\nfrom langchain_community.agent_toolkits import SlackToolkit\\n```\\n\\n## Chat loader\\n\\nSee a [usage example](/docs/integrations/chat_loaders/slack).\\n\\n```python\\nfrom langchain_community.chat_loaders.slack import SlackChatLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/snowflake.mdx', 'file_path': 'docs/docs/integrations/providers/snowflake.mdx', 'file_name': 'snowflake.mdx', 'file_type': '.mdx'}, page_content='# Snowflake\\n\\n> [Snowflake](https://www.snowflake.com/) is a cloud-based data-warehousing platform \\n> that allows you to store and query large amounts of data.\\n\\nThis page covers how to use the `Snowflake` ecosystem within `LangChain`.\\n\\n## Embedding models\\n\\nSnowflake offers their open-weight `arctic` line of embedding models for free\\non [Hugging Face](https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v1.5). The most recent model, snowflake-arctic-embed-m-v1.5 feature [matryoshka embedding](https://arxiv.org/abs/2205.13147) which allows for effective vector truncation. \\nYou can use these models via the \\n[HuggingFaceEmbeddings](/docs/integrations/text_embedding/huggingfacehub) connector:\\n\\n```shell\\npip install langchain-community sentence-transformers\\n```\\n\\n```python\\nfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nmodel = HuggingFaceEmbeddings(model_name=\"snowflake/arctic-embed-m-v1.5\")\\n```\\n\\n## Document loader'), Document(metadata={'source': 'docs/docs/integrations/providers/snowflake.mdx', 'file_path': 'docs/docs/integrations/providers/snowflake.mdx', 'file_name': 'snowflake.mdx', 'file_type': '.mdx'}, page_content='You can use the [`SnowflakeLoader`](/docs/integrations/document_loaders/snowflake) \\nto load data from Snowflake:\\n\\n```python\\nfrom langchain_community.document_loaders import SnowflakeLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/spacy.mdx', 'file_path': 'docs/docs/integrations/providers/spacy.mdx', 'file_name': 'spacy.mdx', 'file_type': '.mdx'}, page_content='# spaCy\\n\\n>[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\\n \\n## Installation and Setup\\n\\n\\n```bash\\npip install spacy\\n```\\n\\n## Text Splitter\\n\\nSee a [usage example](/docs/how_to/split_by_token/#spacy).\\n\\n```python\\nfrom langchain_text_splitters import SpacyTextSplitter\\n```\\n\\n## Text Embedding Models\\n\\nSee a [usage example](/docs/integrations/text_embedding/spacy_embedding)\\n\\n```python\\nfrom langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/spark.mdx', 'file_path': 'docs/docs/integrations/providers/spark.mdx', 'file_name': 'spark.mdx', 'file_type': '.mdx'}, page_content='# Spark\\n\\n>[Apache Spark](https://spark.apache.org/) is a unified analytics engine for \\n> large-scale data processing. It provides high-level APIs in Scala, Java, \\n> Python, and R, and an optimized engine that supports general computation \\n> graphs for data analysis. It also supports a rich set of higher-level \\n> tools including `Spark SQL` for SQL and DataFrames, `pandas API on Spark` \\n> for pandas workloads, `MLlib` for machine learning, \\n> `GraphX` for graph processing, and `Structured Streaming` for stream processing.\\n\\n## Document loaders\\n\\n### PySpark\\n\\nIt loads data from a `PySpark` DataFrame.\\n\\nSee a [usage example](/docs/integrations/document_loaders/pyspark_dataframe).\\n\\n```python\\nfrom langchain_community.document_loaders import PySparkDataFrameLoader\\n```\\n\\n## Tools/Toolkits\\n\\n### Spark SQL toolkit\\n\\nToolkit for interacting with `Spark SQL`.\\n\\nSee a [usage example](/docs/integrations/tools/spark_sql).'), Document(metadata={'source': 'docs/docs/integrations/providers/spark.mdx', 'file_path': 'docs/docs/integrations/providers/spark.mdx', 'file_name': 'spark.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.agent_toolkits import SparkSQLToolkit, create_spark_sql_agent\\nfrom langchain_community.utilities.spark_sql import SparkSQL\\n```\\n\\n#### Spark SQL individual tools\\n\\nYou can use individual tools from the Spark SQL Toolkit:\\n- `InfoSparkSQLTool`: tool for getting metadata about a Spark SQL\\n- `ListSparkSQLTool`: tool for getting tables names\\n- `QueryCheckerTool`: tool uses an LLM to check if a query is correct\\n- `QuerySparkSQLTool`: tool for querying a Spark SQL\\n\\n```python\\nfrom langchain_community.tools.spark_sql.tool import InfoSparkSQLTool\\nfrom langchain_community.tools.spark_sql.tool import ListSparkSQLTool\\nfrom langchain_community.tools.spark_sql.tool import QueryCheckerTool\\nfrom langchain_community.tools.spark_sql.tool import QuerySparkSQLTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/sparkllm.mdx', 'file_path': 'docs/docs/integrations/providers/sparkllm.mdx', 'file_name': 'sparkllm.mdx', 'file_type': '.mdx'}, page_content='# SparkLLM\\n\\n>[SparkLLM](https://xinghuo.xfyun.cn/spark) is a large-scale cognitive model independently developed by iFLYTEK.\\nIt has cross-domain knowledge and language understanding ability by learning a large amount of texts, codes and images.\\nIt can understand and perform tasks based on natural dialogue.\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/sparkllm).\\n\\n```python\\nfrom langchain_community.chat_models import ChatSparkLLM\\n```\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/sparkllm).\\n\\n```python\\nfrom langchain_community.llms import SparkLLM\\n```\\n\\n## Embedding models\\n\\nSee a [usage example](/docs/integrations/text_embedding/sparkllm)\\n\\n```python\\nfrom langchain_community.embeddings import SparkLLMTextEmbeddings\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/spreedly.mdx', 'file_path': 'docs/docs/integrations/providers/spreedly.mdx', 'file_name': 'spreedly.mdx', 'file_type': '.mdx'}, page_content='# Spreedly\\n\\n>[Spreedly](https://docs.spreedly.com/) is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at `Spreedly`, allowing you to independently store a card and then pass that card to different end points based on your business requirements.\\n \\n## Installation and Setup\\n\\nSee [setup instructions](/docs/integrations/document_loaders/spreedly).\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/spreedly).\\n\\n```python\\nfrom langchain_community.document_loaders import SpreedlyLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/sqlite.mdx', 'file_path': 'docs/docs/integrations/providers/sqlite.mdx', 'file_name': 'sqlite.mdx', 'file_type': '.mdx'}, page_content='# SQLite\\n\\n>[SQLite](https://en.wikipedia.org/wiki/SQLite) is a database engine written in the \\n> C programming language. It is not a standalone app; rather, it is a library that \\n> software developers embed in their apps. As such, it belongs to the family of \\n> embedded databases. It is the most widely deployed database engine, as it is \\n> used by several of the top web browsers, operating systems, mobile phones, and other embedded systems.\\n\\n## Installation and Setup\\n\\nWe need to install the `SQLAlchemy` python package.\\n\\n```bash\\npip install SQLAlchemy\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/sqlitevec).\\n\\n```python\\nfrom langchain_community.vectorstores import SQLiteVec\\nfrom langchain_community.vectorstores import SQLiteVSS # legacy\\n```\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/sqlite).\\n\\n```python\\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/stackexchange.mdx', 'file_path': 'docs/docs/integrations/providers/stackexchange.mdx', 'file_name': 'stackexchange.mdx', 'file_type': '.mdx'}, page_content='# Stack Exchange\\n\\n>[Stack Exchange](https://en.wikipedia.org/wiki/Stack_Exchange) is a network of \\nquestion-and-answer (Q&A) websites on topics in diverse fields, each site covering \\na specific topic, where questions, answers, and users are subject to a reputation award process.\\n\\nThis page covers how to use the `Stack Exchange API` within LangChain.\\n\\n## Installation and Setup\\n- Install requirements with \\n```bash\\npip install stackapi\\n```\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a StackExchangeAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities import StackExchangeAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/stackexchange).\\n\\n### Tool\\n\\nYou can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"stackexchange\"])\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/stackexchange.mdx', 'file_path': 'docs/docs/integrations/providers/stackexchange.mdx', 'file_name': 'stackexchange.mdx', 'file_type': '.mdx'}, page_content='For more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/starrocks.mdx', 'file_path': 'docs/docs/integrations/providers/starrocks.mdx', 'file_name': 'starrocks.mdx', 'file_type': '.mdx'}, page_content='# StarRocks\\n\\n>[StarRocks](https://www.starrocks.io/) is a High-Performance Analytical Database.\\n`StarRocks` is a next-gen sub-second MPP database for full analytics scenarios, including multi-dimensional analytics, real-time analytics and ad-hoc query.\\n\\n>Usually `StarRocks` is categorized into OLAP, and it has showed excellent performance in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install pymysql\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/starrocks).\\n\\n```python\\nfrom langchain_community.vectorstores import StarRocks\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/stochasticai.mdx', 'file_path': 'docs/docs/integrations/providers/stochasticai.mdx', 'file_name': 'stochasticai.mdx', 'file_type': '.mdx'}, page_content='# StochasticAI\\n\\nThis page covers how to use the StochasticAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers.\\n\\n## Installation and Setup\\n- Install with `pip install stochasticx`\\n- Get an StochasticAI api key and set it as an environment variable (`STOCHASTICAI_API_KEY`)\\n\\n## Wrappers\\n\\n### LLM\\n\\nThere exists an StochasticAI LLM wrapper, which you can access with \\n```python\\nfrom langchain_community.llms import StochasticAI\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/streamlit.mdx', 'file_path': 'docs/docs/integrations/providers/streamlit.mdx', 'file_name': 'streamlit.mdx', 'file_type': '.mdx'}, page_content='# Streamlit\\n\\n>[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\\n>`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front‑end experience required.\\n>See more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\\n\\n## Installation and Setup\\n\\nWe need to install the  `streamlit` Python package:\\n\\n```bash\\npip install streamlit\\n```\\n\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history).\\n\\n```python\\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\\n```\\n\\n## Callbacks\\n\\nSee a [usage example](/docs/integrations/callbacks/streamlit).\\n\\n```python\\nfrom langchain_community.callbacks import StreamlitCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/stripe.mdx', 'file_path': 'docs/docs/integrations/providers/stripe.mdx', 'file_name': 'stripe.mdx', 'file_type': '.mdx'}, page_content='# Stripe\\n\\n>[Stripe](https://stripe.com/en-ca) is an Irish-American financial services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications.\\n\\n\\n## Installation and Setup\\n\\nSee [setup instructions](/docs/integrations/document_loaders/stripe).\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/stripe).\\n\\n```python\\nfrom langchain_community.document_loaders import StripeLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/supabase.mdx', 'file_path': 'docs/docs/integrations/providers/supabase.mdx', 'file_name': 'supabase.mdx', 'file_type': '.mdx'}, page_content='# Supabase (Postgres)\\n\\n>[Supabase](https://supabase.com/docs) is an open-source `Firebase` alternative. \\n> `Supabase` is built on top of `PostgreSQL`, which offers strong `SQL` \\n> querying capabilities and enables a simple interface with already-existing tools and frameworks.\\n\\n>[PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) also known as `Postgres`,\\n> is a free and open-source relational database management system (RDBMS) \\n> emphasizing extensibility and `SQL` compliance.\\n\\n## Installation and Setup\\n\\nWe need to install `supabase` python package.\\n\\n```bash\\npip install supabase\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/supabase).\\n\\n```python\\nfrom langchain_community.vectorstores import SupabaseVectorStore\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/symblai_nebula.mdx', 'file_path': 'docs/docs/integrations/providers/symblai_nebula.mdx', 'file_name': 'symblai_nebula.mdx', 'file_type': '.mdx'}, page_content=\"# Nebula\\n\\nThis page covers how to use [Nebula](https://symbl.ai/nebula), [Symbl.ai](https://symbl.ai/)'s LLM, ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Nebula wrappers.\\n\\n## Installation and Setup\\n\\n- Get an [Nebula API Key](https://info.symbl.ai/Nebula_Private_Beta.html) and set as environment variable `NEBULA_API_KEY`\\n- Please see the [Nebula documentation](https://docs.symbl.ai/docs/nebula-llm) for more details.\\n\\n### LLM\\n\\nThere exists an Nebula LLM wrapper, which you can access with\\n```python\\nfrom langchain_community.llms import Nebula\\nllm = Nebula()\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/tableau.mdx', 'file_path': 'docs/docs/integrations/providers/tableau.mdx', 'file_name': 'tableau.mdx', 'file_type': '.mdx'}, page_content='# Tableau\\n\\n[Tableau](https://www.tableau.com/) is an analytics platform that enables anyone to\\nsee and understand data. \\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install langchain-tableau\\n```\\n\\n## Tools\\n\\nSee detail on available tools [here](/docs/integrations/tools/tableau).'), Document(metadata={'source': 'docs/docs/integrations/providers/taiga.mdx', 'file_path': 'docs/docs/integrations/providers/taiga.mdx', 'file_name': 'taiga.mdx', 'file_type': '.mdx'}, page_content='# Taiga\\n\\n> [Taiga](https://docs.taiga.io/) is an open-source project management platform designed for agile teams, offering features like Kanban, Scrum, and issue tracking.\\n\\n## Installation and Setup\\n\\nInstall the `langchain-taiga` package:\\n\\n```bash\\npip install langchain-taiga\\n```\\n\\nYou must provide a logins via environment variable so the tools can authenticate.\\n\\n```bash\\nexport TAIGA_URL=\"https://taiga.xyz.org/\"\\nexport TAIGA_API_URL=\"https://taiga.xyz.org/\"\\nexport TAIGA_USERNAME=\"username\"\\nexport TAIGA_PASSWORD=\"pw\"\\nexport OPENAI_API_KEY=\"OPENAI_API_KEY\"\\n```\\n\\n\\n---\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/taiga)\\n\\n---\\n\\n## Toolkit\\n\\n`TaigaToolkit` groups multiple Taiga-related tools into a single interface.\\n\\n```python\\nfrom langchain_taiga.toolkits import TaigaToolkit\\n\\ntoolkit = TaigaToolkit()\\ntools = toolkit.get_tools()\\n\\n```\\n\\n---\\n\\n## Future Integrations'), Document(metadata={'source': 'docs/docs/integrations/providers/taiga.mdx', 'file_path': 'docs/docs/integrations/providers/taiga.mdx', 'file_name': 'taiga.mdx', 'file_type': '.mdx'}, page_content='Check the [Taiga Developer Docs](https://docs.taiga.io/) for more information, and watch for updates or advanced usage examples in the [langchain_taiga GitHub repo](https://github.com/Shikenso-Analytics/langchain-taiga).'), Document(metadata={'source': 'docs/docs/integrations/providers/tair.mdx', 'file_path': 'docs/docs/integrations/providers/tair.mdx', 'file_name': 'tair.mdx', 'file_type': '.mdx'}, page_content='# Tair\\n\\n>[Alibaba Cloud Tair](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair) is a cloud native in-memory database service \\n> developed by `Alibaba Cloud`. It provides rich data models and enterprise-grade capabilities to \\n> support your real-time online scenarios while maintaining full compatibility with open-source `Redis`. \\n> `Tair` also introduces persistent memory-optimized instances that are based on \\n> new non-volatile memory (NVM) storage medium.\\n\\n## Installation and Setup\\n\\nInstall Tair Python SDK:\\n\\n```bash\\npip install tair\\n```\\n\\n## Vector Store\\n\\n```python\\nfrom langchain_community.vectorstores import Tair\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/tair).'), Document(metadata={'source': 'docs/docs/integrations/providers/telegram.mdx', 'file_path': 'docs/docs/integrations/providers/telegram.mdx', 'file_name': 'telegram.mdx', 'file_type': '.mdx'}, page_content='# Telegram\\n\\n>[Telegram Messenger](https://web.telegram.org/a/) is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. The application also provides optional end-to-end encrypted chats and video calling, VoIP, file sharing and several other features.\\n\\n\\n## Installation and Setup\\n\\nSee [setup instructions](/docs/integrations/document_loaders/telegram).\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/telegram).\\n\\n```python\\nfrom langchain_community.document_loaders import TelegramChatFileLoader\\nfrom langchain_community.document_loaders import TelegramChatApiLoader\\n```\\n\\n## Chat loader\\n\\nSee a [usage example](/docs/integrations/chat_loaders/telegram).\\n\\n```python\\nfrom langchain_community.chat_loaders.telegram import TelegramChatLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/tencent.mdx', 'file_path': 'docs/docs/integrations/providers/tencent.mdx', 'file_name': 'tencent.mdx', 'file_type': '.mdx'}, page_content=\"# Tencent\\n\\n>[Tencent Holdings Ltd. (Wikipedia)](https://en.wikipedia.org/wiki/Tencent) (Chinese: 腾讯; pinyin: Téngxùn) \\n> is a Chinese multinational technology conglomerate and holding company headquartered \\n> in Shenzhen. `Tencent` is one of the highest grossing multimedia companies in the \\n> world based on revenue. It is also the world's largest company in the video game industry\\n> based on its equity investments.\\n \\n\\n## Chat model \\n\\n>[Tencent's hybrid model API](https://cloud.tencent.com/document/product/1729) (`Hunyuan API`) \\n> implements dialogue communication, content generation, \\n> analysis and understanding, and can be widely used in various scenarios such as intelligent \\n> customer service, intelligent marketing, role playing, advertising, copyrighting, product description,\\n> script creation, resume generation, article writing, code generation, data analysis, and content\\n> analysis.\\n\\n\\nFor more information, see [this notebook](/docs/integrations/chat/tencent_hunyuan)\"), Document(metadata={'source': 'docs/docs/integrations/providers/tencent.mdx', 'file_path': 'docs/docs/integrations/providers/tencent.mdx', 'file_name': 'tencent.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.chat_models import ChatHunyuan\\n```\\n\\n\\n## Document Loaders\\n\\n### Tencent COS\\n\\n>[Tencent Cloud Object Storage (COS)](https://www.tencentcloud.com/products/cos) is a distributed \\n> storage service that enables you to store any amount of data from anywhere via HTTP/HTTPS protocols. \\n> `COS` has no restrictions on data structure or format. It also has no bucket size limit and \\n> partition management, making it suitable for virtually any use case, such as data delivery, \\n> data processing, and data lakes. COS provides a web-based console, multi-language SDKs and APIs, \\n> command line tool, and graphical tools. It works well with Amazon S3 APIs, allowing you to quickly \\n> access community tools and plugins.\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install cos-python-sdk-v5\\n```\\n\\n#### Tencent COS Directory\\n\\nFor more information, see [this notebook](/docs/integrations/document_loaders/tencent_cos_directory)'), Document(metadata={'source': 'docs/docs/integrations/providers/tencent.mdx', 'file_path': 'docs/docs/integrations/providers/tencent.mdx', 'file_name': 'tencent.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import TencentCOSDirectoryLoader\\nfrom qcloud_cos import CosConfig\\n```\\n\\n#### Tencent COS File\\n\\nFor more information, see [this notebook](/docs/integrations/document_loaders/tencent_cos_file)\\n\\n```python\\nfrom langchain_community.document_loaders import TencentCOSFileLoader\\nfrom qcloud_cos import CosConfig\\n```\\n\\n## Vector Store\\n\\n### Tencent VectorDB'), Document(metadata={'source': 'docs/docs/integrations/providers/tencent.mdx', 'file_path': 'docs/docs/integrations/providers/tencent.mdx', 'file_name': 'tencent.mdx', 'file_type': '.mdx'}, page_content=\">[Tencent Cloud VectorDB](https://www.tencentcloud.com/products/vdb) is a fully managed, \\n> self-developed enterprise-level distributed database service\\n>dedicated to storing, retrieving, and analyzing multidimensional vector data. The database supports a variety of index\\n>types and similarity calculation methods, and a single index supports 1 billion vectors, millions of QPS, and\\n>millisecond query latency. `Tencent Cloud Vector Database` can not only provide an external knowledge base for large\\n>models and improve the accuracy of large models' answers, but also be widely used in AI fields such as\\n>recommendation systems, NLP services, computer vision, and intelligent customer service.\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install tcvectordb\\n```\\n\\nFor more information, see [this notebook](/docs/integrations/vectorstores/tencentvectordb)\\n\\n```python\\nfrom langchain_community.vectorstores import TencentVectorDB\\n```\\n\\n## Chat loader\\n\\n### WeChat\"), Document(metadata={'source': 'docs/docs/integrations/providers/tencent.mdx', 'file_path': 'docs/docs/integrations/providers/tencent.mdx', 'file_name': 'tencent.mdx', 'file_type': '.mdx'}, page_content='>[WeChat](https://www.wechat.com/) or `Weixin` in Chinese is a Chinese \\n> instant messaging, social media, and mobile payment app developed by `Tencent`.\\n\\nSee a [usage example](/docs/integrations/chat_loaders/wechat).'), Document(metadata={'source': 'docs/docs/integrations/providers/tensorflow_datasets.mdx', 'file_path': 'docs/docs/integrations/providers/tensorflow_datasets.mdx', 'file_name': 'tensorflow_datasets.mdx', 'file_type': '.mdx'}, page_content='# TensorFlow Datasets\\n\\n>[TensorFlow Datasets](https://www.tensorflow.org/datasets) is a collection of datasets ready to use, \\n> with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed \\n> as [tf.data.Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), \\n> enabling easy-to-use and high-performance input pipelines. To get started see \\n> the [guide](https://www.tensorflow.org/datasets/overview) and \\n> the [list of datasets](https://www.tensorflow.org/datasets/catalog/overview#all_datasets).\\n\\n## Installation and Setup\\n\\nYou need to install `tensorflow` and `tensorflow-datasets` python packages.\\n\\n```bash\\npip install tensorflow\\n```\\n\\n```bash\\npip install tensorflow-dataset\\n```\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/tensorflow_datasets).\\n\\n```python\\nfrom langchain_community.document_loaders import TensorflowDatasetLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/tidb.mdx', 'file_path': 'docs/docs/integrations/providers/tidb.mdx', 'file_name': 'tidb.mdx', 'file_type': '.mdx'}, page_content='# TiDB\\n\\n> [TiDB Cloud](https://www.pingcap.com/tidb-serverless), is a comprehensive Database-as-a-Service (DBaaS) solution,\\n> that provides dedicated and serverless options. `TiDB Serverless` is now integrating \\n> a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly \\n> develop AI applications using `TiDB Serverless` without the need for a new database or additional \\n> technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.\\n\\n\\n## Installation and Setup\\n\\nYou have to get the connection details for the TiDB database. \\nVisit the [TiDB Cloud](https://tidbcloud.com/) to get the connection details.\\n\\n```bash\\n## Document loader\\n\\n```python\\nfrom langchain_community.document_loaders import TiDBLoader\\n```\\n\\nPlease refer the details [here](/docs/integrations/document_loaders/tidb).\\n\\n## Vector store'), Document(metadata={'source': 'docs/docs/integrations/providers/tidb.mdx', 'file_path': 'docs/docs/integrations/providers/tidb.mdx', 'file_name': 'tidb.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.vectorstores import TiDBVectorStore\\n```\\nPlease refer the details [here](/docs/integrations/vectorstores/tidb_vector).\\n\\n\\n## Memory\\n\\n```python\\nfrom langchain_community.chat_message_histories import TiDBChatMessageHistory\\n```\\n\\nPlease refer the details [here](/docs/integrations/memory/tidb_chat_message_history).'), Document(metadata={'source': 'docs/docs/integrations/providers/tigergraph.mdx', 'file_path': 'docs/docs/integrations/providers/tigergraph.mdx', 'file_name': 'tigergraph.mdx', 'file_type': '.mdx'}, page_content='# TigerGraph\\n\\n>[TigerGraph](https://www.tigergraph.com/tigergraph-db/) is a natively distributed and high-performance graph database.\\n> The storage of data in a graph format of vertices and edges leads to rich relationships, \\n> ideal for grouding LLM responses.\\n\\n## Installation and Setup\\n\\nFollow instructions [how to connect to the `TigerGraph` database](https://docs.tigergraph.com/pytigergraph/current/getting-started/connection).\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install pyTigerGraph\\n```\\n\\n## Graph store\\n\\n### TigerGraph\\n\\nSee a [usage example](/docs/integrations/graphs/tigergraph).\\n\\n```python\\nfrom langchain_community.graphs import TigerGraph\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/tigris.mdx', 'file_path': 'docs/docs/integrations/providers/tigris.mdx', 'file_name': 'tigris.mdx', 'file_type': '.mdx'}, page_content='# Tigris\\n\\n> [Tigris](https://tigrisdata.com) is an open-source Serverless NoSQL Database and Search Platform designed to simplify building high-performance vector search applications.\\n> `Tigris` eliminates the infrastructure complexity of managing, operating, and synchronizing multiple tools, allowing you to focus on building great applications instead.\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install tigrisdb openapi-schema-pydantic\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/tigris).\\n\\n```python\\nfrom langchain_community.vectorstores import Tigris\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/tomarkdown.mdx', 'file_path': 'docs/docs/integrations/providers/tomarkdown.mdx', 'file_name': 'tomarkdown.mdx', 'file_type': '.mdx'}, page_content='# 2Markdown\\n\\n>[2markdown](https://2markdown.com/) service transforms website content into structured markdown files.\\n\\n\\n## Installation and Setup\\n\\nWe need the `API key`. See [instructions how to get it](https://2markdown.com/login).\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/tomarkdown).\\n\\n```python\\nfrom langchain_community.document_loaders import ToMarkdownLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/transwarp.mdx', 'file_path': 'docs/docs/integrations/providers/transwarp.mdx', 'file_name': 'transwarp.mdx', 'file_type': '.mdx'}, page_content='# Transwarp\\n\\n>[Transwarp](https://www.transwarp.cn/en/introduction) aims to build \\n> enterprise-level big data and AI infrastructure software, \\n> to shape the future of data world. It provides enterprises with \\n> infrastructure software and services around the whole data lifecycle, \\n> including integration, storage, governance, modeling, analysis, \\n> mining and circulation. \\n> \\n> `Transwarp` focuses on technology research and \\n> development and has accumulated core technologies in these aspects: \\n> distributed computing, SQL compilations, database technology, \\n> unification for multi-model data management, container-based cloud computing, \\n> and big data analytics and intelligence.\\n\\n## Installation\\n\\nYou have to install several python packages:\\n\\n```bash\\npip install -U tiktoken hippo-api\\n```\\n\\nand get the connection configuration.\\n\\n## Vector stores\\n\\n### Hippo\\n\\nSee [a usage example and installation instructions](/docs/integrations/vectorstores/hippo).'), Document(metadata={'source': 'docs/docs/integrations/providers/transwarp.mdx', 'file_path': 'docs/docs/integrations/providers/transwarp.mdx', 'file_name': 'transwarp.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.vectorstores.hippo import Hippo\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/trello.mdx', 'file_path': 'docs/docs/integrations/providers/trello.mdx', 'file_name': 'trello.mdx', 'file_type': '.mdx'}, page_content='# Trello\\n\\n>[Trello](https://www.atlassian.com/software/trello) is a web-based project management and collaboration tool that allows individuals and teams to organize and track their tasks and projects. It provides a visual interface known as a \"board\" where users can create lists and cards to represent their tasks and activities.\\n>The TrelloLoader allows us to load cards from a `Trello` board.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install py-trello beautifulsoup4\\n```\\n\\nSee [setup instructions](/docs/integrations/document_loaders/trello).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/trello).\\n\\n```python\\nfrom langchain_community.document_loaders import TrelloLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/trubrics.mdx', 'file_path': 'docs/docs/integrations/providers/trubrics.mdx', 'file_name': 'trubrics.mdx', 'file_type': '.mdx'}, page_content='# Trubrics\\n\\n\\n>[Trubrics](https://trubrics.com) is an LLM user analytics platform that lets you collect, analyse and manage user\\nprompts & feedback on AI models.\\n>\\n>Check out [Trubrics repo](https://github.com/trubrics/trubrics-sdk) for more information on `Trubrics`.\\n\\n## Installation and Setup\\n\\nWe need to install the  `trubrics` Python package:\\n\\n```bash\\npip install trubrics\\n```\\n\\n\\n## Callbacks\\n\\nSee a [usage example](/docs/integrations/callbacks/trubrics).\\n\\n```python\\nfrom langchain.callbacks import TrubricsCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/trulens.mdx', 'file_path': 'docs/docs/integrations/providers/trulens.mdx', 'file_name': 'trulens.mdx', 'file_type': '.mdx'}, page_content=\"# TruLens\\n\\n>[TruLens](https://trulens.org) is an [open-source](https://github.com/truera/trulens) package that provides instrumentation and evaluation tools for large language model (LLM) based applications.\\n\\nThis page covers how to use [TruLens](https://trulens.org) to evaluate and track LLM apps built on langchain.\\n\\n\\n## Installation and Setup\\n\\nInstall the `trulens-eval` python package.\\n\\n```bash\\npip install trulens-eval\\n```\\n\\n## Quickstart\\n\\nSee the integration details in the [TruLens documentation](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/).\\n\\n### Tracking\\n\\nOnce you've created your LLM chain, you can use TruLens for evaluation and tracking. \\nTruLens has a number of [out-of-the-box Feedback Functions](https://www.trulens.org/trulens_eval/evaluation/feedback_functions/), \\nand is also an extensible framework for LLM evaluation.\\n\\nCreate the feedback functions:\\n\\n```python\\nfrom trulens_eval.feedback import Feedback, Huggingface,\"), Document(metadata={'source': 'docs/docs/integrations/providers/trulens.mdx', 'file_path': 'docs/docs/integrations/providers/trulens.mdx', 'file_name': 'trulens.mdx', 'file_type': '.mdx'}, page_content=\"# Initialize HuggingFace-based feedback function collection class:\\nhugs = Huggingface()\\nopenai = OpenAI()\\n\\n# Define a language match feedback function using HuggingFace.\\nlang_match = Feedback(hugs.language_match).on_input_output()\\n# By default this will check language match on the main app input and main app\\n# output.\\n\\n# Question/answer relevance between overall question and answer.\\nqa_relevance = Feedback(openai.relevance).on_input_output()\\n# By default this will evaluate feedback on main app input and main app output.\\n\\n# Toxicity of input\\ntoxicity = Feedback(openai.toxicity).on_input()\\n```\\n\\n### Chains\\n\\nAfter you've set up Feedback Function(s) for evaluating your LLM, you can wrap your application with \\nTruChain to get detailed tracing, logging and evaluation of your LLM app.\\n\\nNote: See code for the `chain` creation is in \\nthe [TruLens documentation](https://www.trulens.org/trulens_eval/getting_started/quickstarts/langchain_quickstart/).\\n\\n```python\\nfrom trulens_eval import TruChain\"), Document(metadata={'source': 'docs/docs/integrations/providers/trulens.mdx', 'file_path': 'docs/docs/integrations/providers/trulens.mdx', 'file_name': 'trulens.mdx', 'file_type': '.mdx'}, page_content='# wrap your chain with TruChain\\ntruchain = TruChain(\\n    chain,\\n    app_id=\\'Chain1_ChatApplication\\',\\n    feedbacks=[lang_match, qa_relevance, toxicity]\\n)\\n# Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used.\\ntruchain(\"que hora es?\")\\n```\\n\\n### Evaluation\\n\\nNow you can explore your LLM-based application!\\n\\nDoing so will help you understand how your LLM application is performing at a glance. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you\\'ve set up. You\\'ll also be able to view evaluations at a record level, and explore the chain metadata for each record.\\n\\n```python\\nfrom trulens_eval import Tru\\n\\ntru = Tru()\\ntru.run_dashboard() # open a Streamlit app to explore\\n```\\n\\nFor more information on TruLens, visit [trulens.org](https://www.trulens.org/)'), Document(metadata={'source': 'docs/docs/integrations/providers/twitter.mdx', 'file_path': 'docs/docs/integrations/providers/twitter.mdx', 'file_name': 'twitter.mdx', 'file_type': '.mdx'}, page_content='# Twitter\\n\\n>[Twitter](https://twitter.com/) is an online social media and social networking service.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install tweepy\\n```\\n\\nWe must initialize the loader with the `Twitter API` token, and we need to set up the Twitter `username`.\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/twitter).\\n\\n```python\\nfrom langchain_community.document_loaders import TwitterTweetLoader\\n```\\n\\n## Chat loader\\n\\nSee a [usage example](/docs/integrations/chat_loaders/twitter).'), Document(metadata={'source': 'docs/docs/integrations/providers/typesense.mdx', 'file_path': 'docs/docs/integrations/providers/typesense.mdx', 'file_name': 'typesense.mdx', 'file_type': '.mdx'}, page_content='# Typesense\\n\\n> [Typesense](https://typesense.org) is an open-source, in-memory search engine, that you can either \\n> [self-host](https://typesense.org/docs/guide/install-typesense.html#option-2-local-machine-self-hosting) or run \\n> on [Typesense Cloud](https://cloud.typesense.org/).\\n> `Typesense` focuses on performance by storing the entire index in RAM (with a backup on disk) and also \\n> focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults.\\n\\n## Installation and Setup\\n\\n\\n```bash\\npip install typesense openapi-schema-pydantic\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/typesense).\\n\\n```python\\nfrom langchain_community.vectorstores import Typesense\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='# Unstructured\\n\\n>The `unstructured` package from\\n[Unstructured.IO](https://www.unstructured.io/) extracts clean text from raw source documents like\\nPDFs and Word documents.\\nThis page covers how to use the [`unstructured`](https://github.com/Unstructured-IO/unstructured)\\necosystem within LangChain.\\n\\n## Installation and Setup\\n\\nIf you are using a loader that runs locally, use the following steps to get `unstructured` and its\\ndependencies running.'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content=\"- For the smallest installation footprint and to take advantage of features not available in the\\n  open-source `unstructured` package, install the Python SDK with `pip install unstructured-client`\\n  along with `pip install langchain-unstructured` to use the `UnstructuredLoader` and partition\\n  remotely against the Unstructured API. This loader lives\\n  in a LangChain partner repo instead of the `langchain-community` repo and you will need an\\n  `api_key`, which you can generate a free key [here](https://unstructured.io/api-key/).\\n    - Unstructured's documentation for the sdk can be found here:\\n      https://docs.unstructured.io/api-reference/api-services/sdk\"), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='- To run everything locally, install the open-source python package with `pip install unstructured`\\n  along with `pip install langchain-community` and use the same `UnstructuredLoader` as mentioned above.\\n    - You can install document specific dependencies with extras, e.g. `pip install \"unstructured[docx]\"`. Learn more about extras [here](https://docs.unstructured.io/open-source/installation/full-installation).\\n    - To install the dependencies for all document types, use `pip install \"unstructured[all-docs]\"`.\\n- Install the following system dependencies if they are not already available on your system with e.g. `brew install` for Mac.\\n  Depending on what document types you\\'re parsing, you may not need all of these.\\n    - `libmagic-dev` (filetype detection)\\n    - `poppler-utils` (images and PDFs)\\n    - `tesseract-ocr`(images and PDFs)\\n    - `qpdf` (PDFs)\\n    - `libreoffice` (MS Office docs)\\n    - `pandoc` (EPUBs)\\n- When running locally, Unstructured also recommends using Docker [by following this\\n  guide](https://docs.unstructured.io/open-source/installation/docker-installation) to ensure all\\n  system dependencies are installed correctly.'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content=\"The Unstructured API requires API keys to make requests.\\nYou can request an API key [here](https://unstructured.io/api-key-hosted) and start using it today!\\nCheckout the README [here](https://github.com/Unstructured-IO/unstructured-api) here to get started making API calls.\\nWe'd love to hear your feedback, let us know how it goes in our [community slack](https://join.slack.com/t/unstructuredw-kbe4326/shared_invite/zt-1x7cgo0pg-PTptXWylzPQF9xZolzCnwQ).\\nAnd stay tuned for improvements to both quality and performance!\\nCheck out the instructions\\n[here](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image) if you'd like to self-host the Unstructured API or run it locally.\\n\\n\\n## Data Loaders\\n\\nThe primary usage of `Unstructured` is in data loaders.\\n\\n### UnstructuredLoader\"), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/document_loaders/unstructured_file) to see how you can use\\nthis loader for both partitioning locally and remotely with the serverless Unstructured API.\\n\\n```python\\nfrom langchain_unstructured import UnstructuredLoader\\n```\\n\\n### UnstructuredCHMLoader\\n\\n`CHM` means `Microsoft Compiled HTML Help`.\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredCHMLoader\\n```\\n\\n### UnstructuredCSVLoader\\n\\nA `comma-separated values` (`CSV`) file is a delimited text file that uses \\na comma to separate values. Each line of the file is a data record. \\nEach record consists of one or more fields, separated by commas.\\n\\nSee a [usage example](/docs/integrations/document_loaders/csv#unstructuredcsvloader).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredCSVLoader\\n```\\n\\n### UnstructuredEmailLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/email).'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import UnstructuredEmailLoader\\n```\\n\\n### UnstructuredEPubLoader\\n\\n[EPUB](https://en.wikipedia.org/wiki/EPUB) is an `e-book file format` that uses \\nthe “.epub” file extension. The term is short for electronic publication and \\nis sometimes styled `ePub`. `EPUB` is supported by many e-readers, and compatible \\nsoftware is available for most smartphones, tablets, and computers.\\n\\nSee a [usage example](/docs/integrations/document_loaders/epub).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredEPubLoader\\n```\\n\\n### UnstructuredExcelLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_excel).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredExcelLoader\\n```\\n\\n### UnstructuredFileIOLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/google_drive#passing-in-optional-file-loaders).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredFileIOLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='### UnstructuredHTMLLoader\\n\\nSee a [usage example](/docs/how_to/document_loader_html).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredHTMLLoader\\n```\\n\\n### UnstructuredImageLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/image).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredImageLoader\\n```\\n\\n### UnstructuredMarkdownLoader\\n\\nSee a [usage example](/docs/integrations/vectorstores/starrocks).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\\n```\\n\\n### UnstructuredODTLoader\\n\\nThe `Open Document Format for Office Applications (ODF)`, also known as `OpenDocument`, \\nis an open file format for word processing documents, spreadsheets, presentations \\nand graphics and using ZIP-compressed XML files. It was developed with the aim of \\nproviding an open, XML-based file format specification for office applications.\\n\\nSee a [usage example](/docs/integrations/document_loaders/odt).'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import UnstructuredODTLoader\\n```\\n\\n### UnstructuredOrgModeLoader\\n\\nAn [Org Mode](https://en.wikipedia.org/wiki/Org-mode) document is a document editing, formatting, and organizing mode, designed for notes, planning, and authoring within the free software text editor Emacs.\\n\\nSee a [usage example](/docs/integrations/document_loaders/org_mode).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredOrgModeLoader\\n```\\n\\n### UnstructuredPDFLoader\\n\\nSee a [usage example](/docs/how_to/document_loader_pdf/#layout-analysis-and-extraction-of-text-from-images).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\\n```\\n\\n### UnstructuredPowerPointLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_powerpoint).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredPowerPointLoader\\n```\\n\\n### UnstructuredRSTLoader'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='A `reStructured Text` (`RST`) file is a file format for textual data \\nused primarily in the Python programming language community for technical documentation.\\n\\nSee a [usage example](/docs/integrations/document_loaders/rst).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredRSTLoader\\n```\\n\\n### UnstructuredRTFLoader\\n\\nSee a usage example in the API documentation.\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredRTFLoader\\n```\\n\\n### UnstructuredTSVLoader\\n\\nA `tab-separated values` (`TSV`) file is a simple, text-based file format for storing tabular data.\\nRecords are separated by newlines, and values within a record are separated by tab characters.\\n\\nSee a [usage example](/docs/integrations/document_loaders/tsv).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredTSVLoader\\n```\\n\\n### UnstructuredURLLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/url).'), Document(metadata={'source': 'docs/docs/integrations/providers/unstructured.mdx', 'file_path': 'docs/docs/integrations/providers/unstructured.mdx', 'file_name': 'unstructured.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import UnstructuredURLLoader\\n```\\n\\n### UnstructuredWordDocumentLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/microsoft_word#using-unstructured).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredWordDocumentLoader\\n```\\n\\n### UnstructuredXMLLoader\\n\\nSee a [usage example](/docs/integrations/document_loaders/xml).\\n\\n```python\\nfrom langchain_community.document_loaders import UnstructuredXMLLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='Upstash offers developers serverless databases and messaging\\nplatforms to build powerful applications without having to worry \\nabout the operational complexity of running databases at scale.\\n\\nOne significant advantage of Upstash is that their databases support HTTP and all of their SDKs use HTTP.\\nThis means that you can run this in serverless platforms, edge or any platform that does not support TCP connections.\\n\\nCurrently, there are two Upstash integrations available for LangChain: \\nUpstash Vector as a vector embedding database and Upstash Redis as a cache and memory store.\\n\\n# Upstash Vector\\n\\nUpstash Vector is a serverless vector database that can be used to store and query vectors.\\n\\n## Installation\\n\\nCreate a new serverless vector database at the [Upstash Console](https://console.upstash.com/vector).\\nSelect your preferred distance metric and dimension count according to your model.'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='Install the Upstash Vector Python SDK with `pip install upstash-vector`.\\nThe Upstash Vector integration in langchain is a wrapper for the Upstash Vector Python SDK. That\\'s why the `upstash-vector` package is required.\\n\\n## Integrations\\n\\nCreate a `UpstashVectorStore` object using credentials from the Upstash Console.\\nYou also need to pass in an `Embeddings` object which can turn text into vector embeddings.\\n\\n```python\\nfrom langchain_community.vectorstores.upstash import UpstashVectorStore\\nimport os\\n\\nos.environ[\"UPSTASH_VECTOR_REST_URL\"] = \"<UPSTASH_VECTOR_REST_URL>\"\\nos.environ[\"UPSTASH_VECTOR_REST_TOKEN\"] = \"<UPSTASH_VECTOR_REST_TOKEN>\"\\n\\nstore = UpstashVectorStore(\\n    embedding=embeddings\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='An alternative way of `UpstashVectorStore` is to pass `embedding=True`. This is a unique\\nfeature of the `UpstashVectorStore` thanks to the ability of the Upstash Vector indexes\\nto have an associated embedding model. In this configuration, documents we want to insert or\\nqueries we want to search for are simply sent to Upstash Vector as text. In the background,\\nUpstash Vector embeds these text and executes the request with these embeddings. To use this\\nfeature, [create an Upstash Vector index by selecting a model](https://upstash.com/docs/vector/features/embeddingmodels#using-a-model)\\nand simply pass `embedding=True`:\\n\\n```python\\nfrom langchain_community.vectorstores.upstash import UpstashVectorStore\\nimport os\\n\\nos.environ[\"UPSTASH_VECTOR_REST_URL\"] = \"<UPSTASH_VECTOR_REST_URL>\"\\nos.environ[\"UPSTASH_VECTOR_REST_TOKEN\"] = \"<UPSTASH_VECTOR_REST_TOKEN>\"\\n\\nstore = UpstashVectorStore(\\n    embedding=True\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='See [Upstash Vector documentation](https://upstash.com/docs/vector/features/embeddingmodels)\\nfor more detail on embedding models.\\n\\n## Namespaces\\nYou can use namespaces to partition your data in the index. Namespaces are useful when you want to query over huge amount of data, and you want to partition the data to make the queries faster. When you use namespaces, there won\\'t be post-filtering on the results which will make the query results more precise.\\n\\n```python\\nfrom langchain_community.vectorstores.upstash import UpstashVectorStore\\nimport os\\n\\nos.environ[\"UPSTASH_VECTOR_REST_URL\"] = \"<UPSTASH_VECTOR_REST_URL>\"\\nos.environ[\"UPSTASH_VECTOR_REST_TOKEN\"] = \"<UPSTASH_VECTOR_REST_TOKEN>\"\\n\\nstore = UpstashVectorStore(\\n    embedding=embeddings\\n    namespace=\"my_namespace\"\\n)\\n```\\n\\n### Inserting Vectors\\n\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_openai import OpenAIEmbeddings'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='loader = TextLoader(\"../../modules/state_of_the_union.txt\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocs = text_splitter.split_documents(documents)\\n\\n# Create a new embeddings object\\nembeddings = OpenAIEmbeddings()\\n\\n# Create a new UpstashVectorStore object\\nstore = UpstashVectorStore(\\n    embedding=embeddings\\n)\\n\\n# Insert the document embeddings into the store\\nstore.add_documents(docs)\\n```\\n\\nWhen inserting documents, first they are embedded using the `Embeddings` object.\\n\\nMost embedding models can embed multiple documents at once, so the documents are batched and embedded in parallel.\\nThe size of the batch can be controlled using the `embedding_chunk_size` parameter.'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='The embedded vectors are then stored in the Upstash Vector database. When they are sent, multiple vectors are batched together to reduce the number of HTTP requests.\\nThe size of the batch can be controlled using the `batch_size` parameter. Upstash Vector has a limit of 1000 vectors per batch in the free tier.\\n\\n```python\\nstore.add_documents(\\n    documents,\\n    batch_size=100,\\n    embedding_chunk_size=200\\n)\\n```\\n\\n### Querying Vectors\\n\\nVectors can be queried using a text query or another vector.\\n\\nThe returned value is a list of Document objects.\\n\\n```python\\nresult = store.similarity_search(\\n    \"The United States of America\",\\n    k=5\\n)\\n```\\n\\nOr using a vector:\\n\\n```python\\nvector = embeddings.embed_query(\"Hello world\")\\n\\nresult = store.similarity_search_by_vector(\\n    vector,\\n    k=5\\n)\\n```\\n\\nWhen searching, you can also utilize the `filter` parameter which will allow you to filter by metadata:'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='```python\\nresult = store.similarity_search(\\n    \"The United States of America\",\\n    k=5,\\n    filter=\"type = \\'country\\'\"\\n)\\n```\\n\\nSee [Upstash Vector documentation](https://upstash.com/docs/vector/features/filtering)\\nfor more details on metadata filtering.\\n\\n### Deleting Vectors\\n\\nVectors can be deleted by their IDs.\\n\\n```python\\nstore.delete([\"id1\", \"id2\"])\\n```\\n\\n### Getting information about the store\\n\\nYou can get information about your database like the distance metric dimension using the info function.\\n\\nWhen an insert happens, the database an indexing takes place. While this is happening new vectors can not be queried. `pendingVectorCount` represents the number of vector that are currently being indexed. \\n\\n```python\\ninfo = store.info()\\nprint(info)\\n\\n# Output:\\n# {\\'vectorCount\\': 44, \\'pendingVectorCount\\': 0, \\'indexSize\\': 2642412, \\'dimension\\': 1536, \\'similarityFunction\\': \\'COSINE\\'}\\n```\\n\\n# Upstash Redis\\n\\nThis page covers how to use [Upstash Redis](https://upstash.com/redis) with LangChain.'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='## Installation and Setup\\n- Upstash Redis Python SDK can be installed with `pip install upstash-redis`\\n- A globally distributed, low-latency and highly available database can be created at the [Upstash Console](https://console.upstash.com)\\n\\n\\n## Integrations\\nAll of Upstash-LangChain integrations are based on `upstash-redis` Python SDK being utilized as wrappers for LangChain.\\nThis SDK utilizes Upstash Redis DB by giving UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN parameters from the console.\\n\\n\\n### Cache\\n\\n[Upstash Redis](https://upstash.com/redis) can be used as a cache for LLM prompts and responses.\\n\\nTo import this cache:\\n```python\\nfrom langchain.cache import UpstashRedisCache\\n```\\n\\nTo use with your LLMs:\\n```python\\nimport langchain\\nfrom upstash_redis import Redis\\n\\nURL = \"<UPSTASH_REDIS_REST_URL>\"\\nTOKEN = \"<UPSTASH_REDIS_REST_TOKEN>\"\\n\\nlangchain.llm_cache = UpstashRedisCache(redis_=Redis(url=URL, token=TOKEN))\\n```\\n\\n### Memory'), Document(metadata={'source': 'docs/docs/integrations/providers/upstash.mdx', 'file_path': 'docs/docs/integrations/providers/upstash.mdx', 'file_name': 'upstash.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/memory/upstash_redis_chat_message_history).\\n\\n```python\\nfrom langchain_community.chat_message_histories import (\\n    UpstashRedisChatMessageHistory,\\n)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/usearch.mdx', 'file_path': 'docs/docs/integrations/providers/usearch.mdx', 'file_name': 'usearch.mdx', 'file_type': '.mdx'}, page_content=\"# USearch\\n>[USearch](https://unum-cloud.github.io/usearch/) is a Smaller & Faster Single-File Vector Search Engine.\\n\\n>`USearch's` base functionality is identical to `FAISS`, and the interface should look \\n> familiar if you have ever investigated Approximate Nearest Neighbors search. \\n> `USearch` and `FAISS` both employ `HNSW` algorithm, but they differ significantly \\n> in their design principles. `USearch` is compact and broadly compatible with FAISS without \\n> sacrificing performance, with a primary focus on user-defined metrics and fewer dependencies.\\n> \\n## Installation and Setup\\n\\nWe need to install `usearch` python package.\\n\\n```bash\\npip install usearch\\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/usearch).\\n\\n```python\\nfrom langchain_community.vectorstores import USearch\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/valthera.mdx', 'file_path': 'docs/docs/integrations/providers/valthera.mdx', 'file_name': 'valthera.mdx', 'file_type': '.mdx'}, page_content='# Valthera\\n\\n> [Valthera](https://github.com/valthera/valthera) is an open-source framework that empowers LLM Agents to drive meaningful, context-aware user engagement. It evaluates user motivation and ability in real time, ensuring that notifications and actions are triggered only when users are most receptive.\\n> \\n> **langchain-valthera** integrates Valthera with LangChain, enabling developers to build smarter, behavior-driven engagement systems that deliver personalized interactions.\\n\\n## Installation and Setup\\n\\n### Install langchain-valthera\\n\\nInstall the LangChain Valthera package via pip:\\n\\n```bash\\npip install -U langchain-valthera\\n```\\n\\nImport the ValtheraTool:\\n\\n```python\\nfrom langchain_valthera.tools import ValtheraTool\\n```\\n\\n### Example: Initializing the ValtheraTool for LangChain\\n\\nThis example shows how to initialize the ValtheraTool using a `DataAggregator` and configuration for motivation and ability scoring.'), Document(metadata={'source': 'docs/docs/integrations/providers/valthera.mdx', 'file_path': 'docs/docs/integrations/providers/valthera.mdx', 'file_name': 'valthera.mdx', 'file_type': '.mdx'}, page_content='```python\\nimport os\\nfrom langchain_openai import ChatOpenAI\\nfrom valthera.aggregator import DataAggregator\\nfrom mocks import hubspot, posthog, snowflake  # Replace these with your actual connector implementations\\nfrom langchain_valthera.tools import ValtheraTool\\n\\n# Initialize the DataAggregator with your data connectors\\ndata_aggregator = DataAggregator(\\n    connectors={\\n        \"hubspot\": hubspot(),\\n        \"posthog\": posthog(),\\n        \"app_db\": snowflake()\\n    }\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/valthera.mdx', 'file_path': 'docs/docs/integrations/providers/valthera.mdx', 'file_name': 'valthera.mdx', 'file_type': '.mdx'}, page_content='# Initialize the ValtheraTool with your scoring configurations\\nvalthera_tool = ValtheraTool(\\n    data_aggregator=data_aggregator,\\n    motivation_config=[\\n        {\"key\": \"hubspot_lead_score\", \"weight\": 0.30, \"transform\": lambda x: min(x, 100) / 100.0},\\n        {\"key\": \"posthog_events_count_past_30days\", \"weight\": 0.30, \"transform\": lambda x: min(x, 50) / 50.0},\\n        {\"key\": \"hubspot_marketing_emails_opened\", \"weight\": 0.20, \"transform\": lambda x: min(x / 10.0, 1.0)},\\n        {\"key\": \"posthog_session_count\", \"weight\": 0.20, \"transform\": lambda x: min(x / 5.0, 1.0)}\\n    ],\\n    ability_config=[\\n        {\"key\": \"posthog_onboarding_steps_completed\", \"weight\": 0.30, \"transform\": lambda x: min(x / 5.0, 1.0)},\\n        {\"key\": \"posthog_session_count\", \"weight\": 0.30, \"transform\": lambda x: min(x / 10.0, 1.0)},\\n        {\"key\": \"behavior_complexity\", \"weight\": 0.40, \"transform\": lambda x: 1 - (min(x, 5) / 5.0)}\\n    ]\\n)'), Document(metadata={'source': 'docs/docs/integrations/providers/valthera.mdx', 'file_path': 'docs/docs/integrations/providers/valthera.mdx', 'file_name': 'valthera.mdx', 'file_type': '.mdx'}, page_content='print(\"✅ ValtheraTool successfully initialized for LangChain integration!\")\\n```\\n\\n\\nThe langchain-valthera integration allows you to assess user behavior and decide on the best course of action for engagement, ensuring that interactions are both timely and relevant within your LangChain applications.'), Document(metadata={'source': 'docs/docs/integrations/providers/vdms.mdx', 'file_path': 'docs/docs/integrations/providers/vdms.mdx', 'file_name': 'vdms.mdx', 'file_type': '.mdx'}, page_content='# VDMS\\n\\n> [VDMS](https://github.com/IntelLabs/vdms/blob/master/README.md) is a storage solution for efficient access\\n> of big-”visual”-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata\\n> stored as a graph and enabling machine friendly enhancements to visual data for faster access.\\n\\n## Installation and Setup\\n\\n### Install Client\\n\\n```bash\\npip install langchain-vdms\\n```\\n\\n### Install Database\\n\\nThere are two ways to get started with VDMS:\\n\\n\\n1. Install VDMS on your local machine via docker\\n    ```bash\\n        docker run -d -p 55555:55555 intellabs/vdms:latest\\n    ```\\n\\n2. Install VDMS directly on your local machine. Please see\\n[installation instructions](https://github.com/IntelLabs/vdms/blob/master/INSTALL.md).\\n\\n## VectorStore\\n\\nTo import this vectorstore:\\n\\n```python\\nfrom langchain_vdms import VDMS\\nfrom langchain_vdms.vectorstores import VDMS\\n```\\n\\nTo import the VDMS Client connector:'), Document(metadata={'source': 'docs/docs/integrations/providers/vdms.mdx', 'file_path': 'docs/docs/integrations/providers/vdms.mdx', 'file_name': 'vdms.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_vdms.vectorstores import VDMS_Client\\n```\\n\\nFor a more detailed walkthrough of the VDMS wrapper, see [this guide](/docs/integrations/vectorstores/vdms).'), Document(metadata={'source': 'docs/docs/integrations/providers/vespa.mdx', 'file_path': 'docs/docs/integrations/providers/vespa.mdx', 'file_name': 'vespa.mdx', 'file_type': '.mdx'}, page_content='# Vespa\\n\\n>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database. \\n> It supports vector search (ANN), lexical search, and search in structured data, all in the same query.\\n \\n## Installation and Setup\\n\\n\\n```bash\\npip install pyvespa\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/vespa).\\n\\n```python\\nfrom langchain.retrievers import VespaRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/vlite.mdx', 'file_path': 'docs/docs/integrations/providers/vlite.mdx', 'file_name': 'vlite.mdx', 'file_type': '.mdx'}, page_content='# vlite\\n\\nThis page covers how to use [vlite](https://github.com/sdan/vlite) within LangChain. vlite is a simple and fast vector database for storing and retrieving embeddings.\\n\\n## Installation and Setup\\n\\nTo install vlite, run the following command:\\n\\n```bash\\npip install vlite\\n```\\n\\nFor PDF OCR support, install the `vlite[ocr]` extra:\\n\\n```bash\\npip install vlite[ocr]\\n```\\n\\n## VectorStore\\n\\nvlite provides a wrapper around its vector database, allowing you to use it as a vectorstore for semantic search and example selection.\\n\\nTo import the vlite vectorstore:\\n\\n```python\\nfrom langchain_community.vectorstores import vlite\\n```\\n\\n### Usage\\n\\nFor a more detailed walkthrough of the vlite wrapper, see [this notebook](/docs/integrations/vectorstores/vlite).'), Document(metadata={'source': 'docs/docs/integrations/providers/voyageai.mdx', 'file_path': 'docs/docs/integrations/providers/voyageai.mdx', 'file_name': 'voyageai.mdx', 'file_type': '.mdx'}, page_content='# VoyageAI\\n\\nAll functionality related to VoyageAI\\n\\n>[VoyageAI](https://www.voyageai.com/) Voyage AI builds embedding models, customized for your domain and company, for better retrieval quality.\\n\\n## Installation and Setup\\n\\nInstall the integration package with\\n```bash\\npip install langchain-voyageai\\n```\\n\\nGet a VoyageAI API key and set it as an environment variable (`VOYAGE_API_KEY`)\\n\\n\\n## Text Embedding Model\\n\\nSee a [usage example](/docs/integrations/text_embedding/voyageai)\\n\\n```python\\nfrom langchain_voyageai import VoyageAIEmbeddings\\n```\\n\\n\\n## Reranking\\n\\nSee a [usage example](/docs/integrations/document_transformers/voyageai-reranker)\\n\\n```python\\nfrom langchain_voyageai import VoyageAIRerank\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/wandb.mdx', 'file_path': 'docs/docs/integrations/providers/wandb.mdx', 'file_name': 'wandb.mdx', 'file_type': '.mdx'}, page_content='# Weights & Biases\\n\\n>[Weights & Biases](https://wandb.ai/) is provider of the AI developer platform to train and \\n> fine-tune AI models and develop AI applications.\\n \\n`Weights & Biase` products can be used to log metrics and artifacts during training, \\nand to trace the execution of your code.\\n\\nThere are several main ways to use `Weights & Biases` products within LangChain:\\n- with `wandb_tracing_enabled`\\n- with `Weave` lightweight toolkit\\n- with `WandbCallbackHandler` (deprecated)\\n\\n\\n## wandb_tracing_enabled\\n\\nSee a [usage example](/docs/integrations/providers/wandb_tracing).\\n\\nSee in the [W&B documentation](https://docs.wandb.ai/guides/integrations/langchain).\\n\\n```python\\nfrom langchain_community.callbacks import wandb_tracing_enabled\\n```\\n\\n## Weave\\n\\nSee in the [W&B documentation](https://weave-docs.wandb.ai/guides/integrations/langchain).\\n\\n\\n## WandbCallbackHandler\\n\\n**Note:** the `WandbCallbackHandler` is being deprecated in favour of the `wandb_tracing_enabled`.'), Document(metadata={'source': 'docs/docs/integrations/providers/wandb.mdx', 'file_path': 'docs/docs/integrations/providers/wandb.mdx', 'file_name': 'wandb.mdx', 'file_type': '.mdx'}, page_content='See a [usage example](/docs/integrations/providers/wandb_tracking).\\n\\nSee in the [W&B documentation](https://docs.wandb.ai/guides/integrations/langchain).\\n\\n```python\\nfrom langchain_community.callbacks import WandbCallbackHandler\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/weather.mdx', 'file_path': 'docs/docs/integrations/providers/weather.mdx', 'file_name': 'weather.mdx', 'file_type': '.mdx'}, page_content='# Weather\\n\\n>[OpenWeatherMap](https://openweathermap.org/) is an open-source weather service provider.\\n\\n## Installation and Setup\\n\\n```bash\\npip install pyowm\\n```\\n\\nWe must set up the `OpenWeatherMap API token`.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/weather).\\n\\n```python\\nfrom langchain_community.document_loaders import WeatherDataLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/weaviate.mdx', 'file_path': 'docs/docs/integrations/providers/weaviate.mdx', 'file_name': 'weaviate.mdx', 'file_type': '.mdx'}, page_content='# Weaviate\\n\\n>[Weaviate](https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from\\n>your favorite ML models, and scale seamlessly into billions of data objects.'), Document(metadata={'source': 'docs/docs/integrations/providers/weaviate.mdx', 'file_path': 'docs/docs/integrations/providers/weaviate.mdx', 'file_name': 'weaviate.mdx', 'file_type': '.mdx'}, page_content='What is `Weaviate`?\\n- Weaviate is an open-source \\u200bdatabase of the type \\u200bvector search engine.\\n- Weaviate allows you to store JSON documents in a class property-like fashion while attaching machine learning vectors to these documents to represent them in vector space.\\n- Weaviate can be used stand-alone (aka bring your vectors) or with a variety of modules that can do the vectorization for you and extend the core capabilities.\\n- Weaviate has a GraphQL-API to access your data easily.\\n- We aim to bring your vector search set up to production to query in mere milliseconds (check our [open-source benchmarks](https://weaviate.io/developers/weaviate/current/benchmarks/) to see if Weaviate fits your use case).\\n- Get to know Weaviate in the [basics getting started guide](https://weaviate.io/developers/weaviate/current/core-knowledge/basics.html) in under five minutes.\\n\\n**Weaviate in detail:**'), Document(metadata={'source': 'docs/docs/integrations/providers/weaviate.mdx', 'file_path': 'docs/docs/integrations/providers/weaviate.mdx', 'file_name': 'weaviate.mdx', 'file_type': '.mdx'}, page_content='`Weaviate` is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), etc. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages.\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n\\n```bash\\npip install langchain-weaviate\\n```\\n\\n\\n## Vector Store\\n\\nThere exists a wrapper around `Weaviate` indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_weaviate import WeaviateVectorStore\\n```\\n\\nFor a more detailed walkthrough of the Weaviate wrapper, see [this notebook](/docs/integrations/vectorstores/weaviate)'), Document(metadata={'source': 'docs/docs/integrations/providers/whatsapp.mdx', 'file_path': 'docs/docs/integrations/providers/whatsapp.mdx', 'file_name': 'whatsapp.mdx', 'file_type': '.mdx'}, page_content=\"# WhatsApp\\n\\n>[WhatsApp](https://www.whatsapp.com/) (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content.\\n\\n\\n## Installation and Setup\\n\\nThere isn't any special setup for it.\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/whatsapp_chat).\\n\\n```python\\nfrom langchain_community.document_loaders import WhatsAppChatLoader\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/wikipedia.mdx', 'file_path': 'docs/docs/integrations/providers/wikipedia.mdx', 'file_name': 'wikipedia.mdx', 'file_type': '.mdx'}, page_content='# Wikipedia\\n\\n>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.\\n\\n\\n## Installation and Setup\\n\\n```bash\\npip install wikipedia\\n```\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/wikipedia).\\n\\n```python\\nfrom langchain_community.document_loaders import WikipediaLoader\\n```\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/wikipedia).\\n\\n```python\\nfrom langchain.retrievers import WikipediaRetriever\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/wolfram_alpha.mdx', 'file_path': 'docs/docs/integrations/providers/wolfram_alpha.mdx', 'file_name': 'wolfram_alpha.mdx', 'file_type': '.mdx'}, page_content='# Wolfram Alpha\\n\\n>[WolframAlpha](https://en.wikipedia.org/wiki/WolframAlpha) is an answer engine developed by `Wolfram Research`. \\n> It answers factual queries by computing answers from externally sourced data.\\n\\nThis page covers how to use the `Wolfram Alpha API` within LangChain.\\n\\n## Installation and Setup\\n- Install requirements with \\n```bash\\npip install wolframalpha\\n```\\n- Go to wolfram alpha and sign up for a developer account [here](https://developer.wolframalpha.com/)\\n- Create an app and get your `APP ID`\\n- Set your APP ID as an environment variable `WOLFRAM_ALPHA_APPID`\\n\\n\\n## Wrappers\\n\\n### Utility\\n\\nThere exists a WolframAlphaAPIWrapper utility which wraps this API. To import this utility:\\n\\n```python\\nfrom langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\n```\\n\\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/wolfram_alpha).\\n\\n### Tool'), Document(metadata={'source': 'docs/docs/integrations/providers/wolfram_alpha.mdx', 'file_path': 'docs/docs/integrations/providers/wolfram_alpha.mdx', 'file_name': 'wolfram_alpha.mdx', 'file_type': '.mdx'}, page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"wolfram-alpha\"])\\n```\\n\\nFor more information on tools, see [this page](/docs/how_to/tools_builtin).'), Document(metadata={'source': 'docs/docs/integrations/providers/writer.mdx', 'file_path': 'docs/docs/integrations/providers/writer.mdx', 'file_name': 'writer.mdx', 'file_type': '.mdx'}, page_content='---\\nkeywords: [writer]\\n---\\n\\n# Writer, Inc.\\n\\nAll functionality related to Writer\\n\\n\\n>This page covers how to use the [Writer](https://writer.com/) ecosystem within LangChain. For further information see Writer [docs](https://dev.writer.com/home/introduction).\\n>[Palmyra](https://writer.com/blog/palmyra/) is a Large Language Model (LLM) developed by `Writer, Inc`.\\n>\\n>The [Writer API](https://dev.writer.com/api-guides/introduction) is powered by a diverse set of Palmyra sub-models with different capabilities and price points.\\n\\n## Installation and Setup\\n\\nInstall the integration package with\\n```bash\\npip install langchain-writer\\n```\\n\\nGet an Writer API key and set it as an environment variable (`WRITER_API_KEY`)\\n\\n## Chat model\\n\\n```python\\nfrom langchain_writer import ChatWriter\\n```\\n\\n## PDF Parser\\n\\n\\n```python\\nfrom langchain_writer.pdf_parser import PDFParser\\n```\\n\\n## Text splitter\\n\\n```python\\nfrom langchain_writer.text_splitter import WriterTextSplitter\\n```\\n\\n## Tools calling\\n\\n### Functions'), Document(metadata={'source': 'docs/docs/integrations/providers/writer.mdx', 'file_path': 'docs/docs/integrations/providers/writer.mdx', 'file_name': 'writer.mdx', 'file_type': '.mdx'}, page_content='Support of basic function calls defined via dicts, Pydantic, python functions etc.\\n\\n### Graphs\\n\\n```python\\nfrom langchain_writer.tools import GraphTool\\n```\\n\\nWriter-specific remotely invoking tool'), Document(metadata={'source': 'docs/docs/integrations/providers/xata.mdx', 'file_path': 'docs/docs/integrations/providers/xata.mdx', 'file_name': 'xata.mdx', 'file_type': '.mdx'}, page_content='# Xata\\n\\n> [Xata](https://xata.io) is a serverless data platform, based on `PostgreSQL`. \\n> It provides a Python SDK for interacting with your database, and a UI \\n> for managing your data.\\n> `Xata` has a native vector type, which can be added to any table, and \\n> supports similarity search. LangChain inserts vectors directly to `Xata`, \\n> and queries it for the nearest neighbors of a given vector, so that you can\\n> use all the LangChain Embeddings integrations with `Xata`.\\n\\n\\n## Installation and Setup\\n\\n\\nWe need to install `xata` python package.\\n\\n```bash\\npip install xata==1.0.0a7 \\n```\\n\\n## Vector Store\\n\\nSee a [usage example](/docs/integrations/vectorstores/xata).\\n\\n```python\\nfrom langchain_community.vectorstores import XataVectorStore\\n```\\n\\n## Memory\\n\\nSee a [usage example](/docs/integrations/memory/xata_chat_message_history).\\n\\n```python\\nfrom langchain_community.chat_message_histories import XataChatMessageHistory\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/xinference.mdx', 'file_path': 'docs/docs/integrations/providers/xinference.mdx', 'file_name': 'xinference.mdx', 'file_type': '.mdx'}, page_content='# Xorbits Inference (Xinference)\\n\\nThis page demonstrates how to use [Xinference](https://github.com/xorbitsai/inference)\\nwith LangChain.\\n\\n`Xinference` is a powerful and versatile library designed to serve LLMs, \\nspeech recognition models, and multimodal models, even on your laptop. \\nWith Xorbits Inference, you can effortlessly deploy and serve your or \\nstate-of-the-art built-in models using just a single command.\\n\\n## Installation and Setup\\n\\nXinference can be installed via pip from PyPI: \\n\\n```bash\\npip install \"xinference[all]\"\\n```\\n\\n## LLM\\n\\nXinference supports various models compatible with GGML, including chatglm, baichuan, whisper, \\nvicuna, and orca. To view the builtin models, run the command:\\n\\n```bash\\nxinference list --all\\n```\\n\\n\\n### Wrapper for Xinference\\n\\nYou can start a local instance of Xinference by running:\\n\\n```bash\\nxinference\\n```\\n\\nYou can also deploy Xinference in a distributed cluster. To do so, first start an Xinference supervisor\\non the server you want to run it:'), Document(metadata={'source': 'docs/docs/integrations/providers/xinference.mdx', 'file_path': 'docs/docs/integrations/providers/xinference.mdx', 'file_name': 'xinference.mdx', 'file_type': '.mdx'}, page_content='```bash\\nxinference-supervisor -H \"${supervisor_host}\"\\n```\\n\\n\\nThen, start the Xinference workers on each of the other servers where you want to run them on:\\n\\n```bash\\nxinference-worker -e \"http://${supervisor_host}:9997\"\\n```\\n\\nYou can also start a local instance of Xinference by running:\\n\\n```bash\\nxinference\\n```\\n\\nOnce Xinference is running, an endpoint will be accessible for model management via CLI or \\nXinference client. \\n\\nFor local deployment, the endpoint will be http://localhost:9997. \\n\\n\\nFor cluster deployment, the endpoint will be http://$\\\\{supervisor_host\\\\}:9997.\\n\\n\\nThen, you need to launch a model. You can specify the model names and other attributes \\nincluding model_size_in_billions and quantization. You can use command line interface (CLI) to \\ndo it. For example, \\n\\n```bash\\nxinference launch -n orca -s 3 -q q4_0\\n```\\n\\nA model uid will be returned.\\n\\nExample usage:\\n\\n```python\\nfrom langchain_community.llms import Xinference'), Document(metadata={'source': 'docs/docs/integrations/providers/xinference.mdx', 'file_path': 'docs/docs/integrations/providers/xinference.mdx', 'file_name': 'xinference.mdx', 'file_type': '.mdx'}, page_content='llm = Xinference(\\n    server_url=\"http://0.0.0.0:9997\",\\n    model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\\n)\\n\\nllm(\\n    prompt=\"Q: where can we visit in the capital of France? A:\",\\n    generate_config={\"max_tokens\": 1024, \"stream\": True},\\n)\\n\\n```\\n\\n### Usage\\n\\nFor more information and detailed examples, refer to the\\n[example for xinference LLMs](/docs/integrations/llms/xinference)\\n\\n### Embeddings\\n\\nXinference also supports embedding queries and documents. See\\n[example for xinference embeddings](/docs/integrations/text_embedding/xinference) \\nfor a more detailed demo.'), Document(metadata={'source': 'docs/docs/integrations/providers/yahoo.mdx', 'file_path': 'docs/docs/integrations/providers/yahoo.mdx', 'file_name': 'yahoo.mdx', 'file_type': '.mdx'}, page_content='# Yahoo\\n\\n>[Yahoo (Wikipedia)](https://en.wikipedia.org/wiki/Yahoo) is an American web services provider.\\n>\\n> It provides a web portal, search engine Yahoo Search, and related \\n> services, including `My Yahoo`, `Yahoo Mail`, `Yahoo News`, \\n> `Yahoo Finance`, `Yahoo Sports` and its advertising platform, `Yahoo Native`.\\n\\n\\n## Tools\\n\\n### Yahoo Finance News\\n\\nWe have to install a python package:\\n\\n```bash\\npip install yfinance\\n```\\nSee a [usage example](/docs/integrations/tools/yahoo_finance_news).\\n\\n\\n```python\\nfrom langchain_community.tools import YahooFinanceNewsTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/yandex.mdx', 'file_path': 'docs/docs/integrations/providers/yandex.mdx', 'file_name': 'yandex.mdx', 'file_type': '.mdx'}, page_content='# Yandex\\n\\nAll functionality related to Yandex Cloud\\n\\n>[Yandex Cloud](https://cloud.yandex.com/en/) is a public cloud platform. \\n\\n## Installation and Setup\\n\\nYandex Cloud SDK can be installed via pip from PyPI: \\n\\n```bash\\npip install yandexcloud\\n```\\n\\n## LLMs\\n\\n### YandexGPT\\n\\nSee a [usage example](/docs/integrations/llms/yandex).\\n\\n```python\\nfrom langchain_community.llms import YandexGPT\\n```\\n\\n## Chat models\\n\\n### YandexGPT\\n\\nSee a [usage example](/docs/integrations/chat/yandex).\\n\\n```python\\nfrom langchain_community.chat_models import ChatYandexGPT\\n```\\n\\n## Embedding models\\n\\n### YandexGPT\\n\\nSee a [usage example](/docs/integrations/text_embedding/yandex).\\n\\n```python\\nfrom langchain_community.embeddings import YandexGPTEmbeddings\\n```\\n\\n## Parser\\n\\n### YandexSTTParser\\n\\nIt transcribes and parses audio files. \\n\\n`YandexSTTParser` is similar to the `OpenAIWhisperParser`.\\nSee a [usage example with OpenAIWhisperParser](/docs/integrations/document_loaders/youtube_audio).'), Document(metadata={'source': 'docs/docs/integrations/providers/yandex.mdx', 'file_path': 'docs/docs/integrations/providers/yandex.mdx', 'file_name': 'yandex.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom langchain_community.document_loaders import YandexSTTParser\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_path': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_name': 'yeagerai.mdx', 'file_type': '.mdx'}, page_content='# Yeager.ai\\n\\nThis page covers how to use [Yeager.ai](https://yeager.ai) to generate LangChain tools and agents.\\n\\n## What is Yeager.ai?\\nYeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools. \\n\\nIt features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications.\\n\\n## yAgents\\nLow code generative agent designed to help you build, prototype, and deploy Langchain tools with ease. \\n\\n### How to use?\\n```\\npip install yeagerai-agent\\nyeagerai-agent\\n```\\nGo to http://127.0.0.1:7860'), Document(metadata={'source': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_path': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_name': 'yeagerai.mdx', 'file_type': '.mdx'}, page_content='This will install the necessary dependencies and set up yAgents on your system. After the first run, yAgents will create a .env file where you can input your OpenAI API key. You can do the same directly from the Gradio interface under the tab \"Settings\".\\n\\n`OPENAI_API_KEY=<your_openai_api_key_here>`\\n\\nWe recommend using GPT-4,. However, the tool can also work with GPT-3 if the problem is broken down sufficiently.\\n\\n### Creating and Executing Tools with yAgents\\nyAgents makes it easy to create and execute AI-powered tools. Here\\'s a brief overview of the process:\\n1. Create a tool: To create a tool, provide a natural language prompt to yAgents. The prompt should clearly describe the tool\\'s purpose and functionality. For example:\\n`create a tool that returns the n-th prime number`\\n\\n2. Load the tool into the toolkit: To load a tool into yAgents, simply provide a command to yAgents that says so. For example:\\n`load the tool that you just created it into your toolkit`'), Document(metadata={'source': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_path': 'docs/docs/integrations/providers/yeagerai.mdx', 'file_name': 'yeagerai.mdx', 'file_type': '.mdx'}, page_content=\"3. Execute the tool: To run a tool or agent, simply provide a command to yAgents that includes the name of the tool and any required parameters. For example:\\n`generate the 50th prime number`\\n\\nYou can see a video of how it works [here](https://www.youtube.com/watch?v=KA5hCM3RaWE).\\n\\nAs you become more familiar with yAgents, you can create more advanced tools and agents to automate your work and enhance your productivity.\\n\\nFor more information, see [yAgents' Github](https://github.com/yeagerai/yeagerai-agent) or our [docs](https://yeagerai.gitbook.io/docs/general/welcome-to-yeager.ai)\"), Document(metadata={'source': 'docs/docs/integrations/providers/yellowbrick.mdx', 'file_path': 'docs/docs/integrations/providers/yellowbrick.mdx', 'file_name': 'yellowbrick.mdx', 'file_type': '.mdx'}, page_content='# Yellowbrick\\n\\n>[Yellowbrick](https://yellowbrick.com/) is a provider of \\n> Enterprise Data Warehousing, Ad-hoc and Streaming Analytics, \\n> BI and AI workloads. \\n\\n## Vector store\\n\\nWe have to install a python package:\\n\\n```bash\\npip install psycopg2\\n```\\n\\n```python\\nfrom langchain_community.vectorstores import Yellowbrick\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/yi.mdx', 'file_path': 'docs/docs/integrations/providers/yi.mdx', 'file_name': 'yi.mdx', 'file_type': '.mdx'}, page_content='# 01.AI\\n\\n>[01.AI](https://www.lingyiwanwu.com/en), founded by Dr. Kai-Fu Lee, is a global company at the forefront of AI 2.0. They offer cutting-edge large language models, including the Yi series, which range from 6B to hundreds of billions of parameters. 01.AI also provides multimodal models, an open API platform, and open-source options like Yi-34B/9B/6B and Yi-VL.\\n\\n## Installation and Setup\\n\\nRegister and get an API key from either the China site [here](https://platform.lingyiwanwu.com/apikeys) or the global site [here](https://platform.01.ai/apikeys).\\n\\n## LLMs\\n\\nSee a [usage example](/docs/integrations/llms/yi).\\n\\n```python\\nfrom langchain_community.llms import YiLLM\\n```\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/yi).\\n\\n```python\\nfrom langchain_community.chat_models import ChatYi\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/you.mdx', 'file_path': 'docs/docs/integrations/providers/you.mdx', 'file_name': 'you.mdx', 'file_type': '.mdx'}, page_content='# You\\n\\n>[You](https://you.com/about) company provides an AI productivity platform.\\n\\n## Retriever\\n\\nSee a [usage example](/docs/integrations/retrievers/you-retriever).\\n\\n```python\\nfrom langchain_community.retrievers.you import YouRetriever\\n```\\n\\n## Tools\\n\\nSee a [usage example](/docs/integrations/tools/you).\\n\\n```python\\nfrom langchain_community.tools.you import YouSearchTool\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/youtube.mdx', 'file_path': 'docs/docs/integrations/providers/youtube.mdx', 'file_name': 'youtube.mdx', 'file_type': '.mdx'}, page_content='# YouTube\\n\\n>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform by Google.\\n> We download the `YouTube` transcripts and video information.\\n\\n## Installation and Setup\\n\\n```bash\\npip install youtube-transcript-api\\npip install pytube\\n```\\nSee a [usage example](/docs/integrations/document_loaders/youtube_transcript).\\n\\n\\n## Document Loader\\n\\nSee a [usage example](/docs/integrations/document_loaders/youtube_transcript).\\n\\n```python\\nfrom langchain_community.document_loaders import YoutubeLoader\\nfrom langchain_community.document_loaders import GoogleApiYoutubeLoader\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"# Zep\\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\\n\\n>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\\n> while also reducing hallucinations, latency, and cost.\\n\\n## How Zep works\\n\\nZep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories.\\nIt also embeds messages and summaries, enabling you to search Zep for relevant context from past conversations.\\nZep does all of this asynchronously, ensuring these operations don't impact your user's chat experience.\\nData is persisted to database, allowing you to scale out when growth demands.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"Zep also provides a simple, easy to use abstraction for document vector search called Document Collections.\\nThis is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.\\n\\nZep allows you to be more intentional about constructing your prompt:\\n- automatically adding a few recent messages, with the number customized for your app;\\n- a summary of recent conversations prior to the messages above;\\n- and/or contextually relevant summaries or messages surfaced from the entire chat session.\\n- and/or relevant Business data from Zep Document Collections.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"## What is Zep Cloud?\\n[Zep Cloud](https://www.getzep.com) is a managed service with Zep Open Source at its core.\\nIn addition to Zep Open Source's memory management features, Zep Cloud offers:\\n- **Fact Extraction**: Automatically build fact tables from conversations, without having to define a data schema upfront.\\n- **Dialog Classification**: Instantly and accurately classify chat dialog. Understand user intent and emotion, segment users, and more. Route chains based on semantic context, and trigger events.\\n- **Structured Data Extraction**: Quickly extract business data from chat conversations using a schema you define. Understand what your Assistant should ask for next in order to complete its task.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"## Zep Open Source\\nZep offers an open source version with a self-hosted option.\\nPlease refer to the [Zep Open Source](https://github.com/getzep/zep) repo for more information.\\nYou can also find Zep Open Source compatible [Retriever](/docs/integrations/retrievers/zep_memorystore), [Vector Store](/docs/integrations/vectorstores/zep) and [Memory](/docs/integrations/memory/zep_memory) examples\\n\\n## Zep Cloud Installation and Setup\\n\\n[Zep Cloud Docs](https://help.getzep.com)\\n\\n1. Install the Zep Cloud SDK:\\n\\n```bash\\npip install zep_cloud\\n```\\nor\\n```bash\\npoetry add zep_cloud\\n```\\n\\n## Memory\\n\\nZep's Memory API persists your users' chat history and metadata to a [Session](https://help.getzep.com/chat-history-memory/sessions), enriches the memory, and\\nenables vector similarity search over historical chat messages and dialog summaries.\\n\\nZep offers several approaches to populating prompts with context from historical conversations.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content='### Perpetual Memory\\nThis is the default memory type.\\nSalient facts from the dialog are extracted and stored in a Fact Table.\\nThis is updated in real-time as new messages are added to the Session.\\nEvery time you call the Memory API to get a Memory, Zep returns the Fact Table, the most recent messages (per your Message Window setting), and a summary of the most recent messages prior to the Message Window.\\nThe combination of the Fact Table, summary, and the most recent messages in a prompts provides both factual context and nuance to the LLM.\\n\\n### Summary Retriever Memory\\nReturns the most recent messages and a summary of past messages relevant to the current conversation,\\nenabling you to provide your Assistant with helpful context from past conversations\\n\\n### Message Window Buffer Memory\\nReturns the most recent N messages from the current conversation.\\n\\nAdditionally, Zep enables vector similarity searches for Messages or Summaries stored within its system.'), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"This feature lets you populate prompts with past conversations that are contextually similar to a specific query,\\norganizing the results by a similarity Score.\\n\\n`ZepCloudChatMessageHistory` and `ZepCloudMemory` classes can be imported to interact with Zep Cloud APIs.\\n\\n`ZepCloudChatMessageHistory` is compatible with `RunnableWithMessageHistory`.\\n```python\\nfrom langchain_community.chat_message_histories import ZepCloudChatMessageHistory\\n```\\n\\nSee a [Perpetual Memory Example here](/docs/integrations/memory/zep_cloud_chat_message_history).\\n\\nYou can use `ZepCloudMemory` together with agents that support Memory.\\n```python\\nfrom langchain_community.memory import ZepCloudMemory\\n```\\n\\nSee a [Memory RAG Example here](/docs/integrations/memory/zep_memory_cloud).\\n\\n## Retriever\\n\\nZep's Memory Retriever is a LangChain Retriever that enables you to retrieve messages from a Zep Session and use them to construct your prompt.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content=\"The Retriever supports searching over both individual messages and summaries of conversations. The latter is useful for providing rich, but succinct context to the LLM as to relevant past conversations.\\n\\nZep's Memory Retriever supports both similarity search and [Maximum Marginal Relevance (MMR) reranking](https://help.getzep.com/working-with-search#how-zeps-mmr-re-ranking-works). MMR search is useful for ensuring that the retrieved messages are diverse and not too similar to each other\\n\\nSee a [usage example](/docs/integrations/retrievers/zep_cloud_memorystore).\\n\\n```python\\nfrom langchain_community.retrievers import ZepCloudRetriever\\n```\\n\\n## Vector store\\n\\nZep's [Document VectorStore API](https://help.getzep.com/document-collections) enables you to store and retrieve documents using vector similarity search. Zep doesn't require you to understand\\ndistance functions, types of embeddings, or indexing best practices. You just pass in your chunked documents, and Zep handles the rest.\"), Document(metadata={'source': 'docs/docs/integrations/providers/zep.mdx', 'file_path': 'docs/docs/integrations/providers/zep.mdx', 'file_name': 'zep.mdx', 'file_type': '.mdx'}, page_content='Zep supports both similarity search and [Maximum Marginal Relevance (MMR) reranking](https://help.getzep.com/working-with-search#how-zeps-mmr-re-ranking-works).\\nMMR search is useful for ensuring that the retrieved documents are diverse and not too similar to each other.\\n\\n```python\\nfrom langchain_community.vectorstores import ZepCloudVectorStore\\n```\\n\\nSee a [usage example](/docs/integrations/vectorstores/zep_cloud).'), Document(metadata={'source': 'docs/docs/integrations/providers/zhipuai.mdx', 'file_path': 'docs/docs/integrations/providers/zhipuai.mdx', 'file_name': 'zhipuai.mdx', 'file_type': '.mdx'}, page_content=\"# Zhipu AI\\n\\n>[Zhipu AI](https://www.zhipuai.cn/en/aboutus), originating from the technological \\n> advancements of `Tsinghua University's Computer Science Department`, \\n> is an artificial intelligence company with the mission of teaching machines \\n> to think like humans. Its world-leading AI team has developed the cutting-edge \\n> large language and multimodal models and built the high-precision billion-scale \\n> knowledge graphs, the combination of which uniquely empowers us to create a powerful \\n> data- and knowledge-driven cognitive engine towards artificial general intelligence.\\n\\n\\n## Chat models\\n\\nSee a [usage example](/docs/integrations/chat/zhipuai).\\n\\n```python\\nfrom langchain_community.chat_models import ChatZhipuAI\\n```\"), Document(metadata={'source': 'docs/docs/integrations/providers/zilliz.mdx', 'file_path': 'docs/docs/integrations/providers/zilliz.mdx', 'file_name': 'zilliz.mdx', 'file_type': '.mdx'}, page_content='# Zilliz\\n\\n>[Zilliz Cloud](https://zilliz.com/doc/quick_start) is a fully managed service on cloud for `LF AI Milvus®`,\\n\\n\\n## Installation and Setup\\n\\nInstall the Python SDK:\\n```bash\\npip install pymilvus\\n```\\n\\n## Vectorstore\\n\\nA wrapper around Zilliz indexes allows you to use it as a vectorstore,\\nwhether for semantic search or example selection.\\n\\n```python\\nfrom langchain_community.vectorstores import Milvus\\n```\\n\\nFor a more detailed walkthrough of the Miluvs wrapper, see [this notebook](/docs/integrations/vectorstores/zilliz)'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_label: Graph RAG\\ndescription: Graph traversal over any Vector Store using document metadata.\\n---\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\nimport EmbeddingTabs from \"@theme/EmbeddingTabs\";\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\n\\n\\n# Graph RAG\\n\\nThis guide provides an introduction to Graph RAG. For detailed documentation of all\\nsupported features and configurations, refer to the\\n[Graph RAG Project Page](https://datastax.github.io/graph-rag/).\\n\\n## Overview\\n\\nThe `GraphRetriever` from the `langchain-graph-retriever` package provides a LangChain\\n[retriever](/docs/concepts/retrievers/) that combines **unstructured** similarity search\\non vectors with **structured** traversal of metadata properties. This enables graph-based\\nretrieval over an **existing** vector store.\\n\\n### Integration details'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='| Retriever | Source | PyPI Package | Latest | Project Page |\\n| :--- | :--- | :---: | :---: | :---: |\\n| GraphRetriever | [github.com/datastax/graph-rag](https://github.com/datastax/graph-rag/tree/main/packages/langchain-graph-retriever) | [langchain-graph-retriever](https://pypi.org/project/langchain-graph-retriever/) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-graph-retriever?style=flat-square&label=%20&color=orange) | [Graph RAG](https://datastax.github.io/graph-rag/) |\\n\\n\\n## Benefits\\n\\n* [**Link based on existing metadata:**](https://datastax.github.io/graph-rag/get-started/)\\n  Use existing metadata fields without additional processing. Retrieve more from an\\n  existing vector store!\\n\\n* [**Change links on demand:**](https://datastax.github.io/graph-rag/get-started/edges/)\\n  Edges can be specified on-the-fly, allowing different relationships to be traversed\\n  based on the question.'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='* [**Pluggable Traversal Strategies:**](https://datastax.github.io/graph-rag/get-started/strategies/)\\n  Use built-in traversal strategies like Eager or MMR, or define custom logic to select\\n  which nodes to explore.\\n\\n* [**Broad compatibility:**](https://datastax.github.io/graph-rag/get-started/adapters/)\\n  Adapters are available for a variety of vector stores with support for additional\\n  stores easily added.\\n\\n## Setup\\n\\n### Installation\\n\\nThis retriever lives in the `langchain-graph-retriever` package.\\n\\n```bash\\npip install -qU langchain-graph-retriever\\n```\\n## Instantiation\\n\\nThe following examples will show how to perform graph traversal over some sample\\nDocuments about animals.\\n\\n### Prerequisites\\n\\n<details>\\n  <summary>Toggle for Details</summary>\\n  <div>\\n    1. Ensure you have Python 3.10+ installed\\n\\n    1. Install the following package that provides sample data.\\n        ```bash\\n        pip install -qU graph_rag_example_helpers\\n        ```'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='1. Download the test documents:\\n        ```python\\n        from graph_rag_example_helpers.datasets.animals import fetch_documents\\n        animals = fetch_documents()\\n        ```\\n\\n    1. <EmbeddingTabs/>\\n  </div>\\n</details>\\n\\n### Populating the Vector store\\n\\nThis section shows how to populate a variety of vector stores with the sample data.\\n\\nFor help on choosing one of the vector stores below, or to add support for your\\nvector store, consult the documentation about\\n[Adapters and Supported Stores](https://datastax.github.io/graph-rag/guide/adapters/).\\n\\n<Tabs groupId=\"vector-store\" queryString>\\n  <TabItem value=\"astra-db\" label=\"AstraDB\" default>\\n    <div style={{ paddingLeft: \\'30px\\' }}>\\n      Install the `langchain-graph-retriever` package with the `astra` extra:\\n\\n      ```bash\\n      pip install \"langchain-graph-retriever[astra]\"\\n      ```\\n\\n      Then create a vector store and load the test documents:\\n\\n      ```python\\n      from langchain_astradb import AstraDBVectorStore'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='vector_store = AstraDBVectorStore.from_documents(\\n          documents=animals,\\n          embedding=embeddings,\\n          collection_name=\"animals\",\\n          api_endpoint=ASTRA_DB_API_ENDPOINT,\\n          token=ASTRA_DB_APPLICATION_TOKEN,\\n      )\\n      ```\\n      For the `ASTRA_DB_API_ENDPOINT` and `ASTRA_DB_APPLICATION_TOKEN` credentials,\\n      consult the [AstraDB Vector Store Guide](/docs/integrations/vectorstores/astradb).\\n\\n      :::note\\n      For faster initial testing, consider using the **InMemory** Vector Store.\\n      :::\\n    </div>\\n  </TabItem>\\n  <TabItem value=\"cassandra\" label=\"Apache Cassandra\">\\n    <div style={{ paddingLeft: \\'30px\\' }}>\\n      Install the `langchain-graph-retriever` package with the `cassandra` extra:\\n\\n      ```bash\\n      pip install \"langchain-graph-retriever[cassandra]\"\\n      ```\\n\\n      Then create a vector store and load the test documents:'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='```python\\n      from langchain_community.vectorstores.cassandra import Cassandra\\n      from langchain_graph_retriever.transformers import ShreddingTransformer\\n\\n      vector_store = Cassandra.from_documents(\\n          documents=list(ShreddingTransformer().transform_documents(animals)),\\n          embedding=embeddings,\\n          table_name=\"animals\",\\n      )\\n      ```\\n\\n      For help creating a Cassandra connection, consult the\\n      [Apache Cassandra Vector Store Guide](/docs/integrations/vectorstores/cassandra#connection-parameters)'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content=':::note\\n      Apache Cassandra doesn\\'t support searching in nested metadata. Because of this\\n      it is necessary to use the [`ShreddingTransformer`](https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/#langchain_graph_retriever.transformers.shredding.ShreddingTransformer)\\n      when inserting documents.\\n      :::\\n    </div>\\n  </TabItem>\\n  <TabItem value=\"opensearch\" label=\"OpenSearch\">\\n    <div style={{ paddingLeft: \\'30px\\' }}>\\n      Install the `langchain-graph-retriever` package with the `opensearch` extra:\\n\\n      ```bash\\n      pip install \"langchain-graph-retriever[opensearch]\"\\n      ```\\n\\n      Then create a vector store and load the test documents:\\n\\n      ```python\\n      from langchain_community.vectorstores import OpenSearchVectorSearch'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='vector_store = OpenSearchVectorSearch.from_documents(\\n          documents=animals,\\n          embedding=embeddings,\\n          engine=\"faiss\",\\n          index_name=\"animals\",\\n          opensearch_url=OPEN_SEARCH_URL,\\n          bulk_size=500,\\n      )\\n      ```\\n\\n      For help creating an OpenSearch connection, consult the\\n      [OpenSearch Vector Store Guide](/docs/integrations/vectorstores/opensearch).\\n    </div>\\n  </TabItem>\\n  <TabItem value=\"chroma\" label=\"Chroma\">\\n    <div style={{ paddingLeft: \\'30px\\' }}>\\n      Install the `langchain-graph-retriever` package with the `chroma` extra:\\n\\n      ```bash\\n      pip install \"langchain-graph-retriever[chroma]\"\\n      ```\\n\\n      Then create a vector store and load the test documents:\\n\\n      ```python\\n      from langchain_chroma.vectorstores import Chroma\\n      from langchain_graph_retriever.transformers import ShreddingTransformer'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='vector_store = Chroma.from_documents(\\n          documents=list(ShreddingTransformer().transform_documents(animals)),\\n          embedding=embeddings,\\n          collection_name=\"animals\",\\n      )\\n      ```\\n\\n      For help creating an Chroma connection, consult the\\n      [Chroma Vector Store Guide](/docs/integrations/vectorstores/chroma).\\n\\n      :::note\\n      Chroma doesn\\'t support searching in nested metadata. Because of this\\n      it is necessary to use the [`ShreddingTransformer`](https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/transformers/#langchain_graph_retriever.transformers.shredding.ShreddingTransformer)\\n      when inserting documents.\\n      :::\\n    </div>\\n  </TabItem>\\n  <TabItem value=\"in-memory\" label=\"InMemory\" default>\\n    <div style={{ paddingLeft: \\'30px\\' }}>\\n      Install the `langchain-graph-retriever` package:\\n\\n      ```bash\\n      pip install \"langchain-graph-retriever\"\\n      ```'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content=\"Then create a vector store and load the test documents:\\n\\n      ```python\\n      from langchain_core.vectorstores import InMemoryVectorStore\\n\\n      vector_store = InMemoryVectorStore.from_documents(\\n          documents=animals,\\n          embedding=embeddings,\\n      )\\n      ```\\n\\n      :::tip\\n      Using the `InMemoryVectorStore` is the fastest way to get started with Graph RAG\\n      but it isn't recommended for production use. Instead it is recommended to use\\n      **AstraDB** or **OpenSearch**.\\n      :::\\n    </div>\\n  </TabItem>\\n</Tabs>\\n\\n### Graph Traversal\\n\\nThis graph retriever starts with a single animal that best matches the query, then\\ntraverses to other animals sharing the same `habitat` and/or `origin`.\\n\\n  ```python\\n  from graph_retriever.strategies import Eager\\n  from langchain_graph_retriever import GraphRetriever\"), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='traversal_retriever = GraphRetriever(\\n      store = vector_store,\\n      edges = [(\"habitat\", \"habitat\"), (\"origin\", \"origin\")],\\n      strategy = Eager(k=5, start_k=1, max_depth=2),\\n  )\\n  ```\\n\\nThe above creates a graph traversing retriever that starts with the nearest\\nanimal (`start_k=1`), retrieves 5 documents (`k=5`) and limits the search to documents\\nthat are at most 2 steps away from the first animal (`max_depth=2`).\\n\\nThe `edges` define how metadata values can be used for traversal. In this case, every\\nanimal is connected to other animals with the same `habitat` and/or `origin`.\\n\\n```python\\nresults = traversal_retriever.invoke(\"what animals could be found near a capybara?\")\\n\\nfor doc in results:\\n    print(f\"{doc.id}: {doc.page_content}\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='```output\\ncapybara: capybaras are the largest rodents in the world and are highly social animals.\\nheron: herons are wading birds known for their long legs and necks, often seen near water.\\ncrocodile: crocodiles are large reptiles with powerful jaws and a long lifespan, often living over 70 years.\\nfrog: frogs are amphibians known for their jumping ability and croaking sounds.\\nduck: ducks are waterfowl birds known for their webbed feet and quacking sounds.\\n```\\n\\nGraph traversal improves retrieval quality by leveraging structured relationships in\\nthe data. Unlike standard similarity search (see below), it provides a clear,\\nexplainable rationale for why documents are selected.\\n\\nIn this case, the documents `capybara`, `heron`, `frog`, `crocodile`, and `newt` all\\nshare the same `habitat=wetlands`, as defined by their metadata. This should increase\\nDocument Relevance and the quality of the answer from the LLM.\\n\\n### Comparison to Standard Retrieval'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='When `max_depth=0`, the graph traversing retriever behaves like a standard retriever:\\n\\n```python\\nstandard_retriever = GraphRetriever(\\n    store = vector_store,\\n    edges = [(\"habitat\", \"habitat\"), (\"origin\", \"origin\")],\\n    strategy = Eager(k=5, start_k=5, max_depth=0),\\n)\\n```\\n\\nThis creates a retriever that starts with the nearest 5 animals (`start_k=5`),\\nand returns them without any traversal (`max_depth=0`). The edge definitions\\nare ignored in this case.\\n\\nThis is essentially the same as:\\n\\n```python\\nstandard_retriever = vector_store.as_retriever(search_kwargs={\"k\":5})\\n```\\n\\nFor either case, invoking the retriever returns:\\n\\n```python\\nresults = standard_retriever.invoke(\"what animals could be found near a capybara?\")\\n\\nfor doc in results:\\n    print(f\"{doc.id}: {doc.page_content}\")\\n```'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='```output\\ncapybara: capybaras are the largest rodents in the world and are highly social animals.\\niguana: iguanas are large herbivorous lizards often found basking in trees and near water.\\nguinea pig: guinea pigs are small rodents often kept as pets due to their gentle and social nature.\\nhippopotamus: hippopotamuses are large semi-aquatic mammals known for their massive size and territorial behavior.\\nboar: boars are wild relatives of pigs, known for their tough hides and tusks.\\n```\\n\\nThese documents are joined based on similarity alone. Any structural data that existed\\nin the store is ignored. As compared to graph retrieval, this can decrease Document\\nRelevance because the returned results have a lower chance of being helpful to answer\\nthe query.\\n\\n## Usage\\n\\nFollowing the examples above, `.invoke` is used to initiate retrieval on a query.\\n\\n## Use within a chain\\n\\nLike other retrievers, `GraphRetriever` can be incorporated into LLM applications\\nvia [chains](/docs/how_to/sequence/).'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='<ChatModelTabs customVarName=\"llm\" />\\n\\n```python\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\n\\nprompt = ChatPromptTemplate.from_template(\\n\"\"\"Answer the question based only on the context provided.\\n\\nContext: {context}\\n\\nQuestion: {question}\"\"\"\\n)\\n\\ndef format_docs(docs):\\n    return \"\\\\n\\\\n\".join(f\"text: {doc.page_content} metadata: {doc.metadata}\" for doc in docs)\\n\\nchain = (\\n    {\"context\": traversal_retriever | format_docs, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | llm\\n    | StrOutputParser()\\n)\\n```\\n\\n```python\\nchain.invoke(\"what animals could be found near a capybara?\")\\n```\\n\\n```output\\nAnimals that could be found near a capybara include herons, crocodiles, frogs,\\nand ducks, as they all inhabit wetlands.\\n```\\n\\n## API reference'), Document(metadata={'source': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_path': 'docs/docs/integrations/retrievers/graph_rag.mdx', 'file_name': 'graph_rag.mdx', 'file_type': '.mdx'}, page_content='To explore all available parameters and advanced configurations, refer to the\\n[Graph RAG API reference](https://datastax.github.io/graph-rag/reference/).'), Document(metadata={'source': 'docs/docs/integrations/retrievers/index.mdx', 'file_path': 'docs/docs/integrations/retrievers/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\nimport {CategoryTable, IndexTable} from '@theme/FeatureTables'\\n\\n# Retrievers\\n\\nA [retriever](/docs/concepts/retrievers) is an interface that returns documents given an unstructured query.\\nIt is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\\nRetrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/docs/integrations/retrievers/wikipedia/) and [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/).\\n\\nRetrievers accept a string query as input and return a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) as output.\\n\\nFor specifics on how to use retrievers, see the [relevant how-to guides here](/docs/how_to/#retrievers).\"), Document(metadata={'source': 'docs/docs/integrations/retrievers/index.mdx', 'file_path': 'docs/docs/integrations/retrievers/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Note that all [vector stores](/docs/concepts/vectorstores) can be [cast to retrievers](/docs/how_to/vectorstore_retriever/).\\nRefer to the vector store [integration docs](/docs/integrations/vectorstores/) for available vector stores.\\nThis page lists custom retrievers, implemented via subclassing [BaseRetriever](/docs/how_to/custom_retriever/).\\n\\n## Bring-your-own documents\\n\\nThe below retrievers allow you to index and search a custom corpus of documents.\\n\\n<CategoryTable category=\"document_retrievers\" />\\n\\n## External index\\n\\nThe below retrievers will search over an external index (e.g., constructed from Internet data or similar).\\n\\n<CategoryTable category=\"external_retrievers\" />\\n\\n## All retrievers\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/integrations/text_embedding/index.mdx', 'file_path': 'docs/docs/integrations/text_embedding/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Embedding models\\n\\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\\n\\n[Embedding models](/docs/concepts/embedding_models) create a vector representation of a piece of text.\\n\\nThis page documents integrations with various model providers that allow you to use embeddings in LangChain.\\n\\nimport EmbeddingTabs from \"@theme/EmbeddingTabs\";\\n\\n<EmbeddingTabs/>\\n\\n```python\\nembeddings.embed_query(\"Hello, world!\")\\n```\\n\\n<CategoryTable category=\"text_embedding\" />\\n\\n## All embedding models\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/integrations/vectorstores/index.mdx', 'file_path': 'docs/docs/integrations/vectorstores/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Vector stores\\n\\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\\n\\nA [vector store](/docs/concepts/vectorstores) stores [embedded](/docs/concepts/embedding_models) data and performs similarity search.\\n\\n**Select embedding model:**\\n\\nimport EmbeddingTabs from \"@theme/EmbeddingTabs\";\\n\\n<EmbeddingTabs/>\\n\\n**Select vector store:**\\n\\nimport VectorStoreTabs from \"@theme/VectorStoreTabs\";\\n\\n<VectorStoreTabs/>\\n\\n<CategoryTable category=\"vectorstores\" />\\n\\n## All Vectorstores\\n\\n<IndexTable />'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx', 'file_path': 'docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx', 'file_name': 'INVALID_PROMPT_INPUT.mdx', 'file_type': '.mdx'}, page_content='# INVALID_PROMPT_INPUT\\n\\nA [prompt template](/docs/concepts/prompt_templates) received missing or invalid input variables.\\n\\n## Troubleshooting\\n\\nThe following may help resolve this error:'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx', 'file_path': 'docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx', 'file_name': 'INVALID_PROMPT_INPUT.mdx', 'file_type': '.mdx'}, page_content='- Double-check your prompt template to ensure that it is correct.\\n  - If you are using the default f-string format and you are using curly braces `{` anywhere in your template, they should be double escaped like this: `{{` (and if you want to render a double curly brace, you should use four curly braces: `{{{{`).\\n- If you are using a [`MessagesPlaceholder`](/docs/concepts/prompt_templates/#messagesplaceholder), make sure that you are passing in an array of messages or message-like objects.\\n  - If you are using shorthand tuples to declare your prompt template, make sure that the variable name is wrapped in curly braces (`[\"placeholder\", \"{messages}\"]`).\\n- Try viewing the inputs into your prompt template using [LangSmith](https://docs.smith.langchain.com/) or log statements to confirm they appear as expected.\\n- If you are pulling a prompt from the [LangChain Prompt Hub](https://smith.langchain.com/prompts), try pulling and logging it or running it in isolation with a sample input to confirm that it is what you expect.'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/MODEL_AUTHENTICATION.mdx', 'file_path': 'docs/docs/troubleshooting/errors/MODEL_AUTHENTICATION.mdx', 'file_name': 'MODEL_AUTHENTICATION.mdx', 'file_type': '.mdx'}, page_content='# MODEL_AUTHENTICATION\\n\\nYour model provider is denying you access to their service.\\n\\n## Troubleshooting\\n\\nThe following may help resolve this error:\\n\\n- Confirm that your API key or other credentials are correct.\\n- If you are relying on an environment variable to authenticate, confirm that the variable name is correct and that it has a value set.\\n  - Note that environment variables can also be set by packages like `dotenv`.\\n  - For models, you can try explicitly passing an `api_key` parameter to rule out any environment variable issues like this:\\n\\n```python\\nmodel = ChatOpenAI(api_key=\"YOUR_KEY_HERE\")\\n```\\n\\n- If you are using a proxy or other custom endpoint, make sure that your custom provider does not expect an alternative authentication scheme.'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/MODEL_NOT_FOUND.mdx', 'file_path': 'docs/docs/troubleshooting/errors/MODEL_NOT_FOUND.mdx', 'file_name': 'MODEL_NOT_FOUND.mdx', 'file_type': '.mdx'}, page_content='# MODEL_NOT_FOUND\\n\\nThe model name you have specified is not acknowledged by your provider.\\n\\n## Troubleshooting\\n\\nThe following may help resolve this error:\\n\\n- Double check the model string you are passing in.\\n- If you are using a proxy or other alternative host with a model wrapper, confirm that the permitted model names are not restricted or altered.'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/MODEL_RATE_LIMIT.mdx', 'file_path': 'docs/docs/troubleshooting/errors/MODEL_RATE_LIMIT.mdx', 'file_name': 'MODEL_RATE_LIMIT.mdx', 'file_type': '.mdx'}, page_content='# MODEL_RATE_LIMIT\\n\\nYou have hit the maximum number of requests that a model provider allows over a given time period and are being temporarily blocked.\\nGenerally, this error is temporary and your limit will reset after a certain amount of time.\\n\\n## Troubleshooting\\n\\nThe following may help resolve this error:\\n\\n- Contact your model provider and ask for a rate limit increase.\\n- If many of your incoming requests are the same, utilize [model response caching](/docs/how_to/chat_model_caching/).\\n- Spread requests across different providers if your application allows it.\\n- Use a [`rate_limiter`](/docs/how_to/chat_model_rate_limiting/) to control the rate of requests to the model.'), Document(metadata={'source': 'docs/docs/troubleshooting/errors/index.mdx', 'file_path': 'docs/docs/troubleshooting/errors/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Error reference\\n\\nThis page contains guides around resolving common errors you may find while building with LangChain.\\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\\n\\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT)\\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS)\\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE)\\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION)\\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND)\\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT)\\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE)'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 1\\n---\\n\\n# How to migrate to LangGraph memory\\n\\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into their LangChain application.\\n\\n* Users that rely on `RunnableWithMessageHistory` or `BaseChatMessageHistory` do **not** need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.\\n* Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.\\n\\n## Why use LangGraph for memory?\\n\\nThe main advantages of persistence in LangGraph are:'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.\\n- Ability to save and resume complex conversations at any point. This helps with:\\n  - Error recovery\\n  - Allowing human intervention in AI workflows\\n  - Exploring different conversation paths (\"time travel\")\\n- Full compatibility with both traditional [language models](/docs/concepts/text_llms) and modern [chat models](/docs/concepts/chat_models). Early memory implementations in LangChain weren\\'t designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.\\n- Highly customizable, allowing you to fully control how memory works and use different storage backends.\\n\\n## Evolution of memory in LangChain\\n\\nThe concept of memory has evolved significantly in LangChain since its initial release.\\n\\n### LangChain 0.0.x memory'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\\n\\n| Use Case                             | Example                                                                                                                           |\\n|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|\\n| Managing conversation history        | Keep only the last `n` turns of the conversation between the user and the AI.                                                     |\\n| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user.                     |\\n| Composite memory implementations     | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='While the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\\n\\nMost of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\\n\\n### RunnableWithMessageHistory and BaseChatMessageHistory\\n\\n:::note\\nPlease see [How to use BaseChatMessageHistory with LangGraph](./chat_history), if you would like to use `BaseChatMessageHistory` (with or without `RunnableWithMessageHistory`) in LangGraph.\\n:::'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='As of LangChain v0.1, we started recommending that users rely primarily on [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory). `BaseChatMessageHistory` serves\\nas a simple persistence for storing and retrieving messages in a conversation.\\n\\nAt that time, the only option for orchestrating LangChain chains was via [LCEL](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel). To incorporate memory with `LCEL`, users had to use the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"As of LangChain v0.3, we recommend that **new** code takes advantage of LangGraph for both orchestration and persistence:\\n\\n- Orchestration: In LangGraph, users define [graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/) that specify the flow of the application. This allows users to keep using `LCEL` within individual nodes when `LCEL` is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n- Persistence: Users can rely on LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the `RunnableWithMessageHistory` interface.\"), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=':::important\\nIf you have been using `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\\n:::\\n\\n## Migrations\\n\\n:::info Prerequisites\\n\\nThese guides assume some familiarity with the following concepts:\\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\\n- [v0.0.x Memory](https://python.langchain.com/v0.1/docs/modules/memory/)\\n- [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\\n:::\\n\\n### 1. Managing conversation history\\n\\nThe goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Often this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\\n\\nMemory classes that fall into this category include:'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| Memory Type                       | How to Migrate                                                                                                                                              | Description                                                                                                                                                                                                         |\\n|-----------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `ConversationBufferMemory`        | [Link to Migration Guide](conversation_buffer_memory)                                                                                                       | A basic memory implementation that simply stores the conversation history.                                                                                                                                          |\\n| `ConversationStringBufferMemory`  | [Link to Migration Guide](conversation_buffer_memory)                                                                                                       | A special case of `ConversationBufferMemory` designed for LLMs and no longer relevant.                                                                                                                              |\\n| `ConversationBufferWindowMemory`  | [Link to Migration Guide](conversation_buffer_window_memory)                                                                                                | Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.                                                                                                                        |\\n| `ConversationTokenBufferMemory`   | [Link to Migration Guide](conversation_buffer_window_memory)                                                                                                | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.                                                   |\\n| `ConversationSummaryMemory`       | [Link to Migration Guide](conversation_summary_memory)                                                                                                      | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.                                              |\\n| `ConversationSummaryBufferMemory` | [Link to Migration Guide](conversation_summary_memory)                                                                                                      | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\\n| `VectorStoreRetrieverMemory`      | See related [long-term memory agent tutorial](long_term_memory_agent) | Stores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.                                                                                    |'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='### 2. Extraction of structured information from the conversation history\\n\\nPlease see [long-term memory agent tutorial](long_term_memory_agent) implements an agent that can extract structured information from the conversation history.\\n\\nMemory classes that fall into this category include:'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| Memory Type                | Description                                                                                                                                                                                                       |\\n|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `BaseEntityStore`          | An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs. |\\n| `ConversationEntityMemory` | Combines the ability to summarize the conversation while extracting structured information from the conversation history.                                                                                         |'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='And specific backend implementations of abstractions:\\n\\n| Memory Type               | Description                                                                                              |\\n|---------------------------|----------------------------------------------------------------------------------------------------------|\\n| `InMemoryEntityStore`     | An implementation of `BaseEntityStore` that stores the information in the literal computer memory (RAM). |\\n| `RedisEntityStore`        | A specific implementation of `BaseEntityStore` that uses Redis as the backend.                           |\\n| `SQLiteEntityStore`       | A specific implementation of `BaseEntityStore` that uses SQLite as the backend.                          |\\n| `UpstashRedisEntityStore` | A specific implementation of `BaseEntityStore` that uses Upstash as the backend.                         |'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"These abstractions have received limited development since their initial release. This is because they generally require significant customization for a specific application to be effective, making\\nthem less widely used than the conversation history management abstractions.\\n\\nFor this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application\\nthat relies on these abstractions, please:\\n1) Please review this [Long-term memory agent tutorial](long_term_memory_agent) which should provide a good starting point for how to extract structured information from the conversation history.\\n2) If you're still struggling, please open an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.\"), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='The general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.\\nThe extracted information can then be saved into an appropriate data structure (e.g., a dictionary), and information from it can be retrieved and added into the prompt as needed.\\n\\n### 3. Implementations that provide composite logic on top of one or more memory implementations\\n\\nMemory classes that fall into this category include:'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| Memory Type            | Description                                                                                                                    |\\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------|\\n| `CombinedMemory`       | This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input. |\\n| `SimpleMemory`         | Used to add read-only hard-coded context. Users can simply write this information into the prompt.                             |\\n| `ReadOnlySharedMemory` | Provided a read-only view of an existing `BaseMemory` implementation.                                                          |\\n\\nThese implementations did not seem to be used widely or provide significant value. Users should be able\\nto re-implement these without too much difficulty in custom code.\\n\\n## Related Resources'), Document(metadata={'source': 'docs/docs/versions/migrating_memory/index.mdx', 'file_path': 'docs/docs/versions/migrating_memory/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Explore persistence with LangGraph:\\n\\n* [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n* [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\\n* [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)\\n* [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)\\n\\nAdd persistence with simple LCEL (favor langgraph for more complex use cases):\\n\\n* [How to add message history](https://python.langchain.com/docs/how_to/message_history/)\\n\\nWorking with message history:\\n\\n* [How to trim messages](https://python.langchain.com/docs/how_to/trim_messages)\\n* [How to filter messages](https://python.langchain.com/docs/how_to/filter_messages/)\\n* [How to merge message runs](https://python.langchain.com/docs/how_to/merge_message_runs/)'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content=\"---\\nsidebar_position: 3\\nsidebar_label: Changes\\nkeywords: [retrievalqa, llmchain, conversationalretrievalchain]\\n---\\n\\n# Deprecations and Breaking Changes\\n\\nThis code contains a list of deprecations and removals in the `langchain` and `langchain-core` packages.\\n\\nNew features and improvements are not listed here. See the [overview](/docs/versions/v0_2/overview/) for a summary of what's new in this release.\\n\\n## Breaking changes\\n\\nAs of release 0.2.0, `langchain` is required to be integration-agnostic. This means that code in `langchain`  should not by default instantiate any specific chat models, llms, embedding models, vectorstores etc; instead, the user will be required to specify those explicitly.\\n\\nThe following functions and classes require an explicit LLM to be passed as an argument:\"), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='- `langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit`\\n- `langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit`\\n- `langchain.chains.openai_functions.get_openapi_chain`\\n- `langchain.chains.router.MultiRetrievalQAChain.from_retrievers`\\n- `langchain.indexes.VectorStoreIndexWrapper.query`\\n- `langchain.indexes.VectorStoreIndexWrapper.query_with_sources`\\n- `langchain.indexes.VectorStoreIndexWrapper.aquery_with_sources`\\n- `langchain.chains.flare.FlareChain`\\n\\n\\nThe following classes now require passing an explicit Embedding model as an argument:\\n\\n- `langchain.indexes.VectostoreIndexCreator`\\n\\nThe following code has been removed:\\n\\n- `langchain.natbot.NatBotChain.from_default` removed in favor of the `from_llm` class method.\\n\\nBehavior was changed for the following code:\\n\\n\\n### @tool decorator\\n\\n`@tool` decorator now assigns the function doc-string as the tool description. Previously, the `@tool` decorator\\nusing to prepend the function signature.'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Before 0.2.0:\\n\\n```python\\n@tool\\ndef my_tool(x: str) -> str:\\n    \"\"\"Some description.\"\"\"\\n    return \"something\"\\n\\nprint(my_tool.description)\\n```\\n\\nWould result in: `my_tool: (x: str) -> str - Some description.`\\n\\nAs of 0.2.0:\\n\\nIt will result in: `Some description.`\\n\\n## Code that moved to another package\\n\\nCode that was moved from `langchain` into another package (e.g, `langchain-community`)\\n\\nIf you try to import it from `langchain`, the import will keep on working, but will raise a deprecation warning. The warning will provide a replacement import statement.\\n\\n ```shell\\n python -c \"from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\"\\n```\\n\\n ```shell\\n LangChainDeprecationWarning: Importing UnstructuredMarkdownLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\\n\\n >> from langchain.document_loaders import UnstructuredMarkdownLoader\\n\\n with new imports of:\\n\\n >> from langchain_community.document_loaders import UnstructuredMarkdownLoader\\n```'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='We will continue supporting the imports in `langchain` until release 0.4 as long as the relevant package where the code lives is installed. (e.g., as long as `langchain_community` is installed.)\\n\\nHowever, we advise for users to not rely on these imports and instead migrate to the new imports. To help with this process, we’re releasing a migration script via the LangChain CLI. See further instructions in migration guide.\\n\\n## Code targeted for removal\\n\\nCode that has better alternatives available and will eventually be removed, so there’s only a single way to do things. (e.g., `predict_messages` method in ChatModels has been deprecated in favor of `invoke`).\\n\\n### astream events V1\\n\\nIf you are using `astream_events`, please review how to [migrate to astream events v2](/docs/versions/v0_2/migrating_astream_events).\\n\\n### langchain_core\\n\\n#### try_load_from_hub\\n\\n\\nIn module: `utils.loading`\\nDeprecated: 0.1.30\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: Using the hwchase17/langchain-hub repo for prompts is deprecated. Please use https://smith.langchain.com/hub instead.\\n\\n\\n#### BaseLanguageModel.predict\\n\\n\\nIn module: `language_models.base`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseLanguageModel.predict_messages\\n\\n\\nIn module: `language_models.base`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseLanguageModel.apredict\\n\\n\\nIn module: `language_models.base`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### BaseLanguageModel.apredict_messages\\n\\n\\nIn module: `language_models.base`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### RunTypeEnum\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Use string instead.\\n\\n\\n#### TracerSessionV1Base\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### TracerSessionV1Create\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='#### TracerSessionV1\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### TracerSessionBase\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### TracerSession\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### BaseRun\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Run\\n\\n\\n#### LLMRun\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Run\\n\\n\\n#### ChainRun\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Run\\n\\n\\n#### ToolRun\\n\\n\\nIn module: `tracers.schemas`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Run\\n\\n\\n#### BaseChatModel.__call__\\n\\n\\nIn module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseChatModel.call_as_llm\\n\\n\\nIn module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseChatModel.predict'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='In module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseChatModel.predict_messages\\n\\n\\nIn module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseChatModel.apredict\\n\\n\\nIn module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### BaseChatModel.apredict_messages\\n\\n\\nIn module: `language_models.chat_models`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### BaseLLM.__call__\\n\\n\\nIn module: `language_models.llms`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseLLM.predict\\n\\n\\nIn module: `language_models.llms`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseLLM.predict_messages\\n\\n\\nIn module: `language_models.llms`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseLLM.apredict\\n\\n\\nIn module: `language_models.llms`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### BaseLLM.apredict_messages'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='In module: `language_models.llms`\\nDeprecated: 0.1.7\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### BaseRetriever.get_relevant_documents\\n\\n\\nIn module: `retrievers`\\nDeprecated: 0.1.46\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### BaseRetriever.aget_relevant_documents\\n\\n\\nIn module: `retrievers`\\nDeprecated: 0.1.46\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### ChatPromptTemplate.from_role_strings\\n\\n\\nIn module: `prompts.chat`\\nDeprecated: 0.0.1\\nRemoval:\\n\\n\\nAlternative: from_messages classmethod\\n\\n\\n#### ChatPromptTemplate.from_strings\\n\\n\\nIn module: `prompts.chat`\\nDeprecated: 0.0.1\\nRemoval:\\n\\n\\nAlternative: from_messages classmethod\\n\\n\\n#### BaseTool.__call__\\n\\n\\nIn module: `tools`\\nDeprecated: 0.1.47\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### convert_pydantic_to_openai_function\\n\\n\\nIn module: `utils.function_calling`\\nDeprecated: 0.1.16\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_core.utils.function_calling.convert_to_openai_function()\\n\\n\\n#### convert_pydantic_to_openai_tool'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='In module: `utils.function_calling`\\nDeprecated: 0.1.16\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_core.utils.function_calling.convert_to_openai_tool()\\n\\n\\n#### convert_python_function_to_openai_function\\n\\n\\nIn module: `utils.function_calling`\\nDeprecated: 0.1.16\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_core.utils.function_calling.convert_to_openai_function()\\n\\n\\n#### format_tool_to_openai_function\\n\\n\\nIn module: `utils.function_calling`\\nDeprecated: 0.1.16\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_core.utils.function_calling.convert_to_openai_function()\\n\\n\\n#### format_tool_to_openai_tool\\n\\n\\nIn module: `utils.function_calling`\\nDeprecated: 0.1.16\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_core.utils.function_calling.convert_to_openai_tool()\\n\\n\\n### langchain\\n\\n\\n#### AgentType\\n\\n\\nIn module: `agents.agent_types`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Use [LangGraph](/docs/how_to/migrate_agent/) or new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='#### Chain.__call__\\n\\n\\nIn module: `chains.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### Chain.acall\\n\\n\\nIn module: `chains.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### Chain.run\\n\\n\\nIn module: `chains.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: invoke\\n\\n\\n#### Chain.arun\\n\\n\\nIn module: `chains.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: ainvoke\\n\\n\\n#### Chain.apply\\n\\n\\nIn module: `chains.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: batch\\n\\n\\n#### LLMChain\\n\\n\\nIn module: `chains.llm`\\nDeprecated: 0.1.17\\nRemoval: 0.3.0\\n\\n\\nAlternative: [RunnableSequence](/docs/how_to/sequence/), e.g., `prompt | llm`\\n\\nThis [migration guide](/docs/versions/migrating_chains/llm_chain) has a side-by-side comparison.\\n\\n\\n#### LLMSingleActionAgent\\n\\n\\nIn module: `agents.agent`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: Use [LangGraph](/docs/how_to/migrate_agent/) or new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.\\n\\n\\n#### Agent\\n\\n\\nIn module: `agents.agent`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: Use [LangGraph](/docs/how_to/migrate_agent/) or new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.\\n\\n\\n#### OpenAIFunctionsAgent\\n\\n\\nIn module: `agents.openai_functions_agent.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_openai_functions_agent\\n\\n\\n#### ZeroShotAgent\\n\\n\\nIn module: `agents.mrkl.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_react_agent\\n\\n\\n#### MRKLChain\\n\\n\\nIn module: `agents.mrkl.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### ConversationalAgent\\n\\n\\nIn module: `agents.conversational.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_react_agent\\n\\n\\n#### ConversationalChatAgent'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='In module: `agents.conversational_chat.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_json_chat_agent\\n\\n\\n#### ChatAgent\\n\\n\\nIn module: `agents.chat.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_react_agent\\n\\n\\n#### OpenAIMultiFunctionsAgent\\n\\n\\nIn module: `agents.openai_functions_multi_agent.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_openai_tools_agent\\n\\n\\n#### ReActDocstoreAgent\\n\\n\\nIn module: `agents.react.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### DocstoreExplorer\\n\\n\\nIn module: `agents.react.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### ReActTextWorldAgent\\n\\n\\nIn module: `agents.react.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### ReActChain\\n\\n\\nIn module: `agents.react.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### SelfAskWithSearchAgent\\n\\n\\nIn module: `agents.self_ask_with_search.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_self_ask_with_search\\n\\n\\n#### SelfAskWithSearchChain'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='In module: `agents.self_ask_with_search.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### StructuredChatAgent\\n\\n\\nIn module: `agents.structured_chat.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_structured_chat_agent\\n\\n\\n#### RetrievalQA\\n\\n\\nIn module: `chains.retrieval_qa.base`\\nDeprecated: 0.1.17\\nRemoval: 0.3.0\\n\\n\\nAlternative: [create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain-chains-retrieval-create-retrieval-chain)\\nThis [migration guide](/docs/versions/migrating_chains/retrieval_qa) has a side-by-side comparison.\\n\\n\\n#### load_agent_from_config\\n\\n\\nIn module: `agents.loading`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### load_agent\\n\\n\\nIn module: `agents.loading`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative:\\n\\n\\n#### initialize_agent\\n\\n\\nIn module: `agents.initialize`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: Use [LangGraph](/docs/how_to/migrate_agent/) or new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.\\n\\n\\n#### XMLAgent\\n\\n\\nIn module: `agents.xml.base`\\nDeprecated: 0.1.0\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_xml_agent\\n\\n\\n#### CohereRerank\\n\\n\\nIn module: `retrievers.document_compressors.cohere_rerank`\\nDeprecated: 0.0.30\\nRemoval: 0.3.0\\n\\n\\nAlternative: langchain_cohere.CohereRerank\\n\\n\\n#### ConversationalRetrievalChain\\n\\n\\nIn module: `chains.conversational_retrieval.base`\\nDeprecated: 0.1.17\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: [create_history_aware_retriever](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html) together with [create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain-chains-retrieval-create-retrieval-chain) (see example in docstring)\\nThis [migration guide](/docs/versions/migrating_chains/conversation_retrieval_chain) has a side-by-side comparison.\\n\\n\\n#### create_extraction_chain_pydantic\\n\\n\\nIn module: `chains.openai_tools.extraction`\\nDeprecated: 0.1.14\\nRemoval: 0.3.0\\n\\n\\nAlternative: [with_structured_output](/docs/how_to/structured_output/#the-with_structured_output-method) method on chat models that support tool calling.\\n\\n\\n#### create_openai_fn_runnable\\n\\n\\nIn module: `chains.structured_output.base`\\nDeprecated: 0.1.14\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: [with_structured_output](/docs/how_to/structured_output/#the-with_structured_output-method) method on chat models that support tool calling.\\n\\n\\n#### create_structured_output_runnable\\n\\n\\nIn module: `chains.structured_output.base`\\nDeprecated: 0.1.17\\nRemoval: 0.3.0\\n\\n\\nAlternative: [with_structured_output](/docs/how_to/structured_output/#the-with_structured_output-method) method on chat models that support tool calling.\\n\\n\\n#### create_openai_fn_chain\\n\\n\\nIn module: `chains.openai_functions.base`\\nDeprecated: 0.1.1\\nRemoval: 0.3.0\\n\\n\\nAlternative: create_openai_fn_runnable\\n\\n\\n#### create_structured_output_chain\\n\\n\\nIn module: `chains.openai_functions.base`\\nDeprecated: 0.1.1\\nRemoval: 0.3.0\\n\\nAlternative: ChatOpenAI.with_structured_output\\n\\n\\n#### create_extraction_chain\\n\\n\\nIn module: `chains.openai_functions.extraction`\\nDeprecated: 0.1.14\\nRemoval: 0.3.0'), Document(metadata={'source': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_path': 'docs/docs/versions/v0_2/deprecations.mdx', 'file_name': 'deprecations.mdx', 'file_type': '.mdx'}, page_content='Alternative: [with_structured_output](/docs/how_to/structured_output/#the-with_structured_output-method) method on chat models that support tool calling.\\n\\n\\n#### create_extraction_chain_pydantic\\n\\n\\nIn module: `chains.openai_functions.extraction`\\nDeprecated: 0.1.14\\nRemoval: 0.3.0\\n\\n\\nAlternative: [with_structured_output](/docs/how_to/structured_output/#the-with_structured_output-method) method on chat models that support tool calling.'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 1\\n---\\n\\n# Migration\\n\\nLangChain v0.2 was released in May 2024. This release includes a number of [breaking changes and deprecations](/docs/versions/v0_2/deprecations). This document contains a guide on upgrading to 0.2.x.\\n\\n:::note Reference\\n\\n- [Breaking Changes & Deprecations](/docs/versions/v0_2/deprecations)\\n- [Migrating legacy chains to LCEL](/docs/versions/migrating_chains)\\n- [Migrating to Astream Events v2](/docs/versions/v0_2/migrating_astream_events)\\n\\n:::\\n\\n# Migration\\n\\nThis documentation will help you upgrade your code to LangChain `0.2.x.`. To prepare for migration, we first recommend you take the following steps:'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='1. Install the 0.2.x versions of langchain-core, langchain and upgrade to recent versions of other packages that you may be using. (e.g. langgraph, langchain-community, langchain-openai, etc.)\\n2. Verify that your code runs properly with the new packages (e.g., unit tests pass).\\n3. Install a recent version of `langchain-cli` , and use the tool to replace old imports used by your code with the new imports. (See instructions below.)\\n4. Manually resolve any remaining deprecation warnings.\\n5. Re-run unit tests.\\n6. If you are using `astream_events`, please review how to [migrate to astream events v2](/docs/versions/v0_2/migrating_astream_events).\\n\\n## Upgrade to new imports\\n\\nWe created a tool to help migrate your code. This tool is still in **beta** and may not cover all cases, but\\nwe hope that it will help you migrate your code more quickly.\\n\\nThe migration script has the following limitations:'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='1. It’s limited to helping users move from old imports to new imports. It does not help address other deprecations.\\n2. It can’t handle imports that involve `as` .\\n3. New imports are always placed in global scope, even if the old import that was replaced was located inside some local scope (e..g, function body).\\n4. It will likely miss some deprecated imports.\\n\\nHere is an example of the import changes that the migration script can help apply automatically:'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| From Package        | To Package               | Deprecated Import                                                  | New Import                                                          |\\n|---------------------|--------------------------|--------------------------------------------------------------------|---------------------------------------------------------------------|\\n| langchain           | langchain-community      | from langchain.vectorstores import InMemoryVectorStore             | from langchain_community.vectorstores import InMemoryVectorStore    |\\n| langchain-community | langchain_openai         | from langchain_community.chat_models import ChatOpenAI             | from langchain_openai import ChatOpenAI                             |\\n| langchain-community | langchain-core           | from langchain_community.document_loaders import Blob              | from langchain_core.document_loaders import Blob                    |\\n| langchain           | langchain-core           | from langchain.schema.document import Document                     | from langchain_core.documents import Document                       |\\n| langchain           | langchain-text-splitters | from langchain.text_splitter import RecursiveCharacterTextSplitter | from langchain_text_splitters import RecursiveCharacterTextSplitter |'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='## Installation\\n\\n```bash\\npip install langchain-cli\\nlangchain-cli --version # <-- Make sure the version is at least 0.0.22\\n```\\n\\n## Usage\\n\\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\\n\\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\\n\\nFor example, say your code still uses `from langchain.chat_models import ChatOpenAI`:\\n\\nAfter the first run, you’ll get: `from langchain_community.chat_models import ChatOpenAI`\\nAfter the second run, you’ll get: `from langchain_openai import ChatOpenAI`\\n\\n```bash\\n# Run a first time\\n# Will replace from langchain.chat_models import ChatOpenAI\\nlangchain-cli migrate --diff [path to code] # Preview\\nlangchain-cli migrate [path to code] # Apply\\n\\n# Run a second time to apply more import replacements\\nlangchain-cli migrate --diff [path to code] # Preview\\nlangchain-cli migrate [path to code] # Apply\\n```'), Document(metadata={'source': 'docs/docs/versions/v0_2/index.mdx', 'file_path': 'docs/docs/versions/v0_2/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='### Other options\\n\\n```bash\\n# See help menu\\nlangchain-cli migrate --help\\n# Preview Changes without applying\\nlangchain-cli migrate --diff [path to code]\\n# Run on code including ipython notebooks\\n# Apply all import updates except for updates from langchain to langchain-core\\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\\n```'), Document(metadata={'source': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_path': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_name': 'migrating_astream_events.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 2\\nsidebar_label: astream_events v2\\n---\\n\\n# Migrating to astream_events(..., version=\"v2\")\\n\\nWe\\'ve added a `v2` of the astream_events API with the release of `0.2.x`. You can see this [PR](https://github.com/langchain-ai/langchain/pull/21638) for more details.\\n\\nThe `v2` version is a re-write of the `v1` version, and should be more efficient, with more consistent output for the events. The `v1` version of the API will be deprecated in favor of the `v2` version and will be removed in  `0.4.0`.\\n\\nBelow is a list of changes between the `v1` and `v2` versions of the API.\\n\\n\\n### output for `on_chat_model_end`\\n\\nIn `v1`, the outputs associated with `on_chat_model_end` changed depending on whether the\\nchat model was run as a root level runnable or as part of a chain.\\n\\nAs a root level runnable the output was:\\n\\n```python\\n\"data\": {\"output\": AIMessageChunk(content=\"hello world!\", id=\\'some id\\')}\\n```\\n\\nAs part of a chain the output was:'), Document(metadata={'source': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_path': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_name': 'migrating_astream_events.mdx', 'file_type': '.mdx'}, page_content='```\\n            \"data\": {\\n                \"output\": {\\n                    \"generations\": [\\n                        [\\n                            {\\n                                \"generation_info\": None,\\n                                \"message\": AIMessageChunk(\\n                                    content=\"hello world!\", id=AnyStr()\\n                                ),\\n                                \"text\": \"hello world!\",\\n                                \"type\": \"ChatGenerationChunk\",\\n                            }\\n                        ]\\n                    ],\\n                    \"llm_output\": None,\\n                }\\n            },\\n```\\n\\n\\nAs of `v2`, the output will always be the simpler representation:\\n\\n```python\\n\"data\": {\"output\": AIMessageChunk(content=\"hello world!\", id=\\'some id\\')}\\n```\\n\\n:::note\\nNon chat models (i.e., regular LLMs) are will be consistently associated with the more verbose format for now.\\n:::\\n\\n### output for `on_retriever_end`'), Document(metadata={'source': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_path': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_name': 'migrating_astream_events.mdx', 'file_type': '.mdx'}, page_content='`on_retriever_end` output will always return a list of `Documents`.\\n\\nBefore:\\n```python\\n{\\n    \"data\": {\\n        \"output\": [\\n            Document(...),\\n            Document(...),\\n            ...\\n        ]\\n    }\\n}\\n```\\n\\n### Removed `on_retriever_stream`\\n\\nThe `on_retriever_stream` event was an artifact of the implementation and has been removed.\\n\\nFull information associated with the event is already available in the `on_retriever_end` event.\\n\\nPlease use `on_retriever_end` instead.\\n\\n### Removed `on_tool_stream`\\n\\nThe `on_tool_stream` event was an artifact of the implementation and has been removed.\\n\\nFull information associated with the event is already available in the `on_tool_end` event.\\n\\nPlease use `on_tool_end` instead.\\n\\n### Propagating Names\\n\\nNames of runnables have been updated to be more consistent.'), Document(metadata={'source': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_path': 'docs/docs/versions/v0_2/migrating_astream_events.mdx', 'file_name': 'migrating_astream_events.mdx', 'file_type': '.mdx'}, page_content='```python\\nmodel = GenericFakeChatModel(messages=infinite_cycle).configurable_fields(\\n    messages=ConfigurableField(\\n        id=\"messages\",\\n        name=\"Messages\",\\n        description=\"Messages return by the LLM\",\\n    )\\n)\\n```\\n\\nIn `v1`, the event name was `RunnableConfigurableFields`.\\n\\nIn `v2`, the event name is `GenericFakeChatModel`.\\n\\nIf you\\'re filtering by event names, check if you need to update your filters.\\n\\n### RunnableRetry\\n\\nUsage of [RunnableRetry](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.retry.RunnableRetry.html)\\nwithin an LCEL chain being streamed generated an incorrect `on_chain_end` event in `v1` corresponding\\nto the failed runnable invocation that was being retried. This event has been removed in `v2`.\\n\\nNo action is required for this change.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_position: 0\\n---\\n\\n# Overview\\n\\n## What’s new in LangChain?\\n\\nThe following features have been added during the development of 0.1.x:'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='- Better streaming support via the [Event Streaming API](https://python.langchain.com/docs/expression_language/streaming/#using-stream-events).\\n- [Standardized tool calling support](https://blog.langchain.dev/tool-calling-with-langchain/)\\n- A standardized interface for [structuring output](https://github.com/langchain-ai/langchain/discussions/18154)\\n- [@chain decorator](https://python.langchain.com/docs/expression_language/how_to/decorator/) to more easily create **RunnableLambdas**\\n- https://python.langchain.com/docs/expression_language/how_to/inspect/\\n- In Python, better async support for many core abstractions (thank you [@cbornet](https://github.com/cbornet)!!)\\n- Include response metadata in `AIMessage` to make it easy to access raw output from the underlying models\\n- Tooling to visualize [your runnables](https://python.langchain.com/docs/expression_language/how_to/inspect/) or [your langgraph app](https://github.com/langchain-ai/langgraph/blob/main/examples/visualization.ipynb)\\n- Interoperability of chat message histories across most providers\\n- [Over 20+ partner packages in python](https://python.langchain.com/docs/integrations/providers/) for popular integrations'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='## What’s coming to LangChain?\\n\\n- We’ve been working hard on [langgraph](https://langchain-ai.github.io/langgraph/). We will be building more capabilities on top of it and focusing on making it the go-to framework for agent architectures.\\n- Vectorstores V2! We’ll be revisiting our vectorstores abstractions to help improve usability and reliability.\\n- Better documentation and versioned docs!\\n- We’re planning a breaking release (0.3.0) sometime between July-September to [upgrade to full support of Pydantic 2](https://github.com/langchain-ai/langchain/discussions/19339), and will drop support for Pydantic 1 (including objects originating from the `v1` namespace of Pydantic 2).\\n\\n## What changed?\\n\\nDue to the rapidly evolving field, LangChain has also evolved rapidly.\\n\\nThis document serves to outline at a high level what has changed and why.\\n\\n### TLDR\\n\\n**As of 0.2.0:**'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='- This release completes the work that we started with release 0.1.0 by removing the dependency of `langchain` on `langchain-community`.\\n- `langchain` package no longer requires `langchain-community` . Instead `langchain-community` will now depend on `langchain-core` and `langchain` .\\n- User code that still relies on deprecated imports from `langchain` will continue to work as long `langchain_community` is installed. These imports will start raising errors in release 0.4.x.\\n\\n**As of 0.1.0:**\\n\\n- `langchain` was split into the following component packages: `langchain-core`, `langchain`, `langchain-community`, `langchain-[partner]` to improve the usability of langchain code in production settings. You can read more about it on our [blog](https://blog.langchain.dev/langchain-v0-1-0/).\\n\\n### Ecosystem organization\\n\\nBy the release of 0.1.0, LangChain had grown to a large ecosystem with many integrations and a large community.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='To improve the usability of LangChain in production, we split the single `langchain` package into multiple packages. This allowed us to create a good foundation architecture for the LangChain ecosystem and improve the usability of `langchain` in production.\\n\\nHere is the high level break down of the Eco-system:'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='- **langchain-core**:  contains core abstractions involving LangChain Runnables, tooling for observability, and base implementations of important abstractions (e.g., Chat Models).\\n- **langchain:** contains generic code that is built using interfaces defined in `langchain-core`. This package is for code that generalizes well across different implementations of specific interfaces. For example, `create_tool_calling_agent` works across chat models that support [tool calling capabilities](https://blog.langchain.dev/tool-calling-with-langchain/).\\n- **langchain-community**: community maintained 3rd party integrations. Contains integrations based on interfaces defined in **langchain-core**. Maintained by the LangChain community.\\n- **Partner Packages (e.g., langchain-[partner])**: Partner packages are packages dedicated to especially popular integrations (e.g., `langchain-openai`, `langchain-anthropic` etc.). The dedicated packages generally benefit from better reliability and support.\\n- `langgraph`: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\n- `langserve`: Deploy LangChain chains as REST APIs.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='In the 0.1.0 release, `langchain-community` was retained as required a dependency of `langchain`.\\n\\nThis allowed imports of vectorstores, chat models, and other integrations to continue working through `langchain`\\nrather than forcing users to update all of their imports to `langchain-community`.\\n\\nFor the 0.2.0 release, we’re removing the dependency of `langchain` on `langchain-community`. This is something we’ve been planning to do since the 0.1 release because we believe this is the right package architecture.\\n\\nOld imports will continue to work as long as `langchain-community` is installed. These imports will be removed in the 0.4.0 release.\\n\\nTo understand why we think breaking the dependency of `langchain` on `langchain-community` is best we should understand what each package is meant to do.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='`langchain` is meant to contain high-level chains and agent architectures. The logic in these should be specified at the level of abstractions like `ChatModel` and `Retriever`, and should not be specific to any one integration. This has two main benefits:\\n\\n1. `langchain` is fairly lightweight. Here is the full list of required dependencies (after the split)\\n\\n    ```toml\\n    python = \">=3.8.1,<4.0\"\\n    langchain-core = \"^0.2.0\"\\n    langchain-text-splitters = \">=0.0.1,<0.1\"\\n    langsmith = \"^0.1.17\"\\n    pydantic = \">=1,<3\"\\n    SQLAlchemy = \">=1.4,<3\"\\n    requests = \"^2\"\\n    PyYAML = \">=5.3\"\\n    numpy = \"^1\"\\n    aiohttp = \"^3.8.3\"\\n    tenacity = \"^8.1.0\"\\n    jsonpatch = \"^1.33\"\\n    ```\\n\\n2. `langchain` chains/agents are largely integration-agnostic, which makes it easy to experiment with different integrations and future-proofs your code should there be issues with one specific integration.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='There is also a third less tangible benefit which is that being integration-agnostic forces us to find only those very generic abstractions and architectures which generalize well across integrations. Given how general the abilities of the foundational tech are, and how quickly the space is moving, having generic architectures is a good way of future-proofing your applications.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='`langchain-community` is intended to have all integration-specific components that are not yet being maintained in separate `langchain-{partner}` packages. Today this is still the majority of integrations and a lot of code. This code is primarily contributed by the community, while `langchain` is largely written by core maintainers. All of these integrations use optional dependencies and conditional imports, which prevents dependency bloat and conflicts but means compatible dependency versions are not made explicit. Given the volume of integrations in `langchain-community` and the speed at which integrations change, it’s very hard to follow semver versioning, and we currently don’t.'), Document(metadata={'source': 'docs/docs/versions/v0_2/overview.mdx', 'file_path': 'docs/docs/versions/v0_2/overview.mdx', 'file_name': 'overview.mdx', 'file_type': '.mdx'}, page_content='All of which is to say that there’s no large benefits to `langchain` depending on `langchain-community` and some obvious downsides: the functionality in `langchain` should be integration agnostic anyways, `langchain-community` can’t be properly versioned, and depending on `langchain-community` increases the [vulnerability surface](https://github.com/langchain-ai/langchain/discussions/19083) of `langchain`.\\n\\nFor more context about the reason for the organization please see our blog: https://blog.langchain.dev/langchain-v0-1-0/'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"# LangChain v0.3\\n\\n*Last updated: 09.16.24*\\n\\n## What's changed\\n\\n* All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like `langchain_core.pydantic_v1` or `pydantic.v1`.\\n* Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024.\\n* Python 3.8 will no longer be supported as its end-of-life is October 2024.\\n\\n**These are the only breaking changes.**\\n\\n## What’s new\\n\\nThe following features have been added during the development of 0.2.x:\"), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- Moved more integrations from `langchain-community` to their own `langchain-x` packages. This is a non-breaking change, as the legacy implementations are left in `langchain-community` and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the [API reference](https://python.langchain.com/v0.2/api_reference/reference.html#integrations).\\n- Simplified tool definition and usage. Read more [here](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/).\\n- Added utilities for interacting with chat models: [universal model constructor](https://python.langchain.com/v0.2/docs/how_to/chat_models_universal_init/), [rate limiter](https://python.langchain.com/v0.2/docs/how_to/chat_model_rate_limiting/), [message utilities](https://python.langchain.com/v0.2/docs/how_to/#messages),\\n- Added the ability to [dispatch custom events](https://python.langchain.com/v0.2/docs/how_to/callbacks_custom_events/).\\n- Revamped integration docs and API reference. Read more [here](https://blog.langchain.dev/langchain-integration-docs-revamped/).\\n- Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in `langchain` 1.0.0. See the deprecated chains and associated [migration guides here](https://python.langchain.com/v0.2/docs/versions/migrating_chains/).'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"## How to update your code\\n\\nIf you're using `langchain` / `langchain-community` / `langchain-core` 0.0 or 0.1, we recommend that you first [upgrade to 0.2](https://python.langchain.com/v0.2/docs/versions/v0_2/).\\n\\nIf you're using `langgraph`, upgrade to `langgraph>=0.2.20,<0.3`. This will work with either 0.2 or 0.3 versions of all the base packages.\\n\\nHere is a complete list of all packages that have been released and what we recommend upgrading your version constraints to.\\nAny package that now requires `langchain-core` 0.3 had a minor version bump.\\nAny package that is now compatible with both `langchain-core` 0.2 and 0.3 had a patch version bump.\\n\\nYou can use the `langchain-cli` to update deprecated imports automatically.\\nThe CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as\\nwell as updating the `langchain_core.pydantic_v1` and `langchain.pydantic_v1` imports.\\n\\n\\n### Base packages\"), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| Package                  | Latest | Recommended constraint |\\n|--------------------------|--------|------------------------|\\n| langchain                | 0.3.0  | >=0.3,&lt;0.4             |\\n| langchain-community      | 0.3.0  | >=0.3,&lt;0.4             |\\n| langchain-text-splitters | 0.3.0  | >=0.3,&lt;0.4             |\\n| langchain-core           | 0.3.0  | >=0.3,&lt;0.4             |\\n| langchain-experimental   | 0.3.0  | >=0.3,&lt;0.4             |\\n\\n### Downstream packages\\n\\n| Package   | Latest | Recommended constraint |\\n|-----------|--------|------------------------|\\n| langgraph | 0.2.20 | >=0.2.20,&lt;0.3          |\\n| langserve | 0.3.0  | >=0.3,&lt;0.4             |\\n\\n### Integration packages'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='| Package                                | Latest  | Recommended constraint     |\\n| -------------------------------------- | ------- | -------------------------- |\\n| langchain-ai21                         | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-aws                          | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-anthropic                    | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-astradb                      | 0.4.1   | >=0.4.1,&lt;0.5               |\\n| langchain-azure-dynamic-sessions       | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-box                          | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-chroma                       | 0.1.4   | >=0.1.4,&lt;0.2               |\\n| langchain-cohere                       | 0.3.0   | >=0.3,&lt;0.4                 |\\n| langchain-elasticsearch                | 0.3.0   | >=0.3,&lt;0.4                 |\\n| langchain-exa                          | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-fireworks                    | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-groq                         | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-google-community             | 2.0.0   | >=2,&lt;3                     |\\n| langchain-google-genai                 | 2.0.0   | >=2,&lt;3                     |\\n| langchain-google-vertexai              | 2.0.0   | >=2,&lt;3                     |\\n| langchain-huggingface                  | 0.1.0   | >=0.1,&lt;0.2                 |\\n| langchain-ibm                          | 0.3.0   | >=0.3,&lt;0.4                 |\\n| langchain-milvus                       | 0.1.6   | >=0.1.6,&lt;0.2               |\\n| langchain-mistralai                    | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-mongodb                      | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-nomic                        | 0.1.3   | >=0.1.3,&lt;0.2               |\\n| langchain-nvidia                       | 0.3.0   | >=0.3,&lt;0.4                 |\\n| langchain-ollama                       | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-openai                       | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-pinecone                     | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-postgres                     | 0.0.13  | >=0.0.13,&lt;0.1              |\\n| langchain-prompty                      | 0.1.0   | >=0.1,&lt;0.2                 |\\n| langchain-qdrant                       | 0.1.4   | >=0.1.4,&lt;0.2               |\\n| langchain-redis                        | 0.1.0   | >=0.1,&lt;0.2                 |\\n| langchain-sema4                        | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-together                     | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-unstructured                 | 0.1.4   | >=0.1.4,&lt;0.2               |\\n| langchain-upstage                      | 0.3.0   | >=0.3,&lt;0.4                 |\\n| langchain-voyageai                     | 0.2.0   | >=0.2,&lt;0.3                 |\\n| langchain-weaviate                     | 0.0.3   | >=0.0.3,&lt;0.1               |'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"Once you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2:\\n\\n- If your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be `pydantic>=2,<3`.  See [Pydantic’s migration guide](https://docs.pydantic.dev/latest/migration/) for help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1.\\n- There are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions.\\n\\n## Common issues when transitioning to Pydantic 2\\n\\n### 1. Do not use the `langchain_core.pydantic_v1` namespace\\n\\nReplace any usage of `langchain_core.pydantic_v1` or `langchain.pydantic_v1` with\\ndirect imports from `pydantic`.\\n\\nFor example,\\n\\n```python\\nfrom langchain_core.pydantic_v1 import BaseModel\\n```\\n\\nto:\"), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='```python\\nfrom pydantic import BaseModel\\n```\\n\\nThis may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the [Pydantic Migration](https://docs.pydantic.dev/latest/migration/) for how to upgrade your code from Pydantic 1 to 2.\\n\\n### 2. Passing Pydantic objects to LangChain APIs\\n\\nUsers using the following APIs:\\n\\n* `BaseChatModel.bind_tools`\\n* `BaseChatModel.with_structured_output`\\n* `Tool.from_function`\\n* `StructuredTool.from_function`\\n\\nshould ensure that they are passing Pydantic 2 objects to these APIs rather than\\nPydantic 1 objects (created via the `pydantic.v1` namespace of pydantic 2).\\n\\n:::caution\\nWhile `v1` objects may be accepted by some of these APIs, users are advised to\\nuse Pydantic 2 objects to avoid future issues.\\n:::\\n\\n### 3. Sub-classing LangChain models\\n\\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\\nshould upgrade to use Pydantic 2 features.'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='For example, any user code that\\'s relying on Pydantic 1 features (e.g., `validator`) should\\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\\nwith imports from `pydantic`.\\n\\n```python\\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\\n# from pydantic import validator, Field # if pydantic 1 is installed\\n# from langchain_core.pydantic_v1 import validator, Field\\n# from langchain.pydantic_v1 import validator, Field\\n\\nclass CustomTool(BaseTool): # BaseTool is v1 code\\n    x: int = Field(default=1)\\n\\n    def _run(*args, **kwargs):\\n        return \"hello\"\\n\\n    @validator(\\'x\\') # v1 code\\n    @classmethod\\n    def validate_x(cls, x: int) -> int:\\n        return 1\\n```\\n\\nShould change to:\\n\\n```python\\nfrom pydantic import Field, field_validator # pydantic v2\\nfrom langchain_core.pydantic_v1 import BaseTool'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='class CustomTool(BaseTool): # BaseTool is v1 code\\n    x: int = Field(default=1)\\n\\n    def _run(*args, **kwargs):\\n        return \"hello\"\\n\\n    @field_validator(\\'x\\') # v2 code\\n    @classmethod\\n    def validate_x(cls, x: int) -> int:\\n        return 1\\n\\n\\nCustomTool(\\n    name=\\'custom_tool\\',\\n    description=\"hello\",\\n    x=1,\\n)\\n```\\n\\n### 4. model_rebuild()\\n\\nWhen sub-classing from LangChain models, users may need to add relevant imports\\nto the file and rebuild the model.\\n\\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\\n\\n```python\\nfrom langchain_core.output_parsers import BaseOutputParser\\n\\n\\nclass FooParser(BaseOutputParser):\\n    ...\\n```\\n\\nNew code:\\n\\n```python\\nfrom typing import Optional as Optional\\n\\nfrom langchain_core.output_parsers import BaseOutputParser\\n\\nclass FooParser(BaseOutputParser):\\n    ...\\n\\nFooParser.model_rebuild()\\n```\\n\\n## Migrate using langchain-cli'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='The `langchain-cli` can help update deprecated LangChain imports in your code automatically.\\n\\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\\nhelp to upgrade your code from pydantic 1 to pydantic 2.\\n\\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\\n\\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\\n\\n### Installation\\n\\n```bash\\npip install -U langchain-cli\\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\\n```\\n\\n### Usage\\n\\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='The `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\\n\\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\\n\\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\\n\\nAfter the first run, you’ll get: `from langchain_community.chat_models import ChatOpenAI`\\nAfter the second run, you’ll get: `from langchain_openai import ChatOpenAI`\\n\\n```bash\\n# Run a first time\\n# Will replace from langchain.chat_models import ChatOpenAI\\nlangchain-cli migrate --help [path to code] # Help\\nlangchain-cli migrate [path to code] # Apply\\n\\n# Run a second time to apply more import replacements\\nlangchain-cli migrate --diff [path to code] # Preview\\nlangchain-cli migrate [path to code] # Apply\\n```\\n\\n### Other options'), Document(metadata={'source': 'docs/docs/versions/v0_3/index.mdx', 'file_path': 'docs/docs/versions/v0_3/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='```bash\\n# See help menu\\nlangchain-cli migrate --help\\n# Preview Changes without applying\\nlangchain-cli migrate --diff [path to code]\\n# Approve changes interactively\\nlangchain-cli migrate --interactive [path to code]\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_path': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_name': 'guidelines.mdx', 'file_type': '.mdx'}, page_content='# General guidelines\\n\\nHere are some things to keep in mind for all types of contributions:'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_path': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_name': 'guidelines.mdx', 'file_type': '.mdx'}, page_content='- Follow the [\"fork and pull request\"](https://docs.github.com/en/get-started/exploring-projects-on-github/contributing-to-a-project) workflow.\\n- Fill out the checked-in pull request template when opening pull requests. Note related issues and tag relevant maintainers.\\n- Ensure your PR passes formatting, linting, and testing checks before requesting a review.\\n  - If you would like comments or feedback on your current progress, please open an issue or discussion and tag a maintainer.\\n  - See the sections on [Testing](setup.mdx#testing) and [Formatting and Linting](setup.mdx#formatting-and-linting) for how to run these checks locally.\\n- Backwards compatibility is key. Your changes must not be breaking, except in case of critical bug and security fixes.\\n- Look for duplicate PRs or issues that have already been opened before opening a new one.\\n- Keep scope as isolated as possible. As a general rule, your changes should not affect more than one package at a time.\\n\\n## Bugfixes'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_path': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_name': 'guidelines.mdx', 'file_type': '.mdx'}, page_content=\"We encourage and appreciate bugfixes. We ask that you:\\n\\n- Explain the bug in enough detail for maintainers to be able to reproduce it.\\n  - If an accompanying issue exists, link to it. Prefix with `Fixes` so that the issue will close automatically when the PR is merged.\\n- Avoid breaking changes if possible.\\n- Include unit tests that fail without the bugfix.\\n\\nIf you come across a bug and don't know how to fix it, we ask that you open an issue for it describing in detail the environment in which you encountered the bug.\\n\\n## New features\\n\\nWe aim to keep the bar high for new features. We generally don't accept new core abstractions, changes to infra, changes to dependencies,\\nor new agents/chains from outside contributors without an existing GitHub discussion or issue that demonstrates an acute need for them.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_path': 'docs/docs/contributing/how_to/code/guidelines.mdx', 'file_name': 'guidelines.mdx', 'file_type': '.mdx'}, page_content='- New features must come with docs, unit tests, and (if appropriate) integration tests.\\n- New integrations must come with docs, unit tests, and (if appropriate) integration tests.\\n  - See [this page](../integrations/index.mdx) for more details on contributing new integrations.\\n- New functionality should not inherit from or use deprecated methods or classes.\\n- We will reject features that are likely to lead to security vulnerabilities or reports.\\n- Do not add any hard dependencies. Integrations may add optional dependencies.'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/index.mdx', 'file_path': 'docs/docs/contributing/how_to/code/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Contribute Code\\n\\nIf you would like to add a new feature or update an existing one, please read the resources below before getting started:\\n\\n- [General guidelines](guidelines.mdx)\\n- [Setup](setup.mdx)'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='# Setup\\n\\nThis guide walks through how to run the repository locally and check in your first code.\\nFor a [development container](https://containers.dev/), see the [.devcontainer folder](https://github.com/langchain-ai/langchain/tree/master/.devcontainer).\\n\\n## Dependency Management: `uv` and other env/dependency managers\\n\\nThis project utilizes [uv](https://docs.astral.sh/uv/) v0.5+ as a dependency manager.\\n\\nInstall `uv`: **[documentation on how to install it](https://docs.astral.sh/uv/getting-started/installation/)**.\\n\\n## Different packages'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='This repository contains multiple packages:\\n- `langchain-core`: Base interfaces for key abstractions as well as logic for combining them in chains (LangChain Expression Language).\\n- `langchain-community`: Third-party integrations of various components.\\n- `langchain`: Chains, agents, and retrieval logic that makes up the cognitive architecture of your applications.\\n- `langchain-experimental`: Components and chains that are experimental, either in the sense that the techniques are novel and still being tested, or they require giving the LLM more access than would be possible in most production systems.\\n- Partner integrations: Partner packages in `libs/partners` that are independently version controlled.\\n\\nEach of these has its own development environment. Docs are run from the top-level makefile, but development\\nis split across separate test & release flows.\\n\\nFor this quickstart, start with langchain-community:\\n\\n```bash\\ncd libs/community\\n```\\n\\n## Local Development Dependencies'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='Install langchain-community development requirements (for running langchain, running examples, linting, formatting, tests, and coverage):\\n\\n```bash\\nuv sync\\n```\\n\\nThen verify dependency installation:\\n\\n```bash\\nmake test\\n```\\n\\n## Testing\\n\\n**Note:** In `langchain`, `langchain-community`, and `langchain-experimental`, some test dependencies are optional. See the following section about optional dependencies.\\n\\nUnit tests cover modular logic that does not require calls to outside APIs.\\nIf you add new logic, please add a unit test.\\n\\nTo run unit tests:\\n\\n```bash\\nmake test\\n```\\n\\nTo run unit tests in Docker:\\n\\n```bash\\nmake docker_tests\\n```\\n\\nThere are also [integration tests and code-coverage](../testing.mdx) available.\\n\\n### Only develop langchain_core or langchain_community\\n\\nIf you are only developing `langchain_core` or `langchain_community`, you can simply install the dependencies for the respective projects and run tests:\\n\\n```bash\\ncd libs/core\\nmake test\\n```\\n\\nOr:'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='```bash\\ncd libs/community\\nmake test\\n```\\n\\n## Formatting and Linting\\n\\nRun these locally before submitting a PR; the CI system will check also.\\n\\n### Code Formatting\\n\\nFormatting for this project is done via [ruff](https://docs.astral.sh/ruff/rules/).\\n\\nTo run formatting for docs, cookbook and templates:\\n\\n```bash\\nmake format\\n```\\n\\nTo run formatting for a library, run the same command from the relevant library directory:\\n\\n```bash\\ncd libs/{LIBRARY}\\nmake format\\n```\\n\\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\\n\\n```bash\\nmake format_diff\\n```\\n\\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\\n\\n#### Linting\\n\\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).'), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content=\"To run linting for docs, cookbook and templates:\\n\\n```bash\\nmake lint\\n```\\n\\nTo run linting for a library, run the same command from the relevant library directory:\\n\\n```bash\\ncd libs/{LIBRARY}\\nmake lint\\n```\\n\\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\\n\\n```bash\\nmake lint_diff\\n```\\n\\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\\n\\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\\n\\n### Spellcheck\"), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content=\"Spellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\\n\\nTo check spelling for this project:\\n\\n```bash\\nmake spell_check\\n```\\n\\nTo fix spelling in place:\\n\\n```bash\\nmake spell_fix\\n```\\n\\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\\n\\n```python\\n[tool.codespell]\\n...\\n# Add here:\\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\\n```\\n\\n## Working with Optional Dependencies\\n\\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\\n\\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content=\"You'll notice that `pyproject.toml` and `uv.lock` are **not** touched when you add optional dependencies below.\\n\\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\\nthat most users won't have it installed.\\n\\nUsers who do not have the dependency installed should be able to **import** your code without\\nany side effects (no warnings, no errors, no exceptions).\\n\\nTo introduce the dependency to a library, please do the following:\\n\\n1. Open extended_testing_deps.txt and add the dependency\\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\\ntest makes use of lightweight fixtures to test the logic of the code.\\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\\n\\n## Adding a Jupyter Notebook\\n\\nIf you are adding a Jupyter Notebook example, you'll want to run with `test` dependencies:\\n\\n```bash\\nuv run --group test jupyter notebook\\n```\"), Document(metadata={'source': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/code/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='When you run `uv sync`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/index.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Contribute Documentation\\n\\nDocumentation is a vital part of LangChain. We welcome both new documentation for new features and \\ncommunity improvements to our current documentation. Please read the resources below before getting started:\\n\\n- [Documentation style guide](style_guide.mdx)\\n- [Setup](setup.mdx)'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_class_name: \"hidden\"\\n---\\n\\n# Setup\\n\\nLangChain documentation consists of two components:\\n\\n1. Main Documentation: Hosted at [python.langchain.com](https://python.langchain.com/),\\nthis comprehensive resource serves as the primary user-facing documentation.\\nIt covers a wide array of topics, including tutorials, use cases, integrations,\\nand more, offering extensive guidance on building with LangChain.\\nThe content for this documentation lives in the `/docs` directory of the monorepo.\\n2. In-code Documentation: This is documentation of the codebase itself, which is also\\nused to generate the externally facing [API Reference](https://python.langchain.com/api_reference/langchain/index.html).\\nThe content for the API reference is autogenerated by scanning the docstrings in the codebase. For this reason we ask that\\ndevelopers document their code well.'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='The `API Reference` is largely autogenerated by [sphinx](https://www.sphinx-doc.org/en/master/)\\nfrom the code and is hosted by [Read the Docs](https://readthedocs.org/).\\n\\nWe appreciate all contributions to the documentation, whether it be fixing a typo,\\nadding a new tutorial or example and whether it be in the main documentation or the API Reference.\\n\\nSimilar to linting, we recognize documentation can be annoying. If you do not want\\nto do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\\n\\n## 📜 Main Documentation\\n\\nThe content for the main documentation is located in the `/docs` directory of the monorepo.\\n\\nThe documentation is written using a combination of ipython notebooks (`.ipynb` files)\\nand markdown (`.mdx` files). The notebooks are converted to markdown\\nand then built using [Docusaurus 2](https://docusaurus.io/).\\n\\nFeel free to make contributions to the main documentation! 🥰'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content=\"After modifying the documentation:\\n\\n1. Run the linting and formatting commands (see below) to ensure that the documentation is well-formatted and free of errors.\\n2. Optionally build the documentation locally to verify that the changes look good.\\n3. Make a pull request with the changes.\\n4. You can preview and verify that the changes are what you wanted by clicking the `View deployment` or `Visit Preview` buttons on the pull request `Conversation` page. This will take you to a preview of the documentation changes.\\n\\n## ⚒️ Linting and Building Documentation Locally\\n\\nAfter writing up the documentation, you may want to lint and build the documentation\\nlocally to ensure that it looks good and is free of errors.\\n\\nIf you're unable to build it locally that's okay as well, as you will be able to\\nsee a preview of the documentation on the pull request page.\\n\\n\\n### Building\\n\\nThe code that builds the documentation is located in the `/docs` directory of the monorepo.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content=\"In the following commands, the prefix `api_` indicates that those are operations for the API Reference.\\n\\nBefore building the documentation, it is always a good idea to clean the build directory:\\n\\n```bash\\nmake docs_clean\\nmake api_docs_clean\\n```\\n\\nNext, you can build the documentation as outlined below:\\n\\n```bash\\nmake docs_build\\nmake api_docs_build\\n```\\n\\n:::tip\\n\\nThe `make api_docs_build` command takes a long time. If you're making cosmetic changes to the API docs and want to see how they look, use:\\n\\n```bash\\nmake api_docs_quick_preview\\n```\\n\\nwhich will just build a small subset of the API reference.\\n\\n:::\\n\\nFinally, run the link checker to ensure all links are valid:\\n\\n```bash\\nmake docs_linkcheck\\nmake api_docs_linkcheck\\n```\\n\\n### Linting and Formatting\\n\\nThe Main Documentation is linted from the **monorepo root**. To lint the main documentation, run the following from there:\\n\\n```bash\\nmake lint\\n```\\n\\nIf you have formatting-related errors, you can fix them automatically with:\\n\\n```bash\\nmake format\\n```\"), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='## ⌨️ In-code Documentation\\n\\nThe in-code documentation is largely autogenerated by [sphinx](https://www.sphinx-doc.org/en/master/) from the code and is hosted by [Read the Docs](https://readthedocs.org/).\\n\\nFor the API reference to be useful, the codebase must be well-documented. This means that all functions, classes, and methods should have a docstring that explains what they do, what the arguments are, and what the return value is. This is a good practice in general, but it is especially important for LangChain because the API reference is the primary resource for developers to understand how to use the codebase.\\n\\nWe generally follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings) for docstrings.\\n\\nHere is an example of a well-documented function:\\n\\n```python\\n\\ndef my_function(arg1: int, arg2: str) -> float:\\n    \"\"\"This is a short description of the function. (It should be a single sentence.)'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='This is a longer description of the function. It should explain what\\n    the function does, what the arguments are, and what the return value is.\\n    It should wrap at 88 characters.\\n\\n    Examples:\\n        This is a section for examples of how to use the function.\\n\\n        .. code-block:: python\\n\\n            my_function(1, \"hello\")\\n\\n    Args:\\n        arg1: This is a description of arg1. We do not need to specify the type since\\n            it is already specified in the function signature.\\n        arg2: This is a description of arg2.\\n\\n    Returns:\\n        This is a description of the return value.\\n    \"\"\"\\n    return 3.14\\n```\\n\\n### Linting and Formatting\\n\\nThe in-code documentation is linted from the directories belonging to the packages\\nbeing documented.\\n\\nFor example, if you\\'re working on the `langchain-community` package, you would change\\nthe working directory to the `langchain-community` directory:\\n\\n```bash\\ncd [root]/libs/langchain-community\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/setup.mdx', 'file_name': 'setup.mdx', 'file_type': '.mdx'}, page_content='Then you can run the following commands to lint and format the in-code documentation:\\n\\n```bash\\nmake format\\nmake lint\\n```\\n\\n## Verify Documentation Changes\\n\\nAfter pushing documentation changes to the repository, you can preview and verify that the changes are\\nwhat you wanted by clicking the `View deployment` or `Visit Preview` buttons on the pull request `Conversation` page.\\nThis will take you to a preview of the documentation changes.\\nThis preview is created by [Vercel](https://vercel.com/docs/getting-started-with-vercel).'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar_class_name: \"hidden\"\\n---\\n\\n# Documentation Style Guide\\n\\nAs LangChain continues to grow, the amount of documentation required to cover the various concepts and integrations continues to grow too.\\nThis page provides guidelines for anyone writing documentation for LangChain and outlines some of our philosophies around\\norganization and structure.\\n\\n## Philosophy\\n\\nLangChain\\'s documentation follows the [Diataxis framework](https://diataxis.fr).\\nUnder this framework, all documentation falls under one of four categories: [Tutorials](#tutorials),\\n[How-to guides](#how-to-guides),\\n[References](#references), and [Explanations](#conceptual-guide).\\n\\n### Tutorials'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content=\"Tutorials are lessons that take the reader through a practical activity. Their purpose is to help the user\\ngain an understanding of concepts and how they interact by showing one way to achieve a specific goal in a hands-on manner. They should **avoid** giving\\nmultiple permutations of ways to achieve that goal in-depth. Instead, it should guide a new user through a recommended path to accomplish the tutorial's goal. While the end result of a tutorial does not necessarily need to\\nbe completely production-ready, it should be useful and practically satisfy the goal that is clearly stated in the tutorial's introduction. Information on how to address additional scenarios\\nbelongs in how-to guides.\\n\\nTo quote the Diataxis website:\\n\\n> A tutorial serves the user’s\\xa0*acquisition*\\xa0of skills and knowledge - their study. Its purpose is not to help the user get something done, but to help them learn.\\n\\nIn LangChain, these are often higher level guides that show off end-to-end use cases.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='Some examples include:\\n\\n- [Build a Simple LLM Application with LCEL](/docs/tutorials/llm_chain/)\\n- [Build a Retrieval Augmented Generation (RAG) App](/docs/tutorials/rag/)\\n\\nA good structural rule of thumb is to follow the structure of this [example from Numpy](https://numpy.org/numpy-tutorials/content/tutorial-svd.html).\\n  \\nHere are some high-level tips on writing a good tutorial:'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='- Focus on guiding the user to get something done, but keep in mind the end-goal is more to impart principles than to create a perfect production system.\\n- Be specific, not abstract and follow one path.\\n  - No need to go deeply into alternative approaches, but it’s ok to reference them, ideally with a link to an appropriate how-to guide.\\n- Get \"a point on the board\" as soon as possible - something the user can run that outputs something.\\n  - You can iterate and expand afterwards.\\n  - Try to frequently checkpoint at given steps where the user can run code and see progress.\\n- Focus on results, not technical explanation.\\n  - Crosslink heavily to appropriate conceptual/reference pages.\\n- The first time you mention a LangChain concept, use its full name (e.g. \"LangChain Expression Language (LCEL)\"), and link to its conceptual/other documentation page.\\n  - It\\'s also helpful to add a prerequisite callout that links to any pages with necessary background information.\\n- End with a recap/next steps section summarizing what the tutorial covered and future reading, such as related how-to guides.\\n  \\n### How-to guides'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='A how-to guide, as the name implies, demonstrates how to do something discrete and specific.\\nIt should assume that the user is already familiar with underlying concepts, and is focused on solving an immediate problem. However,\\nit should still provide some background or list certain scenarios where the information may be relevant.\\nThey can and should discuss alternatives if one approach may be better than another in certain cases.\\n\\nTo quote the Diataxis website:\\n\\n> A how-to guide serves the work of the already-competent user, whom you can assume to know what they want to do, and to be able to follow your instructions correctly.\\n\\nSome examples include:\\n\\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\\n- [How to: write a custom chat model](/docs/how_to/custom_chat_model/)\\n\\nHere are some high-level tips on writing a good how-to guide:'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='- Clearly explain what you are guiding the user through at the start.\\n- Assume higher intent than a tutorial and show what the user needs to do to get that task done.\\n- Assume familiarity of concepts, but explain why suggested actions are helpful.\\n  - Crosslink heavily to conceptual/reference pages.\\n- Discuss alternatives and responses to real-world tradeoffs that may arise when solving a problem.\\n- Use lots of example code.\\n  - Prefer full code blocks that the reader can copy and run.\\n- End with a recap/next steps section summarizing what the tutorial covered and future reading, such as other related how-to guides.\\n\\n### Conceptual guide'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content=\"LangChain's conceptual guide falls under the **Explanation** quadrant of Diataxis. These guides should cover LangChain terms and concepts\\nin a more abstract way than how-to guides or tutorials, targeting curious users interested in\\ngaining a deeper understanding and insights of the framework. Try to avoid excessively large code examples as the primary goal is to\\nprovide perspective to the user rather than to finish a practical project. These guides should cover **why** things work they way they do.\\n\\nThis guide on documentation style is meant to fall under this category.\\n\\nTo quote the Diataxis website:\\n\\n> The perspective of explanation is higher and wider than that of the other types. It does not take the user’s eye-level view, as in a how-to guide, or a close-up view of the machinery, like reference material. Its scope in each case is a topic - “an area of knowledge”, that somehow has to be bounded in a reasonable, meaningful way.\\n\\nSome examples include:\"), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='- [Retrieval conceptual docs](/docs/concepts/retrieval)\\n- [Chat model conceptual docs](/docs/concepts/chat_models)\\n\\nHere are some high-level tips on writing a good conceptual guide:\\n\\n- Explain design decisions. Why does concept X exist and why was it designed this way?\\n- Use analogies and reference other concepts and alternatives\\n- Avoid blending in too much reference content\\n- You can and should reference content covered in other guides, but make sure to link to them\\n\\n### References\\n\\nReferences contain detailed, low-level information that describes exactly what functionality exists and how to use it.\\nIn LangChain, this is mainly our API reference pages, which are populated from docstrings within code.\\nReferences pages are generally not read end-to-end, but are consulted as necessary when a user needs to know\\nhow to use something specific.\\n\\nTo quote the Diataxis website:'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content=\"> The only purpose of a reference guide is to describe, as succinctly as possible, and in an orderly way. Whereas the content of tutorials and how-to guides are led by needs of the user, reference material is led by the product it describes.\\n\\nMany of the reference pages in LangChain are automatically generated from code,\\nbut here are some high-level tips on writing a good docstring:\\n\\n- Be concise\\n- Discuss special cases and deviations from a user's expectations\\n- Go into detail on required inputs and outputs\\n- Light details on when one might use the feature are fine, but in-depth details belong in other sections.\\n\\nEach category serves a distinct purpose and requires a specific approach to writing and structuring the content.\\n\\n## General guidelines\\n\\nHere are some other guidelines you should think about when writing and organizing documentation.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='We generally do not merge new tutorials from outside contributors without an actue need.\\nWe welcome updates as well as new integration docs, how-tos, and references.\\n\\n### Avoid duplication\\n\\nMultiple pages that cover the same material in depth are difficult to maintain and cause confusion. There should\\nbe only one (very rarely two), canonical pages for a given concept or feature. Instead, you should link to other guides.\\n\\n### Link to other sections\\n\\nBecause sections of the docs do not exist in a vacuum, it is important to link to other sections frequently,\\nto allow a developer to learn more about an unfamiliar topic within the flow of reading.\\n\\nThis includes linking to the API references and conceptual sections!\\n\\n### Be concise\\n\\nIn general, take a less-is-more approach. If another section with a good explanation of a concept exists, you should link to it rather than\\nre-explain it, unless the concept you are documenting presents some new wrinkle.\\n\\nBe concise, including in code samples.'), Document(metadata={'source': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_path': 'docs/docs/contributing/how_to/documentation/style_guide.mdx', 'file_name': 'style_guide.mdx', 'file_type': '.mdx'}, page_content='### General style\\n\\n- Use active voice and present tense whenever possible\\n- Use examples and code snippets to illustrate concepts and usage\\n- Use appropriate header levels (`#`, `##`, `###`, etc.) to organize the content hierarchically\\n- Use fewer cells with more code to make copy/paste easier\\n- Use bullet points and numbered lists to break down information into easily digestible chunks\\n- Use tables (especially for **Reference** sections) and diagrams often to present information visually\\n- Include the table of contents for longer documentation pages to help readers navigate the content, but hide it for shorter pages'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_name': 'community.mdx', 'file_type': '.mdx'}, page_content='---\\npagination_next: null\\npagination_prev: null\\n---\\n## How to add a community integration (not recommended)\\n\\n:::danger\\n\\nWe recommend following the [main integration guide](./index.mdx) to add new integrations instead.\\n\\nIf you follow this guide, there is a high likelihood we will close your PR with the above\\nguide linked without much discussion.\\n\\n:::\\n\\nThe `langchain-community` package is in `libs/community`.\\n\\nIt can be installed with `pip install langchain-community`, and exported members can be imported with code like \\n\\n```python\\nfrom langchain_community.chat_models import ChatParrotLink\\nfrom langchain_community.llms import ParrotLinkLLM\\nfrom langchain_community.vectorstores import ParrotLinkVectorStore\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_name': 'community.mdx', 'file_type': '.mdx'}, page_content='The `community` package relies on manually-installed dependent packages, so you will see errors \\nif you try to import a package that is not installed. In our fake example, if you tried to import `ParrotLinkLLM` without installing `parrot-link-sdk`, you will see an `ImportError` telling you to install it when trying to use it.\\n\\nLet\\'s say we wanted to implement a chat model for Parrot Link AI. We would create a new file in `libs/community/langchain_community/chat_models/parrot_link.py` with the following code:\\n\\n```python\\nfrom langchain_core.language_models.chat_models import BaseChatModel\\n\\nclass ChatParrotLink(BaseChatModel):\\n    \"\"\"ChatParrotLink chat model.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain_community.chat_models import ChatParrotLink\\n\\n            model = ChatParrotLink()\\n    \"\"\"\\n\\n    ...\\n```\\n\\nAnd we would write tests in:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/community.mdx', 'file_name': 'community.mdx', 'file_type': '.mdx'}, page_content='- Unit tests: `libs/community/tests/unit_tests/chat_models/test_parrot_link.py`\\n- Integration tests: `libs/community/tests/integration_tests/chat_models/test_parrot_link.py`\\n\\nAnd add documentation to:\\n\\n- `docs/docs/integrations/chat/parrot_link.ipynb`'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_name': 'from_template.mdx', 'file_type': '.mdx'}, page_content='---\\npagination_next: null\\npagination_prev: null\\n---\\n\\n# How to publish an integration package from a template\\n\\n:::danger\\nThis guide is a work-in-progress.\\n:::\\n\\nFirst, duplicate this template repository: https://github.com/langchain-ai/integration-repo-template\\n\\nIn this guide, we will create a `libs/langchain-parrot-link` folder, simulating the creation\\nof a partner package for a fake company, \"Parrot Link AI\".\\n\\nA package is \\ninstalled by users with `pip install langchain-{partner}`, and the package members \\ncan be imported with code like:\\n\\n```python\\nfrom langchain_{partner} import X\\n```\\n\\n## Set up a new package\\n\\nTo set up a new partner package, use the latest version of the LangChain CLI. You can install or update it with:\\n\\n```bash\\npip install -U langchain-cli\\n```\\n\\nLet\\'s say you want to create a new partner package working for a company called Parrot Link AI.\\n\\nThen, run the following command to create a new partner package:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_name': 'from_template.mdx', 'file_type': '.mdx'}, page_content=\"```bash\\nmkdir libs\\ncd libs/\\nlangchain-cli integration new\\n> Name: parrot-link\\n> Name of integration in PascalCase [ParrotLink]: ParrotLink\\n```\\n\\nThis will create a new package in `libs/parrot-link` with the following structure:\\n\\n```\\nlibs/parrot-link/\\n  langchain_parrot_link/ # folder containing your package\\n    ...\\n  tests/\\n    ...\\n  docs/ # bootstrapped docs notebooks, must be moved to /docs in monorepo root\\n    ...\\n  scripts/ # scripts for CI\\n    ...\\n  LICENSE\\n  README.md # fill out with information about your package\\n  Makefile # default commands for CI\\n  pyproject.toml # package metadata, mostly managed by Poetry\\n  poetry.lock # package lockfile, managed by Poetry\\n  .gitignore\\n```\\n\\n## Implement your package\\n\\nFirst, add any dependencies your package needs, such as your company's SDK:\\n\\n```bash\\npoetry add parrot-link-sdk\\n```\\n\\nIf you need separate dependencies for type checking, you can add them to the `typing` group with:\\n\\n```bash\\npoetry add --group typing types-parrot-link-sdk\\n```\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_name': 'from_template.mdx', 'file_type': '.mdx'}, page_content=\"Then, implement your package in `libs/partners/parrot-link/langchain_parrot_link`.\\n\\nBy default, this will include stubs for a Chat Model, an LLM, and/or a Vector Store. You should delete any of the files you won't use and remove them from `__init__.py`.\\n\\n## Write Unit and Integration Tests\\n\\nSome basic tests are presented in the `tests/` directory. You should add more tests to cover your package's functionality.\\n\\nFor information on running and implementing tests, see the [Testing guide](../testing.mdx).\\n\\n## Write documentation\\n\\nDocumentation is generated from Jupyter notebooks in the `docs/` directory. You should place the notebooks with examples\\nto the relevant `docs/docs/integrations` directory in the monorepo root.\\n\\n## (If Necessary) Deprecate community integration\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_name': 'from_template.mdx', 'file_type': '.mdx'}, page_content='Note: this is only necessary if you\\'re migrating an existing community integration into \\na partner package. If the component you\\'re integrating is net-new to LangChain (i.e. \\nnot already in the `community` package), you can skip this step.\\n\\nLet\\'s pretend we migrated our `ChatParrotLink` chat model from the community package to \\nthe partner package. We would need to deprecate the old model in the community package.\\n\\nWe would do that by adding a `@deprecated` decorator to the old model as follows, in\\n`libs/community/langchain_community/chat_models/parrot_link.py`.\\n\\nBefore our change, our chat model might look like this:\\n\\n```python\\nclass ChatParrotLink(BaseChatModel):\\n  ...\\n```\\n\\nAfter our change, it would look like this:\\n\\n```python\\nfrom langchain_core._api.deprecation import deprecated\\n\\n@deprecated(\\n    since=\"0.0.<next community version>\", \\n    removal=\"1.0.0\", \\n    alternative_import=\"langchain_parrot_link.ChatParrotLink\"\\n)\\nclass ChatParrotLink(BaseChatModel):\\n  ...\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/from_template.mdx', 'file_name': 'from_template.mdx', 'file_type': '.mdx'}, page_content=\"You should do this for *each* component that you're migrating to the partner package.\\n\\n## Additional steps\\n\\nContributor steps:\\n\\n- [ ] Add secret names to manual integrations workflow in `.github/workflows/_integration_test.yml`\\n- [ ] Add secrets to release workflow (for pre-release testing) in `.github/workflows/_release.yml`\\n- [ ] set up pypi and test pypi projects\\n- [ ] add credential secrets to Github Actions\\n- [ ] add package to conda-forge\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\npagination_prev: null\\npagination_next: contributing/how_to/integrations/package\\n---\\n\\n# Contribute Integrations\\n\\nIntegrations are a core component of LangChain.\\nLangChain provides standard interfaces for several different components (language models, vector stores, etc) that are crucial when building LLM applications.\\n\\n\\n## Why contribute an integration to LangChain?\\n\\n- **Discoverability:** LangChain is the most used framework for building LLM applications, with over 20 million monthly downloads. LangChain integrations are discoverable by a large community of GenAI builders.\\n- **Interoperability:** LangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.\\n- **Best Practices:** Through their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc)'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='## Components to Integrate\\n\\n:::info\\n\\nSee the [Conceptual Guide](../../../concepts/index.mdx) for an overview of all components\\nsupported in LangChain\\n\\n:::\\n\\nWhile any component can be integrated into LangChain, there are specific types of integrations we encourage more:\\n\\n<table>\\n  <tr>\\n    <th>Integrate these ✅</th>\\n    <th>Not these ❌</th>\\n  </tr>\\n  <tr>\\n    <td>\\n      <ul>\\n        <li>Chat Models</li>\\n        <li>Tools/Toolkits</li>\\n        <li>Retrievers</li>\\n        <li>Vector Stores</li>\\n        <li>Embedding Models</li>\\n      </ul>\\n    </td>\\n    <td>\\n      <ul>\\n        <li>LLMs (Text-Completion Models)</li>\\n        <li>Document Loaders</li>\\n        <li>Key-Value Stores</li>\\n        <li>Document Transformers</li>\\n        <li>Model Caches</li>\\n        <li>Graphs</li>\\n        <li>Message Histories</li>\\n        <li>Callbacks</li>\\n        <li>Chat Loaders</li>\\n        <li>Adapters</li>\\n        <li>Other abstractions</li>\\n      </ul>\\n    </td>\\n  </tr>\\n</table>'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='## How to contribute an integration\\n\\nIn order to contribute an integration, you should follow these steps:\\n\\n1. Confirm that your integration is in the [list of components](#components-to-integrate) we are currently encouraging.\\n2. [Implement your package](/docs/contributing/how_to/integrations/package/) and publish it to a public github repository.\\n3. [Implement the standard tests](./standard_tests) for your integration and successfully run them.\\n4. [Publish your integration](./publish.mdx) by publishing the package to PyPi and add docs in the `docs/docs/integrations` directory of the LangChain monorepo.\\n5. [Optional] Open and merge a PR to add documentation for your integration to the official LangChain docs.\\n6. [Optional] Engage with the LangChain team for joint co-marketing ([see below](#co-marketing)).\\n\\n## Co-Marketing'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='With over 20 million monthly downloads, LangChain has a large audience of developers building LLM applications.\\nBesides just adding integrations, we also like to show them examples of cool tools or APIs they can use.\\n\\nWhile traditionally called \"co-marketing\", we like to think of this more as \"co-education\".\\nFor that reason, while we are happy to highlight your integration through our social media channels, we prefer to highlight examples that also serve some educational purpose.\\nOur main social media channels are Twitter and LinkedIn.\\n\\nHere are some heuristics for types of content we are excited to promote:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='- **Integration announcement:** If you announce the integration with a link to the LangChain documentation page, we are happy to re-tweet/re-share on Twitter/LinkedIn.\\n- **Educational content:** We highlight good educational content on the weekends - if you write a good blog or make a good YouTube video, we are happy to share there! Note that we prefer content that is NOT framed as \"here\\'s how to use integration XYZ\", but rather \"here\\'s how to do ABC\", as we find that is more educational and helpful for developers.\\n- **End-to-end applications:** End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use [LangGraph](https://github.com/langchain-ai/langgraph) as the orchestration framework. We get particularly excited about anything involving long-term memory, human-in-the-loop interaction patterns, or multi-agent architectures.\\n- **Research:** We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"## Further Reading\\nTo get started, let's learn [how to implement an integration package](/docs/contributing/how_to/integrations/package/) for LangChain.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"---\\npagination_next: contributing/how_to/integrations/standard_tests\\npagination_prev: contributing/how_to/integrations/index\\n---\\n# How to implement an integration package\\n\\nThis guide walks through the process of implementing a LangChain integration \\npackage.\\n\\nIntegration packages are just Python packages that can be installed with `pip install <your-package>`,\\nwhich contain classes that are compatible with LangChain's core interfaces.\\n\\nWe will cover:\\n\\n1. (Optional) How to bootstrap a new integration package\\n2. How to implement components, such as [chat models](/docs/concepts/chat_models/) and [vector stores](/docs/concepts/vectorstores/), that adhere\\nto the LangChain interface;  \\n\\n## (Optional) bootstrapping a new integration package\\n\\nIn this section, we will outline 2 options for bootstrapping a new integration package, \\nand you're welcome to use other tools if you prefer!\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='1. **langchain-cli**: This is a command-line tool that can be used to bootstrap a new integration package with a template for LangChain components and Poetry for dependency management.\\n2. **Poetry**: This is a Python dependency management tool that can be used to bootstrap a new Python package with dependencies. You can then add LangChain components to this package.\\n\\n<details>\\n    <summary>Option 1: langchain-cli (recommended)</summary>\\n\\nIn this guide, we will be using the `langchain-cli` to create a new integration package\\nfrom a template, which can be edited to implement your LangChain components.\\n\\n### **Prerequisites**\\n\\n- [GitHub](https://github.com) account\\n- [PyPi](https://pypi.org/) account\\n\\n### Boostrapping a new Python package with langchain-cli\\n\\nFirst, install `langchain-cli` and `poetry`:\\n\\n```bash\\npip install langchain-cli poetry\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"Next, come up with a name for your package. For this guide, we'll use `langchain-parrot-link`.\\nYou can confirm that the name is available on PyPi by searching for it on the [PyPi website](https://pypi.org/).\\n\\nNext, create your new Python package with `langchain-cli`, and navigate into the new directory with `cd`:\\n\\n```bash\\nlangchain-cli integration new\\n\\n> The name of the integration to create (e.g. `my-integration`): parrot-link\\n> Name of integration in PascalCase [ParrotLink]:\\n\\ncd parrot-link\\n```\\n\\nNext, let's add any dependencies we need\\n\\n```bash\\npoetry add my-integration-sdk\\n```\\n\\nWe can also add some `typing` or `test` dependencies in a separate poetry dependency group.\\n\\n```\\npoetry add --group typing my-typing-dep\\npoetry add --group test my-test-dep\\n```\\n\\nAnd finally, have poetry set up a virtual environment with your dependencies, as well\\nas your integration package:\\n\\n```bash\\npoetry install --with lint,typing,test,test_integration\\n```\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"You now have a new Python package with a template for LangChain components! This\\ntemplate comes with files for each integration type, and you're welcome to duplicate or\\ndelete any of these files as needed (including the associated test files).\\n\\nTo create any individual files from the [template], you can run e.g.:\\n\\n```bash\\nlangchain-cli integration new \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink \\\\\\n    --src integration_template/chat_models.py \\\\\\n    --dst langchain_parrot_link/chat_models_2.py\\n```\\n\\n</details>\\n\\n<details>\\n    <summary>Option 2: Poetry (manual)</summary>\\n\\nIn this guide, we will be using [Poetry](https://python-poetry.org/) for\\ndependency management and packaging, and you're welcome to use any other tools you prefer.\\n\\n### **Prerequisites**\\n\\n- [GitHub](https://github.com) account\\n- [PyPi](https://pypi.org/) account\\n\\n### Boostrapping a new Python package with Poetry\\n\\nFirst, install Poetry:\\n\\n```bash\\npip install poetry\\n```\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"Next, come up with a name for your package. For this guide, we'll use `langchain-parrot-link`.\\nYou can confirm that the name is available on PyPi by searching for it on the [PyPi website](https://pypi.org/).\\n\\nNext, create your new Python package with Poetry, and navigate into the new directory with `cd`:\\n\\n```bash\\npoetry new langchain-parrot-link\\ncd langchain-parrot-link\\n```\\n\\nAdd main dependencies using Poetry, which will add them to your `pyproject.toml` file:\\n\\n```bash\\npoetry add langchain-core\\n```\\n\\nWe will also add some `test` dependencies in a separate poetry dependency group. If\\nyou are not using Poetry, we recommend adding these in a way that won't package them\\nwith your published package, or just installing them separately when you run tests.\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='`langchain-tests` will provide the [standard tests](../standard_tests) we will use later. \\nWe recommended pinning these to the latest version: <img src=\"https://img.shields.io/pypi/v/langchain-tests\" style={{position:\"relative\",top:4,left:3}} />\\n\\nNote: Replace `<latest_version>` with the latest version of `langchain-tests` below.\\n\\n```bash\\npoetry add --group test pytest pytest-socket pytest-asyncio langchain-tests==<latest_version>\\n```\\n\\nAnd finally, have poetry set up a virtual environment with your dependencies, as well\\nas your integration package:\\n\\n```bash\\npoetry install --with test\\n```\\n\\nYou\\'re now ready to start writing your integration package!\\n\\n### Writing your integration\\n\\nLet\\'s say you\\'re building a simple integration package that provides a `ChatParrotLink`\\nchat model integration for LangChain. Here\\'s a simple example of what your project\\nstructure might look like:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"```plaintext\\nlangchain-parrot-link/\\n├── langchain_parrot_link/\\n│   ├── __init__.py\\n│   └── chat_models.py\\n├── tests/\\n│   ├── __init__.py\\n│   └── test_chat_models.py\\n├── pyproject.toml\\n└── README.md\\n```\\n\\nAll of these files should already exist from step 1, except for \\n`chat_models.py` and `test_chat_models.py`! We will implement `test_chat_models.py` \\nlater, following the [standard tests](../standard_tests) guide.\\n\\nFor `chat_models.py`, simply paste the contents of the chat model implementation\\n[above](#implementing-langchain-components).\\n\\n</details>\\n\\n### Push your package to a public Github repository\\n\\nThis is only required if you want to publish your integration in the LangChain documentation.\\n\\n1. Create a new repository on GitHub.\\n2. Push your code to the repository.\\n3. Confirm that your repository is viewable by the public (e.g. in a private browsing window, where you're not logged into Github).\\n\\n## Implementing LangChain components\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='LangChain components are subclasses of base classes in [langchain-core](/docs/concepts/architecture/#langchain-core).\\nExamples include [chat models](/docs/concepts/chat_models/),\\n[vector stores](/docs/concepts/vectorstores/), [tools](/docs/concepts/tools/),\\n[embedding models](/docs/concepts/embedding_models/) and [retrievers](/docs/concepts/retrievers/).\\n\\nYour integration package will typically implement a subclass of at least one of these\\ncomponents. Expand the tabs below to see details on each.\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\nimport CodeBlock from \\'@theme/CodeBlock\\';\\n\\n<Tabs>\\n\\n    <TabItem value=\"chat_models\" label=\"Chat models\">\\n        \\n        Refer to the [Custom Chat Model Guide](/docs/how_to/custom_chat_model) guide for\\n        detail on a starter chat model [implementation](/docs/how_to/custom_chat_model/#implementation).\\n\\n        You can start from the following template or langchain-cli command:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='```bash\\n        langchain-cli integration new \\\\\\n            --name parrot-link \\\\\\n            --name-class ParrotLink \\\\\\n            --src integration_template/chat_models.py \\\\\\n            --dst langchain_parrot_link/chat_models.py\\n        ```\\n\\n        <details>\\n            <summary>Example chat model code</summary>\\n\\nimport ChatModelSource from \\'../../../../src/theme/integration_template/integration_template/chat_models.py\\';\\n\\n        <CodeBlock language=\"python\" title=\"langchain_parrot_link/chat_models.py\">\\n            {\\n                ChatModelSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n                    .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n                    .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n                    .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n            }\\n        </CodeBlock>\\n\\n        </details>\\n\\n    </TabItem>\\n    <TabItem value=\"vector_stores\" label=\"Vector stores\">'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='Your vector store implementation will depend on your chosen database technology.\\n        `langchain-core` includes a minimal\\n        [in-memory vector store](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html)\\n        that we can use as a guide. You can access the code [here](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/vectorstores/in_memory.py).\\n\\n        All vector stores must inherit from the [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html)\\n        base class. This interface consists of methods for writing, deleting and searching\\n        for documents in the vector store.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='`VectorStore` supports a variety of synchronous and asynchronous search types (e.g., \\n        nearest-neighbor or maximum marginal relevance), as well as interfaces for adding\\n        documents to the store. See the [API Reference](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html)\\n        for all supported methods. The required methods are tabulated below:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='| Method/Property         | Description                                          |\\n        |------------------------ |------------------------------------------------------|\\n        | `add_documents`         | Add documents to the vector store.                   |\\n        | `delete`                | Delete selected documents from vector store (by IDs) |\\n        | `get_by_ids`            | Get selected documents from vector store (by IDs)    |\\n        | `similarity_search`     | Get documents most similar to a query.               |\\n        | `embeddings` (property) | Embeddings object for vector store.                  |\\n        | `from_texts`            | Instantiate vector store via adding texts.           |\\n\\n        Note that `InMemoryVectorStore` implements some optional search types, as well as\\n        convenience methods for loading and dumping the object to a file, but this is not\\n        necessary for all implementations.\\n\\n        :::tip'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='The [in-memory vector store](https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/vectorstores/in_memory.py)\\n        is tested against the standard tests in the LangChain Github repository.\\n\\n        :::\\n\\n        <details>\\n            <summary>Example vector store code</summary>\\n\\nimport VectorstoreSource from \\'../../../../src/theme/integration_template/integration_template/vectorstores.py\\';\\n\\n        <CodeBlock language=\"python\" title=\"langchain_parrot_link/vectorstores.py\">\\n            {\\n                VectorstoreSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n                    .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n                    .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n                    .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n            }\\n        </CodeBlock>\\n\\n        </details>\\n\\n    </TabItem>\\n    <TabItem value=\"embeddings\" label=\"Embeddings\">'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"Embeddings are used to convert `str` objects from `Document.page_content` fields\\ninto a vector representation (represented as a list of floats).\\n\\nRefer to the [Custom Embeddings Guide](/docs/how_to/custom_embeddings) guide for\\ndetail on a starter embeddings [implementation](/docs/how_to/custom_embeddings/#implementation).\\n\\nYou can start from the following template or langchain-cli command:\\n\\n```bash\\nlangchain-cli integration new \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink \\\\\\n    --src integration_template/embeddings.py \\\\\\n    --dst langchain_parrot_link/embeddings.py\\n```\\n\\n        <details>\\n            <summary>Example embeddings code</summary>\\n\\nimport EmbeddingsSource from '/src/theme/integration_template/integration_template/embeddings.py';\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='<CodeBlock language=\"python\" title=\"langchain_parrot_link/embeddings.py\">\\n            {\\n                EmbeddingsSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n                    .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n                    .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n                    .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n            }\\n        </CodeBlock>\\n\\n        </details>\\n\\n    </TabItem>\\n    <TabItem value=\"tools\" label=\"Tools\">\\n\\nTools are used in 2 main ways:\\n\\n1. To define an \"input schema\" or \"args schema\" to pass to a chat model\\'s tool calling\\nfeature along with a text request, such that the chat model can generate a \"tool call\",\\nor parameters to call the tool with.\\n2. To take a \"tool call\" as generated above, and take some action and return a response\\nthat can be passed back to the chat model as a ToolMessage.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content=\"The `Tools` class must inherit from the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a \\nsubclass.\\n\\n| Method/Property         | Description                                          |\\n|------------------------ |------------------------------------------------------|\\n| `name`                  | Name of the tool (passed to the LLM too).            |\\n| `description`           | Description of the tool (passed to the LLM too).     |\\n| `args_schema`           | Define the schema for the tool's input arguments.    |\\n| `_run`                  | Run the tool with the given arguments.               |\\n| `_arun`                 | Asynchronously run the tool with the given arguments.|\\n\\n### Properties\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='`name`, `description`, and `args_schema` are all properties that should be implemented\\nin the subclass. `name` and `description` are strings that are used to identify the tool\\nand provide a description of what the tool does. Both of these are passed to the LLM,\\nand users may override these values depending on the LLM they are using as a form of\\n\"prompt engineering.\" Giving these a concise and LLM-usable name and description is\\nimportant for the initial user experience of the tool.\\n\\n`args_schema` is a Pydantic `BaseModel` that defines the schema for the tool\\'s input\\narguments. This is used to validate the input arguments to the tool, and to provide\\na schema for the LLM to fill out when calling the tool. Similar to the `name` and\\n`description` of the overall Tool class, the fields\\' names (the variable name) and\\ndescription (part of `Field(..., description=\"description\")`) are passed to the LLM, \\nand the values in these fields should be concise and LLM-usable.\\n\\n### Run Methods'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='`_run` is the main method that should be implemented in the subclass. This method\\ntakes in the arguments from `args_schema` and runs the tool, returning a string\\nresponse. This method is usually called in a LangGraph [`ToolNode`](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/), and can also be called in a legacy\\n`langchain.agents.AgentExecutor`.\\n\\n`_arun` is optional because by default, `_run` will be run in an async executor.\\nHowever, if your tool is calling any apis or doing any async work, you should implement\\nthis method to run the tool asynchronously in addition to `_run`.\\n\\n### Implementation\\n\\nYou can start from the following template or langchain-cli command:\\n\\n```bash\\nlangchain-cli integration new \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink \\\\\\n    --src integration_template/tools.py \\\\\\n    --dst langchain_parrot_link/tools.py\\n```\\n\\n        <details>\\n            <summary>Example tool code</summary>'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='import ToolSource from \\'/src/theme/integration_template/integration_template/tools.py\\';\\n\\n        <CodeBlock language=\"python\" title=\"langchain_parrot_link/tools.py\">\\n            {\\n                ToolSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n                    .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n                    .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n                    .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n            }\\n        </CodeBlock>\\n\\n        </details>\\n\\n    </TabItem>\\n    <TabItem value=\"retrievers\" label=\"Retrievers\">\\n\\nRetrievers are used to retrieve documents from APIs, databases, or other sources\\nbased on a query. The `Retriever` class must inherit from the [BaseRetriever](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html) base class. This interface has 1 attribute and 2 methods that should be implemented in a subclass.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='| Method/Property         | Description                                          |\\n|------------------------ |------------------------------------------------------|\\n| `k`                     | Default number of documents to retrieve (configurable). |\\n| `_get_relevant_documents`| Retrieve documents based on a query.                 |\\n| `_aget_relevant_documents`| Asynchronously retrieve documents based on a query.  |\\n\\n### Attributes\\n\\n`k` is an attribute that should be implemented in the subclass. This attribute\\ncan simply be defined at the top of the class with a default value like\\n`k: int = 5`. This attribute is the default number of documents to retrieve\\nfrom the retriever, and can be overridden by the user when constructing or calling\\nthe retriever.\\n\\n### Methods\\n\\n`_get_relevant_documents` is the main method that should be implemented in the subclass.\\n\\nThis method takes in a query and returns a list of `Document` objects, which have 2\\nmain properties:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='- `page_content` - the text content of the document\\n- `metadata` - a dictionary of metadata about the document\\n\\nRetrievers are typically directly invoked by a user, e.g. as\\n`MyRetriever(k=4).invoke(\"query\")`, which will automatically call `_get_relevant_documents`\\nunder the hood.\\n\\n`_aget_relevant_documents` is optional because by default, `_get_relevant_documents` will\\nbe run in an async executor. However, if your retriever is calling any apis or doing\\nany async work, you should implement this method to run the retriever asynchronously\\nin addition to `_get_relevant_documents` for performance reasons.\\n\\n### Implementation\\n\\nYou can start from the following template or langchain-cli command:\\n\\n```bash\\nlangchain-cli integration new \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink \\\\\\n    --src integration_template/retrievers.py \\\\\\n    --dst langchain_parrot_link/retrievers.py\\n```\\n\\n        <details>\\n            <summary>Example retriever code</summary>'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/package.mdx', 'file_name': 'package.mdx', 'file_type': '.mdx'}, page_content='import RetrieverSource from \\'/src/theme/integration_template/integration_template/retrievers.py\\';\\n\\n        <CodeBlock language=\"python\" title=\"langchain_parrot_link/retrievers.py\">\\n            {\\n                RetrieverSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n                    .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n                    .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n                    .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n            }\\n        </CodeBlock>\\n\\n        </details>\\n\\n    </TabItem>\\n</Tabs>\\n\\n---\\n\\n## Next Steps\\n\\nNow that you\\'ve implemented your package, you can move on to [testing your integration](../standard_tests) for your integration and successfully run them.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content=\"---\\npagination_prev: contributing/how_to/integrations/standard_tests\\npagination_next: null\\n---\\n\\n# Publishing your package\\n\\nNow that your package is implemented and tested, you can:\\n\\n1. Publish your package to PyPi\\n2. Add documentation for your package to the LangChain Monorepo\\n\\n## Publishing your package to PyPi\\n\\nThis guide assumes you have already implemented your package and written tests for it. If you haven't done that yet, please refer to the [implementation guide](../package) and the [testing guide](../standard_tests).\\n\\nNote that Poetry is not required to publish a package to PyPi, and we're using it in this guide end-to-end for convenience.\\nYou are welcome to publish your package using any other method you prefer.\\n\\nFirst, make sure you have a PyPi account and have logged in with Poetry:\\n\\n<details>\\n    <summary>How to create a PyPi Token</summary>\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content='1. Go to the [PyPi website](https://pypi.org/) and create an account.\\n2. Verify your email address by clicking the link that PyPi emails to you.\\n3. Go to your account settings and click \"Generate Recovery Codes\" to enable 2FA. To generate an API token, you **must** have 2FA enabled currently.\\n4. Go to your account settings and [generate a new API token](https://pypi.org/manage/account/token/).\\n\\n</details>\\n\\n```bash\\npoetry config pypi-token.pypi <your-pypi-token>\\n```\\n\\nNext, build your package:\\n\\n```bash\\npoetry build\\n```\\n\\nFinally, publish your package to PyPi:\\n\\n```bash\\npoetry publish\\n```\\n\\nYou\\'re all set! Your package is now available on PyPi and can be installed with `pip install langchain-parrot-link`.\\n\\n## Adding documentation to the LangChain Monorepo\\n\\nTo add documentation for your package to the LangChain Monorepo, you will need to:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content='1. Fork and clone the LangChain Monorepo\\n2. Make a \"Provider Page\" at `docs/docs/integrations/providers/<your-package-name>.ipynb`\\n3. Make \"Component Pages\" at `docs/docs/integrations/<component-type>/<your-package-name>.ipynb`\\n4. Register your package in `libs/packages.yml`\\n5. Submit a PR with **only these changes** to the LangChain Monorepo\\n\\n### Fork and clone the LangChain Monorepo\\n\\nFirst, fork the [LangChain Monorepo](https://github.com/langchain-ai/langchain) to your GitHub account.\\n\\nNext, clone the repository to your local machine:\\n\\n```bash\\ngit clone https://github.com/<your-username>/langchain.git\\n```\\n\\nYou\\'re now ready to make your PR!\\n\\n### Bootstrap your documentation pages with the langchain-cli (recommended)\\n\\nTo make it easier to create the necessary documentation pages, you can use the `langchain-cli` to bootstrap them for you.\\n\\nFirst, install the latest version of the `langchain-cli` package:\\n\\n```bash\\npip install --upgrade langchain-cli\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content=\"To see the available commands to bootstrap your documentation pages, run:\\n\\n```bash\\nlangchain-cli integration create-doc --help\\n```\\n\\nLet's bootstrap a provider page from the root of the monorepo:\\n\\n```bash\\nlangchain-cli integration create-doc \\\\\\n    --component-type Provider \\\\\\n    --destination-dir docs/docs/integrations/providers \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink\\n```\\n\\nAnd a chat model component page:\\n\\n```bash\\nlangchain-cli integration create-doc \\\\\\n    --component-type ChatModel \\\\\\n    --destination-dir docs/docs/integrations/chat \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink\\n```\\n\\nAnd a vector store component page:\\n\\n```bash\\nlangchain-cli integration create-doc \\\\\\n    --component-type VectorStore \\\\\\n    --destination-dir docs/docs/integrations/vectorstores \\\\\\n    --name parrot-link \\\\\\n    --name-class ParrotLink\\n```\\n\\nThese commands will create the following 3 files, which you should fill out with information about your package:\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content='- `docs/docs/integrations/providers/parrot_link.ipynb`\\n- `docs/docs/integrations/chat/parrot_link.ipynb`\\n- `docs/docs/integrations/vectorstores/parrot_link.ipynb`\\n\\n### Manually create your documentation pages (if you prefer)\\n\\nIf you prefer to create the documentation pages manually, you can create the same files listed\\nabove and fill them out with information about your package.\\n\\nYou can view the templates that the CLI uses to create these files [here](https://github.com/langchain-ai/langchain/tree/master/libs/cli/langchain_cli/integration_template/docs) if helpful!\\n\\n### Register your package in `libs/packages.yml`\\n\\nFinally, add your package to the end of the `libs/packages.yml` file in the LangChain Monorepo.\\n\\n```yaml\\npackages:\\n  - name: langchain-parrot-link\\n    repo: <your github handle>/<your repo>\\n    path: .\\n```\\n\\nFor `path`, you can use `.` if your package is in the root of your repository, or specify a subdirectory (e.g. `libs/parrot-link`) if it is in a subdirectory.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/publish.mdx', 'file_name': 'publish.mdx', 'file_type': '.mdx'}, page_content='If you followed the [package bootstrapping guide](../package), then your path is `.`.\\n\\n### Submit a PR with your changes\\n\\nOnce you have completed these steps, you can submit a PR to the LangChain Monorepo with **only these changes**.\\n\\nIf you have additional changes to request, please submit them in a separate PR.'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='---\\npagination_next: contributing/how_to/integrations/publish\\npagination_prev: contributing/how_to/integrations/package\\n---\\n# How to add standard tests to an integration\\n\\nWhen creating either a custom class for yourself or to publish in a LangChain integration, it is important to add standard tests to ensure it works as expected. This guide will show you how to add standard tests to each integration type.\\n\\n## Setup\\n\\nFirst, let\\'s install 2 dependencies:\\n\\n- `langchain-core` will define the interfaces we want to import to define our custom tool.\\n- `langchain-tests` will provide the standard tests we want to use, as well as pytest plugins necessary to run them. Recommended to pin to the latest version: <img src=\"https://img.shields.io/pypi/v/langchain-tests\" style={{position:\"relative\",top:4,left:3}} />\\n\\n:::note\\n\\nBecause added tests in new versions of `langchain-tests` can break your CI/CD pipelines, we recommend pinning the \\nversion of `langchain-tests` to avoid unexpected changes.\\n\\n:::'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='import Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\n\\n<Tabs>\\n    <TabItem value=\"poetry\" label=\"Poetry\" default>\\nIf you followed the [previous guide](../package), you should already have these dependencies installed!\\n\\n```bash\\npoetry add langchain-core\\npoetry add --group test langchain-tests==<latest_version>\\npoetry install --with test\\n```\\n    </TabItem>\\n    <TabItem value=\"pip\" label=\"Pip\">\\n```bash\\npip install -U langchain-core langchain-tests\\n\\n# install current package in editable mode\\npip install --editable .\\n```\\n    </TabItem>\\n</Tabs>\\n\\n## Add and configure standard tests\\n\\nThere are 2 namespaces in the `langchain-tests` package:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='- [unit tests](../../../concepts/testing.mdx#unit-tests) (`langchain_tests.unit_tests`): designed to be used to test the component in isolation and without access to external services\\n- [integration tests](../../../concepts/testing.mdx#integration-tests) (`langchain_tests.integration_tests`): designed to be used to test the component with access to external services (in particular, the external service that the component is designed to interact with).\\n\\nBoth types of tests are implemented as [`pytest` class-based test suites](https://docs.pytest.org/en/7.1.x/getting-started.html#group-multiple-tests-in-a-class).\\n\\nBy subclassing the base classes for each type of standard test (see below), you get all of the standard tests for that type, and you\\ncan override the properties that the test suite uses to configure the tests.\\n\\nIn order to run the tests in the same way as this guide, we recommend subclassing these\\nclasses in test files under two test subdirectories:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='- `tests/unit_tests` for unit tests\\n- `tests/integration_tests` for integration tests\\n\\n### Implementing standard tests\\n\\nimport CodeBlock from \\'@theme/CodeBlock\\';\\n\\nIn the following tabs, we show how to implement the standard tests for\\neach component type:\\n\\n<Tabs>\\n\\n    <TabItem value=\"chat_models\" label=\"Chat models\">\\n\\nTo configure standard tests for a chat model, we subclass `ChatModelUnitTests` and `ChatModelIntegrationTests`. On each subclass, we override the following `@property` methods to specify the chat model to be tested and the chat model\\'s configuration:\\n\\n| Property | Description |\\n| --- | --- |\\n| `chat_model_class` | The class for the chat model to be tested |\\n| `chat_model_params` | The parameters to pass to the chat\\nmodel\\'s constructor |\\n\\nAdditionally, chat model standard tests test a range of behaviors, from the most basic requirements (generating a response to a query) to optional capabilities like multi-modal support and tool-calling. For a test run to be successful:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='1. If a feature is intended to be supported by the model, it should pass;\\n2. If a feature is not intended to be supported by the model, it should be skipped.\\n\\nTests for \"optional\" capabilities are controlled via a set of properties that can be overridden on the test model subclass.\\n\\nYou can see the **entire list of configurable capabilities** in the API references for\\n[unit tests](https://python.langchain.com/api_reference/standard_tests/unit_tests/langchain_tests.unit_tests.chat_models.ChatModelUnitTests.html)\\nand [integration tests](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.chat_models.ChatModelIntegrationTests.html).\\n\\nFor example, to enable integration tests for image inputs, we can implement\\n\\n```python\\n@property\\ndef supports_image_inputs(self) -> bool:\\n    return True\\n```\\n\\non the integration test class.\\n\\n:::note'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='Details on what tests are run, how each test can be skipped, and troubleshooting tips for each test can be found in the API references. See details:\\n\\n- [Unit tests API reference](https://python.langchain.com/api_reference/standard_tests/unit_tests/langchain_tests.unit_tests.chat_models.ChatModelUnitTests.html)\\n- [Integration tests API reference](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.chat_models.ChatModelIntegrationTests.html)\\n\\n:::\\n\\nUnit test example:\\n\\nimport ChatUnitSource from \\'../../../../src/theme/integration_template/tests/unit_tests/test_chat_models.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/unit_tests/test_chat_models.py\">\\n{\\n    ChatUnitSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='Integration test example:\\n\\n\\nimport ChatIntegrationSource from \\'../../../../src/theme/integration_template/tests/integration_tests/test_chat_models.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/integration_tests/test_chat_models.py\">\\n{\\n    ChatIntegrationSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\n    </TabItem>\\n    <TabItem value=\"vector_stores\" label=\"Vector stores\">\\n\\n\\nHere\\'s how you would configure the standard tests for a typical vector store (using\\n`ParrotVectorStore` as a placeholder):\\n\\nVector store tests do not have optional capabilities to be configured at this time.\\n\\nimport VectorStoreIntegrationSource from \\'../../../../src/theme/integration_template/tests/integration_tests/test_vectorstores.py\\';'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='<CodeBlock language=\"python\" title=\"tests/integration_tests/test_vectorstores.py\">\\n{\\n    VectorStoreIntegrationSource.replaceAll(\\'__ModuleName__\\', \\'Parrot\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\nConfiguring the tests consists of implementing pytest fixtures for setting up an\\nempty vector store and tearing down the vector store after the test run ends.\\n\\n| Fixture | Description |\\n| --- | --- |\\n| `vectorstore` | A generator that yields an empty vector store for unit tests. The vector store is cleaned up after the test run ends. |\\n\\nFor example, below is the `VectorStoreIntegrationTests` class for the [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\\nintegration:\\n\\n```python\\nfrom typing import Generator'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='import pytest\\nfrom langchain_core.vectorstores import VectorStore\\nfrom langchain_tests.integration_tests.vectorstores import VectorStoreIntegrationTests\\n\\nfrom langchain_chroma import Chroma\\n\\n\\nclass TestChromaStandard(VectorStoreIntegrationTests):\\n    @pytest.fixture()\\n    def vectorstore(self) -> Generator[VectorStore, None, None]:  # type: ignore\\n        \"\"\"Get an empty vectorstore for unit tests.\"\"\"\\n        store = Chroma(embedding_function=self.get_embeddings())\\n        try:\\n            yield store\\n        finally:\\n            store.delete_collection()\\n            pass\\n\\n```'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='Note that before the initial `yield`, we instantiate the vector store with an\\n[embeddings](/docs/concepts/embedding_models/) object. This is a pre-defined\\n[\"fake\" embeddings model](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.vectorstores.VectorStoreIntegrationTests.html#langchain_tests.integration_tests.vectorstores.VectorStoreIntegrationTests.get_embeddings)\\nthat will generate short, arbitrary vectors for documents. You can use a different\\nembeddings object if desired.\\n\\nIn the `finally` block, we call whatever integration-specific logic is needed to\\nbring the vector store to a clean state. This logic is executed in between each test\\n(e.g., even if tests fail).\\n\\n:::note'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='Details on what tests are run and troubleshooting tips for each test can be found in the [API reference](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.vectorstores.VectorStoreIntegrationTests.html).\\n\\n:::\\n\\n\\n    </TabItem>\\n    <TabItem value=\"embeddings\" label=\"Embeddings\">\\n\\nTo configure standard tests for an embeddings model, we subclass `EmbeddingsUnitTests` and `EmbeddingsIntegrationTests`. On each subclass, we override the following `@property` methods to specify the embeddings model to be tested and the embeddings model\\'s configuration:\\n\\n| Property | Description |\\n| --- | --- |\\n| `embeddings_class` | The class for the embeddings model to be tested |\\n| `embedding_model_params` | The parameters to pass to the embeddings model\\'s constructor |\\n\\n:::note\\n\\nDetails on what tests are run, how each test can be skipped, and troubleshooting tips for each test can be found in the API references. See details:'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='- [Unit tests API reference](https://python.langchain.com/api_reference/standard_tests/unit_tests/langchain_tests.unit_tests.embeddings.EmbeddingsUnitTests.html)\\n- [Integration tests API reference](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.embeddings.EmbeddingsIntegrationTests.html)\\n\\n:::\\n\\nUnit test example:\\n\\nimport EmbeddingsUnitSource from \\'../../../../src/theme/integration_template/tests/unit_tests/test_embeddings.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/unit_tests/test_embeddings.py\">\\n{\\n    EmbeddingsUnitSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\nIntegration test example:\\n\\n\\n```python title=\"tests/integration_tests/test_embeddings.py\"\\nfrom typing import Type'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='from langchain_parrot_link.embeddings import ParrotLinkEmbeddings\\nfrom langchain_tests.integration_tests import EmbeddingsIntegrationTests\\n\\n\\nclass TestParrotLinkEmbeddingsIntegration(EmbeddingsIntegrationTests):\\n    @property\\n    def embeddings_class(self) -> Type[ParrotLinkEmbeddings]:\\n        return ParrotLinkEmbeddings\\n\\n    @property\\n    def embedding_model_params(self) -> dict:\\n        return {\"model\": \"nest-embed-001\"}\\n```\\n\\nimport EmbeddingsIntegrationSource from \\'../../../../src/theme/integration_template/tests/integration_tests/test_embeddings.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/integration_tests/test_embeddings.py\">\\n{\\n    EmbeddingsIntegrationSource.replaceAll(\\'__ModuleName__\\', \\'ParrotLink\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT_LINK\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\n    </TabItem>\\n    <TabItem value=\"tools\" label=\"Tools\">'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content=\"To configure standard tests for a tool, we subclass `ToolsUnitTests` and\\n`ToolsIntegrationTests`. On each subclass, we override the following `@property` methods\\nto specify the tool to be tested and the tool's configuration:\\n\\n| Property | Description |\\n| --- | --- |\\n| `tool_constructor` | The constructor for the tool to be tested, or an instantiated tool. |\\n| `tool_constructor_params` | The parameters to pass to the tool (optional). |\\n| `tool_invoke_params_example` | An example of the parameters to pass to the tool's `invoke` method. |\\n\\nIf you are testing a tool class and pass a class like `MyTool` to `tool_constructor`, you can pass the parameters to the constructor in `tool_constructor_params`. \\n\\nIf you are testing an instantiated tool, you can pass the instantiated tool to `tool_constructor` and do not\\noverride `tool_constructor_params`.\\n\\n:::note\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='Details on what tests are run, how each test can be skipped, and troubleshooting tips for each test can be found in the API references. See details:\\n\\n- [Unit tests API reference](https://python.langchain.com/api_reference/standard_tests/unit_tests/langchain_tests.unit_tests.tools.ToolsUnitTests.html)\\n- [Integration tests API reference](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.tools.ToolsIntegrationTests.html)\\n\\n:::\\n\\nimport ToolsUnitSource from \\'../../../../src/theme/integration_template/tests/unit_tests/test_tools.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/unit_tests/test_tools.py\">\\n{\\n    ToolsUnitSource.replaceAll(\\'__ModuleName__\\', \\'Parrot\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='import ToolsIntegrationSource from \\'../../../../src/theme/integration_template/tests/integration_tests/test_tools.py\\';\\n\\n<CodeBlock language=\"python\" title=\"tests/integration_tests/test_tools.py\">\\n{\\n    ToolsIntegrationSource.replaceAll(\\'__ModuleName__\\', \\'Parrot\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\n    </TabItem>\\n\\n    <TabItem value=\"retrievers\" label=\"Retrievers\">\\n\\nTo configure standard tests for a retriever, we subclass `RetrieversUnitTests` and\\n`RetrieversIntegrationTests`. On each subclass, we override the following `@property` methods'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content=\"| Property | Description |\\n| --- | --- |\\n| `retriever_constructor` | The class for the retriever to be tested |\\n| `retriever_constructor_params` | The parameters to pass to the retriever's constructor |\\n| `retriever_query_example` | An example of the query to pass to the retriever's `invoke` method |\\n\\n:::note\\n\\nDetails on what tests are run and troubleshooting tips for each test can be found in the [API reference](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.retrievers.RetrieversIntegrationTests.html).\\n\\n:::\\n\\nimport RetrieverIntegrationSource from '../../../../src/theme/integration_template/tests/integration_tests/test_retrievers.py';\"), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='<CodeBlock language=\"python\" title=\"tests/integration_tests/test_retrievers.py\">\\n{\\n    RetrieverIntegrationSource.replaceAll(\\'__ModuleName__\\', \\'Parrot\\')\\n        .replaceAll(\\'__package_name__\\', \\'langchain-parrot-link\\')\\n        .replaceAll(\\'__MODULE_NAME__\\', \\'PARROT\\')\\n        .replaceAll(\\'__module_name__\\', \\'langchain_parrot_link\\')\\n}\\n</CodeBlock>\\n\\n    </TabItem>\\n</Tabs>\\n\\n---\\n\\n### Running the tests\\n\\nYou can run these with the following commands from your project root\\n\\n<Tabs>\\n    <TabItem value=\"poetry\" label=\"Poetry\" default>\\n\\n```bash\\n# run unit tests without network access\\npoetry run pytest --disable-socket --allow-unix-socket --asyncio-mode=auto tests/unit_tests\\n\\n# run integration tests\\npoetry run pytest --asyncio-mode=auto tests/integration_tests\\n```\\n\\n    </TabItem>\\n    <TabItem value=\"pip\" label=\"Pip\">\\n\\n```bash\\n# run unit tests without network access\\npytest --disable-socket --allow-unix-socket --asyncio-mode=auto tests/unit_tests'), Document(metadata={'source': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_path': 'docs/docs/contributing/how_to/integrations/standard_tests.mdx', 'file_name': 'standard_tests.mdx', 'file_type': '.mdx'}, page_content='# run integration tests\\npytest --asyncio-mode=auto tests/integration_tests\\n```\\n\\n    </TabItem>\\n</Tabs>\\n\\n## Test suite information and troubleshooting\\n\\nFor a full list of the standard test suites that are available, as well as\\ninformation on which tests are included and how to troubleshoot common issues,\\nsee the [Standard Tests API Reference](https://python.langchain.com/api_reference/standard_tests/index.html).\\n\\nYou can see troubleshooting guides under the individual test suites listed in that API Reference. For example,\\n[here is the guide for `ChatModelIntegrationTests.test_usage_metadata`](https://python.langchain.com/api_reference/standard_tests/integration_tests/langchain_tests.integration_tests.chat_models.ChatModelIntegrationTests.html#langchain_tests.integration_tests.chat_models.ChatModelIntegrationTests.test_usage_metadata).'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Vectara\\n\\n>[Vectara](https://vectara.com/) provides a Trusted Generative AI platform, allowing organizations to rapidly create a ChatGPT-like experience (an AI assistant) \\n> which is grounded in the data, documents, and knowledge that they have (technically, it is Retrieval-Augmented-Generation-as-a-service).'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='**Vectara Overview:**\\n[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications.\\nVectara serverless RAG-as-a-service provides all the components of RAG behind an easy-to-use API, including:\\n1. A way to extract text from files (PDF, PPT, DOCX, etc)\\n2. ML-based chunking that provides state of the art performance.\\n3. The [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model.\\n4. Its own internal vector database where text chunks and embedding vectors are stored.\\n5. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments, including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) as well as multiple reranking options such as the [multi-lingual relevance reranker](https://www.vectara.com/blog/deep-dive-into-vectara-multilingual-reranker-v1-state-of-the-art-reranker-across-100-languages), [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/), [UDF reranker](https://www.vectara.com/blog/rag-with-user-defined-functions-based-reranking). \\n6. An LLM to for creating a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents (context), including citations.'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='For more information:\\n- [Documentation](https://docs.vectara.com/docs/)\\n- [API Playground](https://docs.vectara.com/docs/rest-api/)\\n- [Quickstart](https://docs.vectara.com/docs/quickstart)\\n\\n## Installation and Setup\\n\\nTo use `Vectara` with LangChain no special installation steps are required. \\nTo get started, [sign up](https://vectara.com/integrations/langchain) for a free Vectara trial,\\nand follow the [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \\nOnce you have these, you can provide them as arguments to the Vectara `vectorstore`, or you can set them as environment variables.\\n\\n- export `VECTARA_CUSTOMER_ID`=\"your_customer_id\"\\n- export `VECTARA_CORPUS_ID`=\"your_corpus_id\"\\n- export `VECTARA_API_KEY`=\"your-vectara-api-key\"\\n\\n## Vectara as a Vector Store\\n\\nThere exists a wrapper around the Vectara platform, allowing you to use it as a `vectorstore` in LangChain:'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='To import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import Vectara\\n```\\n\\nTo create an instance of the Vectara vectorstore:\\n```python\\nvectara = Vectara(\\n    vectara_customer_id=customer_id, \\n    vectara_corpus_id=corpus_id, \\n    vectara_api_key=api_key\\n)\\n```\\nThe `customer_id`, `corpus_id` and `api_key` are optional, and if they are not supplied will be read from \\nthe environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`, respectively.\\n\\n### Adding Texts or Files\\n\\nAfter you have the vectorstore, you can `add_texts` or `add_documents` as per the standard `VectorStore` interface, for example:\\n\\n```python\\nvectara.add_texts([\"to be or not to be\", \"that is the question\"])\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Since Vectara supports file-upload in the platform, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly. \\nWhen using this method, each file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don\\'t have to use the LangChain document loader or chunking mechanism.\\n\\nAs an example:\\n\\n```python\\nvectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\\n```\\n\\nOf course you do not have to add any data, and instead just connect to an existing Vectara corpus where data may already be indexed.\\n\\n### Querying the VectorStore\\n\\nTo query the Vectara vectorstore, you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:\\n```python\\nresults = vectara.similarity_search_with_score(\"what is LangChain?\")\\n```\\nThe results are returned as a list of relevant documents, and a relevance score of each document.'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='In this case, we used the default retrieval parameters, but you can also specify the following additional arguments in `similarity_search` or `similarity_search_with_score`:\\n- `k`: number of results to return (defaults to 5)\\n- `lambda_val`: the [lexical matching](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) factor for hybrid search (defaults to 0.025)\\n- `filter`: a [filter](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview) to apply to the results (default None)\\n- `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2.\\n- `rerank_config`: can be used to specify reranker for thr results\\n   - `reranker`: mmr, rerank_multilingual_v1 or none. Note that \"rerank_multilingual_v1\" is a Scale only feature\\n   - `rerank_k`: number of results to use for reranking\\n   - `mmr_diversity_bias`: 0 = no diversity, 1 = full diversity. This is the lambda parameter in the MMR formula and is in the range 0...1'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='To get results without the relevance score, you can simply use the \\'similarity_search\\' method:\\n```python   \\nresults = vectara.similarity_search(\"what is LangChain?\")\\n```\\n\\n## Vectara for Retrieval Augmented Generation (RAG)'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content=\"Vectara provides a full RAG pipeline, including generative summarization. To use it as a complete RAG solution, you can use the `as_rag` method.\\nThere are a few additional parameters that can be specified in the `VectaraQueryConfig` object to control retrieval and summarization:\\n* k: number of results to return\\n* lambda_val: the lexical matching factor for hybrid search\\n* summary_config (optional): can be used to request an LLM summary in RAG\\n   - is_enabled: True or False\\n   - max_results: number of results to use for summary generation\\n   - response_lang: language of the response summary, in ISO 639-2 format (e.g. 'en', 'fr', 'de', etc)\\n* rerank_config (optional): can be used to specify Vectara Reranker of the results\\n   - reranker: mmr, rerank_multilingual_v1 or none\\n   - rerank_k: number of results to use for reranking\\n   - mmr_diversity_bias: 0 = no diversity, 1 = full diversity. \\n     This is the lambda parameter in the MMR formula and is in the range 0...1\\n\\nFor example:\"), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n```\\nThen you can use the `as_rag` method to create a RAG pipeline:\\n\\n```python\\nquery_str = \"what did Biden say?\"\\n\\nrag = vectara.as_rag(config)\\nrag.invoke(query_str)[\\'answer\\']\\n```\\n\\nThe `as_rag` method returns a `VectaraRAG` object, which behaves just like any LangChain Runnable, including the `invoke` or `stream` methods.\\n\\n## Vectara Chat\\n\\nThe RAG functionality can be used to create a chatbot. For example, you can create a simple chatbot that responds to user input:'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n\\nquery_str = \"what did Biden say?\"\\nbot = vectara.as_chat(config)\\nbot.invoke(query_str)[\\'answer\\']\\n```\\n\\nThe main difference is the following: with `as_chat` Vectara internally tracks the chat history and conditions each response on the full chat history.\\nThere is no need to keep that history locally to LangChain, as Vectara will manage it internally.\\n\\n## Vectara as a LangChain retriever only\\n\\nIf you want to use Vectara as a retriever only, you can use the `as_retriever` method, which returns a `VectaraRetriever` object.\\n```python\\nretriever = vectara.as_retriever(config=config)\\nretriever.invoke(query_str)\\n```'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='Like with as_rag, you provide a `VectaraQueryConfig` object to control the retrieval parameters.\\nIn most cases you would not enable the summary_config, but it is left as an option for backwards compatibility. \\nIf no summary is requested, the response will be a list of relevant documents, each with a relevance score.\\nIf a summary is requested, the response will be a list of relevant documents as before, plus an additional document that includes the generative summary.\\n\\n## Hallucination Detection score\\n\\nVectara created [HHEM](https://huggingface.co/vectara/hallucination_evaluation_model) - an open source model that can be used to evaluate RAG responses for factual consistency. \\nAs part of the Vectara RAG, the \"Factual Consistency Score\" (or FCS), which is an improved version of the open source HHEM is made available via the API. \\nThis is automatically included in the output of the RAG pipeline'), Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n\\nrag = vectara.as_rag(config)\\nresp = rag.invoke(query_str)\\nprint(resp[\\'answer\\'])\\nprint(f\"Vectara FCS = {resp[\\'fcs\\']}\")\\n```\\n\\n## Example Notebooks\\n\\nFor a more detailed examples of using Vectara with LangChain, see the following example notebooks:\\n* [this notebook](/docs/integrations/vectorstores/vectara) shows how to use Vectara: with full RAG or just as a retriever.\\n* [this notebook](/docs/integrations/retrievers/self_query/vectara_self_query) shows the self-query capability with Vectara.\\n* [this notebook](/docs/integrations/providers/vectara/vectara_chat) shows how to build a chatbot with Langchain and Vectara'), Document(metadata={'source': 'docs/docs/integrations/retrievers/self_query/index.mdx', 'file_path': 'docs/docs/integrations/retrievers/self_query/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar-position: 0\\n---\\n\\n# Self-querying retrievers\\n\\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query).\\n\\nimport DocCardList from \"@theme/DocCardList\";\\n\\n<DocCardList />')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqJ29UqH58c"
      },
      "source": [
        "### Embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "b-tYk8ruH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2_NJu6xH58c",
        "outputId": "a0518fa8-786c-4724-a317-242e166c8bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n",
            "[0.020096343010663986, -0.008024655282497406, 0.033230509608983994, -0.02780599147081375, 0.046300604939460754, 0.025371365249156952, -0.01656186394393444, 0.01854800619184971, 0.02675952948629856, -0.021751461550593376, 0.010160292498767376, -0.011222771368920803, -0.015526079572737217, -0.005072137340903282, -0.011778037063777447, 0.06244602054357529, 0.0359000563621521, -0.00035404853406362236, -0.04642874374985695, 0.02588391862809658, 0.03083859570324421, 0.03453324735164642, -0.03901808336377144, 0.021847564727067947, 0.01714916341006756, -0.018622752279043198, -0.020277870818972588, 0.030112478882074356, -0.008494495414197445, -0.10310854762792587, -0.008264914155006409, -0.057363204658031464, -0.03444782271981239, 0.04728299751877785, -0.02212519757449627, 0.03502444177865982, 0.02195434644818306, 0.011308196932077408, -0.007399981375783682, -0.03256846219301224, 0.0006660517537966371, -0.02133501134812832, 0.020886527374386787, 0.01245076209306717, 0.006892767734825611, 0.005416508764028549, 0.007004888728260994, -0.0026909024454653263, -0.046215180307626724, 0.00823821872472763, -0.037437714636325836, 0.0331023707985878, -0.0020288550294935703, -0.022253334522247314, 0.000585965346544981, 0.04907693341374397, -0.018857672810554504, 0.025328652933239937, 0.001668466255068779, -0.019808031618595123, 0.01725594513118267, 0.004781156778335571, -0.050571877509355545, 0.06453894078731537, -0.0007174405036494136, 0.03438375145196915, 0.0023772306740283966, -0.03092402033507824, -0.06603389233350754, 0.004180509131401777, -0.0390607975423336, 0.039338428527116776, 0.032269470393657684, -0.04130321741104126, -0.007864482700824738, -0.014180628582835197, -0.033486783504486084, 0.03478952497243881, -0.012749752029776573, 0.02962128259241581, -0.00823287945240736, 0.05381804704666138, -0.024986950680613518, -0.05116985738277435, 0.024154052138328552, 0.017384083941578865, 0.006422927603125572, 0.028040912002325058, -0.0015670234570279717, 0.009263324551284313, 0.04036353528499603, 0.01673271507024765, -0.026631390675902367, 0.02449575439095497, 0.013262304477393627, 0.03867638111114502, 0.027720564976334572, -0.04316122084856033, -0.003126038471236825, 0.02752835862338543, -0.020929239690303802, -0.030368754640221596, 0.03440510854125023, -0.03782212734222412, 0.06411181390285492, -0.0604812353849411, 0.0414954237639904, -0.0013207579031586647, 0.028788384050130844, 0.0031607423443347216, -0.056765224784612656, -0.00023675535339862108, -0.024367615580558777, -0.0351739376783371, 0.013838926330208778, -0.012920602224767208, 0.02117483876645565, 0.01899649016559124, -0.0032995587680488825, -0.0024372953921556473, -0.03357221186161041, 0.037501782178878784, -0.01628423109650612, -0.03831332549452782, 0.006967515219002962, -0.0383346788585186, 0.00012922270980197936, -0.06223245710134506, -0.037587206810712814, -0.00029014627216383815, 0.05915714055299759, -0.017362726852297783, 0.08516919612884521, -0.06458165496587753, 0.05390347167849541, -0.01567557454109192, 0.00017418788047507405, -0.027934128418564796, 0.00040710577741265297, 0.012183807790279388, 0.0189217422157526, -0.02021380141377449, 0.02848939411342144, 0.028809741139411926, -0.046129755675792694, 0.018676143139600754, 0.017202554270625114, -0.01872953400015831, -0.04651416838169098, -0.03709601238369942, -0.05821745842695236, -0.020502112805843353, -0.0326111726462841, -0.04207204654812813, -0.06193346530199051, 0.003243498271331191, -0.01699966937303543, 0.025755779817700386, 0.035323433578014374, 0.020085664466023445, -0.056338097900152206, -0.01830240711569786, 0.018441224470734596, 0.018836315721273422, 0.002885779133066535, 0.016775427386164665, 0.02455982379615307, -0.01985074393451214, -0.018110200762748718, -0.06031038239598274, -0.005056119989603758, -0.011094633489847183, -0.01664728857576847, 0.004564923699945211, 0.014661146327853203, -0.031073516234755516, -0.018163591623306274, -0.018142234534025192, -0.002939170226454735, -0.030988089740276337, -0.007442694157361984, -0.04608704149723053, -0.025072375312447548, 0.04715485870838165, -0.04706943407654762, -0.021932989358901978, -0.02359878644347191, 0.05847373604774475, -0.043695129454135895, 0.007773717865347862, -0.03372170403599739, 0.07803616672754288, -0.008510512299835682, 0.015387263149023056, 0.02535000815987587, -0.007629562634974718, -0.0033396021462976933, -0.006577761378139257, 0.012707038782536983, -0.00426860386505723, 0.03137250617146492, 0.0003827461623586714, 0.017533577978610992, 0.013646719045937061, -0.020096343010663986, 0.030112478882074356, -0.02162332274019718, 0.007880499586462975, -0.017458830028772354, -0.0379716232419014, 0.013048741035163403, 0.04724028706550598, -0.019113948568701744, 0.019274121150374413, -0.024965593591332436, -0.028702959418296814, 0.018345119431614876, 0.07060415297746658, -0.0002943174331448972, 0.017352048307657242, -0.006657847668975592, 0.03630582615733147, -0.011532438918948174, -0.0006677202181890607, -0.02003227360546589, 0.035601064562797546, -0.02127094194293022, -0.05582554638385773, 0.007885838858783245, -0.027037162333726883, 0.05181054770946503, 0.0006313476478680968, -0.011137345805764198, 0.016487115994095802, -0.008553225547075272, 0.009823929518461227, -0.05642352253198624, 0.03686109185218811, -0.011083954945206642, -0.04147406667470932, -0.029920270666480064, -0.014938779175281525, -0.026439184322953224, 0.03425561264157295, -0.03651938959956169, -0.028895165771245956, -0.008264914155006409, -0.019145984202623367, 0.010715557262301445, 0.05061459168791771, 0.007255826145410538, -0.021997058764100075, 0.031586069613695145, -0.05202411115169525, -0.04835081845521927, -0.017320014536380768, -0.015312516130506992, 0.0017325353110209107, 0.0008062028791755438, -0.0062734331004321575, 0.014137915335595608, 0.023769637569785118, 0.00564342038705945, 0.022808600217103958, -0.000880950188729912, -0.02989891543984413, 0.0371173657476902, -0.00429263012483716, 0.00012254885223228484, 0.028169048950076103, 0.02039533108472824, -0.0128992460668087, 0.06436809152364731, -0.017928671091794968, -0.007779057137668133, 0.03021926060318947, 0.0019888116512447596, -0.054501451551914215, 0.030368754640221596, 0.033764418214559555, 0.009236629121005535, 0.006444284226745367, -0.02552085928618908, 0.03572920337319374, -0.006972854025661945, 0.009247307665646076, -0.031393859535455704, -0.009770538657903671, -0.061036501079797745, 0.006268093828111887, 0.005221631843596697, -0.02317165955901146, 0.016967633739113808, -0.01595320738852024, 0.010598097927868366, 0.021559253334999084, -0.0135079026222229, 0.007469389587640762, 0.018110200762748718, -0.004719757474958897, 0.011212092824280262, 0.03923164680600166, 0.014127237722277641, 0.010010797530412674, -0.012098382227122784, -0.0037880856543779373, -0.05462959036231041, -0.04014997184276581, 0.030261972919106483, -0.019274121150374413, -0.007309217005968094, 0.02761378325521946, 0.0038494854234158993, 0.01455436460673809, -0.002098263241350651, -0.0531773567199707, -0.01619880460202694, -0.0023905783891677856, 0.025926630944013596, -0.016049310564994812, 0.019893456250429153, 0.008953657001256943, -0.02054482512176037, -0.009973423555493355, -0.06099378690123558, 0.012034313753247261, -0.04228560999035835, -0.019637180492281914, -0.028702959418296814, -0.006300128530710936, 0.02710123173892498, 0.011863462626934052, 0.018526649102568626, -0.006780646741390228, 0.01630558632314205, -0.010822339914739132, -0.006310806609690189, 0.01881496049463749, -0.003523800754919648, -0.01681813970208168, -0.04583076387643814, 0.02569171041250229, -0.007410659454762936, -0.04459209740161896, 0.021708747372031212, 0.0068607330322265625, -0.010090883821249008, 0.04136728495359421, 0.01341179944574833, -0.021634001284837723, 0.018708178773522377, 0.0073839640244841576, 0.0016564532415941358, 0.010475298389792442, 0.016625933349132538, 0.00810474157333374, -0.02806226722896099, -0.06398367881774902, 0.014180628582835197, -0.057277780026197433, -0.021932989358901978, 0.01647643744945526, 0.015728965401649475, -0.0007027579704299569, 0.009231289848685265, 0.01708509400486946, 0.019060557708144188, 0.00340901012532413, 0.027079874649643898, -0.016508473083376884, 0.03446917608380318, 0.013305016793310642, 0.010411228984594345, -0.035408858209848404, -0.0001281382137676701, 0.011500404216349125, -0.03519529476761818, 0.019466329365968704, -0.019989561289548874, 0.02185824327170849, -0.011778037063777447, 0.04173034429550171, 0.023833706974983215, 0.017800532281398773, 0.010571402497589588, 0.02100398764014244, -0.015269802883267403, 0.02438897266983986, 0.01848393678665161, -0.03186370059847832, -0.0062467376701533794, -0.010421907529234886, -0.026610033586621284, 0.02517915703356266, -0.015312516130506992, -0.014864032156765461, 0.008211523294448853, -0.0009283346007578075, 0.0046049668453633785, 0.008457121439278126, 0.005931731313467026, 0.029407719150185585, 0.03109487146139145, 0.0062734331004321575, 0.022595036774873734, 0.0195944681763649, 0.024089982733130455, -0.0059424093924462795, 0.00928468070924282, -0.05984054133296013, 0.020000237971544266, -0.007885838858783245, 0.028702959418296814, -0.036711595952510834, -0.007325234357267618, 0.02118551731109619, -0.04249917343258858, 0.010854373686015606, -0.02255232445895672, 0.038911301642656326, 0.009348750114440918, -0.01442622672766447, -0.025243226438760757, -0.031500641256570816, -0.03231218457221985, 0.02413269504904747, 0.07244080305099487, -0.034212902188301086, 0.0208117812871933, -0.010042832233011723, 0.028980590403079987, -0.03570784628391266, 0.04984576255083084, 0.019893456250429153, -0.03613497316837311, 0.016423046588897705, 0.001954107778146863, -0.018953775987029076, 0.03694651648402214, -0.01888970658183098, 0.008772128261625767, 0.007773717865347862, -0.04217882826924324, 0.035494282841682434, -0.07060415297746658, 0.014041812159121037, 0.021484505385160446, 0.05796118080615997, 0.0006760625401511788, 0.002696241484954953, 0.06261686980724335, -0.0475819893181324, -0.016070667654275894, -0.01619880460202694, -0.013262304477393627, -0.009898676536977291, -0.018355797976255417, -0.04608704149723053, 0.0647525042295456, -0.0303046852350235, 0.004954677540808916, 0.04484837129712105, -0.03434104099869728, 0.016529828310012817, -0.015803711488842964, -0.04668502137064934, 0.03910350799560547, 0.020630251616239548, 0.01995752565562725, 0.004914634395390749, 0.0294504314661026, 0.009001709520816803, -0.027250725775957108, 0.014565043151378632, -0.03649803251028061, -0.023492004722356796, -0.016935599967837334, -0.03250439092516899, -0.020363297313451767, -0.029685351997613907, 0.013443833217024803, 0.01647643744945526, -0.03502444177865982, -0.03024061769247055, 0.022787244990468025, 0.11002800613641739, -0.016487115994095802, 0.03265388682484627, 0.010704879648983479, 0.023235728964209557, -0.016487115994095802, 0.018441224470734596, 0.033337291330099106, -0.005755541380494833, -0.0020595546811819077, -0.015985241159796715, -0.04100422561168671, -0.03844146057963371, -0.026353757828474045, 0.002199705922976136, -0.03961606323719025, -0.06265958398580551, -0.04941863566637039, 0.010245717130601406, -0.02066228538751602, -0.08952589333057404, -0.0686393678188324, 0.007848464883863926, -0.011308196932077408, -0.054757729172706604, 0.020448721945285797, 0.05322006717324257, -0.008617294020950794, 0.018846994265913963, -0.019231408834457397, -0.0019073906587436795, -0.0009530279203318059, 0.028702959418296814, 0.004265934694558382, 0.014970813877880573, -0.02866024523973465, -0.0026188245974481106, 0.0391889363527298, -0.04907693341374397, -0.009231289848685265, 0.042050689458847046, -0.042200181633234024, -0.04425039514899254, 0.021997058764100075, -0.003985632210969925, -0.0026722154580056667, -0.05279294028878212, -0.01198092196136713, -0.024282190948724747, -0.007255826145410538, -0.05984054133296013, 0.020000237971544266, -0.014607755467295647, -0.019840065389871597, -0.013358408585190773, 0.011180059053003788, 0.017138484865427017, -0.026716817170381546, 0.03677566722035408, 0.02857482060790062, -0.025926630944013596, 0.003417018800973892, -0.04228560999035835, -0.0010384534252807498, 0.015835747122764587, -0.0077897352166473866, 0.0007294534589163959, 0.015921171754598618, 0.017191875725984573, 0.03816382959485054, 0.004658357705920935, 0.0011505743023008108, 0.02814769372344017, -0.03730957582592964, 0.010432586073875427, -0.02517915703356266, -0.0045035239309072495, -0.02054482512176037, 0.008393052965402603, -0.007997959852218628, -0.0027496323455125093, -0.025777136906981468, -0.06624745577573776, -0.017651038244366646, 0.014586399309337139, 0.00594774866476655, 0.04446395859122276, 0.006812681443989277, 0.06667458266019821, -0.002965865656733513, 0.008643990382552147, 0.027421576902270317, -0.052237674593925476, -0.006390892900526524, -0.001900716801173985, 0.015568791888654232, -0.023684212937951088, -0.01856936141848564, 0.018163591623306274, -0.012514831498265266, 0.020587539300322533, 0.04634331911802292, -0.03406340628862381, -0.013326373882591724, -0.011254806071519852, 0.01441554818302393, -0.011137345805764198, 0.011468369513750076, -0.010518011637032032, 0.01154311653226614, -0.015323193743824959, 0.01768307201564312, 0.010907764546573162, -0.05714964121580124, 0.022338761016726494, 0.04252052679657936, -0.037523139268159866, 0.0060812258161604404, 0.0374804250895977, -0.0037880856543779373, 0.01636965572834015, 0.008745432831346989, -0.01567557454109192, 0.03199183940887451, 0.022595036774873734, 0.056338097900152206, -0.003152733901515603, -0.034704096615314484, -0.038291968405246735, -0.0007728335913270712, 0.025371365249156952, -0.01924208737909794, 0.01063547097146511, -0.002629502909258008, 0.010106900706887245, -0.014362157322466373, -0.004805182572454214, 0.029728064313530922, 0.01276042964309454, -0.0346827432513237, -0.003702660324051976, -0.03476816788315773, 0.013347730040550232, -0.059541553258895874, 0.04173034429550171, -0.07068958133459091, 0.05052916705608368, -0.004762470256537199, -0.008969674818217754, 0.028959235176444054, 0.013219592161476612, 0.028468038886785507, -0.006262755021452904, -0.024986950680613518, 0.026076124981045723, -0.00760286720469594, -0.002960526617243886, 0.025157801806926727, -0.02404727041721344, 0.03756584972143173, -0.0034517229069024324, -0.016337621957063675, 0.03572920337319374, -0.0014695851132273674, -0.019498363137245178, -0.009386124089360237, -0.01546201016753912, 0.012279911898076534, 0.0007394642452709377, 0.018366476520895958, 0.0037720685359090567, -0.005085485056042671, -0.012867211364209652, 0.020587539300322533, 0.003216802841052413, -0.017480187118053436, -0.04817996546626091, -0.018430545926094055, 0.02665274776518345, -0.015963885933160782, -0.0056113856844604015, 0.005846305750310421, 0.031073516234755516, 0.009466210380196571, 0.012408049777150154, -0.005525960121303797, 0.006839376874268055, 0.02362014353275299, -0.018975133076310158, 0.01630558632314205, -0.018227659165859222, -0.004954677540808916, 0.0026108159217983484, -0.007303877733647823, 0.0202031247317791, -0.007314555812627077, 0.013582649640738964, -0.0016350969672203064, -0.0533054955303669, 0.016625933349132538, -0.012279911898076534, -0.002004829002544284, 0.008046011440455914, 0.03165013715624809, -0.0014722546329721808, 0.022595036774873734, -0.010897086933255196, -0.009834607131779194, -0.041602205485105515, 7.295368413906544e-05, 0.0315219983458519, -0.012482796795666218, 0.01891106367111206, -0.023684212937951088, 0.01363604050129652, -0.002528059994801879, -0.03632718324661255, -0.010352499783039093, -0.022189266979694366, -0.027165299281477928, -0.020502112805843353, -0.008307627402245998, -0.004917303565889597, 0.009343410842120647, -0.026951735839247704, -0.05134071037173271, 0.002189027611166239, -0.03827061131596565, -0.012685682624578476, -0.029834846034646034, -0.03004840947687626, 0.0074587115086615086, -0.008126097731292248, -0.030881308019161224, 0.007234469521790743, -0.0037880856543779373, 0.010122918523848057, -0.009898676536977291, -0.04702672362327576, 0.03021926060318947, 0.010784965939819813, 0.0033209151588380337, 0.0026028072461485863, 0.00349710532464087, 0.02840396948158741, 0.03666888549923897, -0.024773387238383293, -0.02902330458164215, 0.008457121439278126, 0.0032595156226307154, -0.011254806071519852, -0.011222771368920803, 0.03496037423610687, -0.03788619861006737, -0.016529828310012817, -0.032717954367399216, -0.0029365005902945995, 0.04869252070784569, 0.04467752203345299, -0.022851314395666122, -0.023684212937951088, -9.393465006724e-05, 0.06838309019804001, -0.005120189394801855, -0.01656186394393444, 0.0165511853992939, 0.0038574938662350178, 0.008222201839089394, -0.029471788555383682, 0.016924921423196793, -0.008932300843298435, 0.006908785086125135, -0.007480067666620016, -0.001234665047377348, -0.007949908263981342, 0.009823929518461227, 0.037160079926252365, 0.02673817239701748, 0.02562764100730419, -0.01656186394393444, 0.026353757828474045, -0.06701628118753433, 0.0014922762056812644, 0.02682359889149666, -0.013700109906494617, 0.0035718525759875774, 0.003705329727381468, 0.05360448360443115, 0.0025200513191521168, 0.006940819788724184, 0.010288430377840996, -0.005307057406753302, -0.01697831228375435, -0.009380784817039967, -0.024346260353922844, -0.014127237722277641, -0.0023091572802513838, -0.0056861331686377525, -0.029300937429070473, -0.024623891338706017, 0.026866311207413673, -0.040812019258737564, 0.0327393114566803, 0.009391462430357933, -0.023812349885702133, -0.023769637569785118, -0.015323193743824959, 0.015494044870138168, -0.006609796080738306, 0.005563333630561829, -0.008638651110231876, -0.016967633739113808, -0.016572540625929832, -0.03921028971672058, -0.0025507512036710978, -0.010304447263479233, -0.00444479426369071, -0.0003390323545318097, 0.033935267478227615, 0.1120782196521759, -0.0046183145605027676, -0.03406340628862381, 0.002411934779956937, -0.04467752203345299, -0.030710456892848015, -0.029236868023872375, 0.019551753997802734, -0.003905545687302947, 0.0161881260573864, -0.021014666184782982, -0.002661537379026413, -0.007020906079560518, 0.007907195016741753, 0.008926961570978165, 0.04224289581179619, 0.026439184322953224, -0.027143944054841995, -0.012141095474362373, -0.014565043151378632, 0.03709601238369942, 0.00411643972620368, 0.03572920337319374, 0.00919391680508852, -0.02073703333735466, -0.052237674593925476, 0.014041812159121037, 0.04249917343258858, 0.019327513873577118, -0.009065777994692326, 0.05010204017162323, 0.017917992547154427, 0.0006236726767383516, -0.05343363434076309, -0.03572920337319374, -0.03128707781434059, -0.04425039514899254, -0.016348300501704216, 0.017362726852297783, -0.02885245345532894, -0.00526968389749527, -0.011083954945206642, -0.02150586247444153, 0.02291538193821907, -0.005112180486321449, -0.0660765990614891, 0.014928101561963558, 0.005905035883188248, -0.030475536361336708, -0.015483366325497627, 0.021484505385160446, 0.06223245710134506, -0.012397371232509613, -0.04749656096100807, -0.0258625615388155, 0.018750891089439392, 0.01750154420733452, 0.01777917705476284, -0.03856959939002991, 0.03205590695142746, 0.013646719045937061, 0.014201984740793705, 0.012418728321790695, -2.5590101358829997e-05, 0.034725453704595566, 0.006241398397833109, 0.03231218457221985, 0.023833706974983215, -0.0411323644220829, 0.003908215556293726, -0.007421337999403477, -0.040299467742443085, -0.005483247339725494, 0.060865648090839386, -0.018622752279043198, 0.003470409894362092, -0.03214133530855179, -0.005144215188920498, -0.05219496414065361, 0.03041146881878376, -0.010603436268866062, -0.03630582615733147, 0.026076124981045723, -0.007405320648103952, 0.035515639930963516, 0.02327844128012657, -0.01802477426826954, 0.023492004722356796, -0.013230269774794579, -0.00525099691003561, 0.02492288127541542, 0.021751461550593376, -0.011575151234865189, 0.008286270312964916, 0.0019167340360581875, 0.02351336181163788, 0.009300698526203632, 0.011468369513750076, -0.0004681716382037848, 0.010736914351582527, 0.03049689345061779, -0.009898676536977291, 0.012877889908850193, 0.04715485870838165, -0.006353519391268492, -0.013593328185379505, 0.013497225008904934, 0.016049310564994812, 0.0009550300892442465, -0.02475203014910221, 0.0282331183552742, 0.0005939739639870822, -0.012247877195477486, 0.023726925253868103, 0.0023011486046016216, -0.007084975019097328, 0.028809741139411926, -0.016796782612800598, 0.01198092196136713, 0.018345119431614876, -0.003360958304256201, -0.006150634028017521, 0.00165511853992939, -0.040812019258737564, -0.017298657447099686, -0.0020101680420339108, 0.009781216271221638, -0.005194936413317919, -0.03739500045776367, 0.006577761378139257, 0.00668454309925437, -0.05035831406712532, 0.03583598509430885, -0.0125041538849473, -0.00102910993155092, 0.0024399650283157825, -0.0035852002911269665, 0.025905273854732513, 0.014319445006549358, 0.0032782023772597313, 0.03773670271039009, -0.019722605124115944, 0.020598215982317924, -0.01864410936832428, 0.008724076673388481, -0.005064128898084164, -0.011350909247994423, -0.006866072304546833, 0.02866024523973465, -0.028446681797504425, -0.010555384680628777, -0.0029818827752023935, 0.06432537734508514, -0.00464234035462141, 0.03128707781434059, 0.0071009923703968525, 0.01995752565562725, -0.004394072573632002, -0.005125528201460838, -0.014799962751567364, 0.020331261679530144, 0.0004338012367952615, 0.008312966674566269, 0.024025913327932358, -0.01664728857576847, -0.011404300108551979, -0.00793389044702053, 0.038548242300748825, -0.016017276793718338, 0.025157801806926727, -0.027122586965560913, -0.0013254295336082578, 0.021281620487570763, -0.032013196498155594, 0.011083954945206642, 0.018590718507766724, 0.04800911620259285, 0.022509612143039703, 0.016401691362261772, -0.017298657447099686, 0.007346590515226126, 0.0011572481598705053, 0.013080775737762451, -0.01986142247915268, 0.03805704787373543, 0.005590029526501894, 0.023641498759388924, 0.004313986282795668, -0.028040912002325058, -0.010614114813506603, 0.002680224133655429, -0.004968025255948305, 0.012408049777150154, -0.020373975858092308, 0.019145984202623367, 0.021132126450538635, -0.012664326466619968, -0.02806226722896099, -7.132693281164393e-05, 0.01663661003112793, 0.023748280480504036, 0.030603675171732903, 0.010213683359324932, -0.01734137162566185, -0.010421907529234886, 0.0122905895113945, 0.012536187656223774, 0.017917992547154427, -0.003230150556191802, 0.0141165591776371, -0.013390442356467247, -0.056508950889110565, -0.04446395859122276, -0.005261674989014864, -0.006823359522968531, 0.0075227804481983185, 0.003019256517291069, -0.035067155957221985, -0.04775283858180046, 0.04467752203345299, -0.011500404216349125, 0.022979451343417168, 0.0014775936724618077, 0.01611337997019291, 0.06992074847221375, 0.03963742032647133, 0.010550045408308506, 0.02100398764014244, 0.0002294141158927232, 0.028190406039357185, -0.008649328723549843, 0.03363627940416336, -0.003721347078680992, 0.03583598509430885, 0.03728821873664856, -0.010945138521492481, 0.0004384729254525155, 0.005363117903470993, 0.011372266337275505, 0.007528119720518589, -0.012130416929721832, 0.0475819893181324, 0.018857672810554504, 0.042114757001399994, -0.02752835862338543, 0.04655688256025314, -0.0010431250557303429, 0.011746002361178398, -0.05279294028878212, 0.01663661003112793, 0.0362631119787693, 0.02351336181163788, 0.02927958033978939, 0.011051920242607594, 0.01428741030395031, -0.012408049777150154, 0.028895165771245956, 0.018174268305301666, 0.02308623306453228, -0.011735323816537857, -0.018313085660338402, -0.004060379229485989, 0.001413524616509676, 0.014949457719922066, 0.03617768734693527, 0.01233330275863409, 0.009428836405277252, -0.0033262541983276606, 0.033508140593767166, 0.008798823691904545, -0.006470979657024145, -0.009818590246140957, -0.03162878006696701, -0.0023398571647703648, -0.010315125808119774, 0.008056689985096455, -0.02866024523973465, 0.007111670449376106, -0.013646719045937061, -0.007923212833702564, -0.017352048307657242, 0.014565043151378632, 0.011628542095422745, -0.04382326826453209, 0.04390869289636612, -0.025777136906981468, 0.03291016444563866, -0.013582649640738964, -0.0014655807754024863, -0.005782236810773611, -0.0062734331004321575, 0.03355085477232933, 0.009738503955304623, -0.014223340898752213, -0.03024061769247055, 0.01813155598938465, 0.005237649194896221, -0.020448721945285797, 0.014095203019678593, -0.0021983711048960686, -0.0023144963197410107, -0.0038628331385552883, -0.018366476520895958, 0.015355228446424007, 0.05800389498472214, -0.00165511853992939, 0.010010797530412674, -0.0025934639852494, -0.030133835971355438, 0.008120758458971977, 0.05497129261493683, -0.01154311653226614, -0.007656258065253496, -0.010523349978029728, -0.01233330275863409, -0.024324903264641762, 0.011500404216349125, 0.019359547644853592, -0.007981942966580391, -0.03256846219301224, 0.0012233194429427385, -0.01760832592844963, -0.010320465080440044, 0.010742252692580223, -0.0315219983458519, 0.0125041538849473, 0.03850553184747696, -0.00033669648109935224, -0.005080146249383688, 0.006177329458296299, 0.025563571602106094, -0.0035665135364979506, 0.010614114813506603, -0.014404870569705963, 0.010154953226447105, 0.010053509846329689, -0.005606046412140131, 0.01793934963643551, -0.04034217819571495, 0.014212663285434246, 0.019455650821328163, 0.04869252070784569, 0.009375445544719696, 0.011991600506007671, -0.01233330275863409, 0.023043520748615265, -0.04860709235072136, 0.0005883012199774384, -0.0362631119787693, 0.010656828060746193, 0.009786555543541908, 0.004791834857314825, 0.013187557458877563, 0.00018353128689341247, -0.017629681155085564, -0.02891652286052704, 0.014810641296207905, 0.012642969377338886, -0.0002220728638349101, 0.024175409227609634, 0.0034997747279703617, -0.002222397131845355, -0.02091856300830841, -0.005002729129046202, 0.0007968595018610358, -0.014661146327853203, -0.03592140972614288, -0.025157801806926727, -0.004826539196074009, 0.009119168855249882, 0.006967515219002962, -0.03004840947687626, 0.01970124989748001, 0.007560154423117638, -0.016262874007225037, 0.008761449716985226, 0.02560628578066826, -0.03094537742435932, -0.05838831141591072, 0.003131377510726452, -0.04051303118467331, -0.019583789631724358, 0.015312516130506992, -0.006465640384703875, 0.035430215299129486, -0.02998434007167816, 0.012973993085324764, -0.03649803251028061, -0.019669214263558388, 0.026631390675902367, 0.007117009721696377, -0.05236581340432167, 0.01707441546022892, -0.027827346697449684, 0.04450666904449463, 0.028190406039357185, -0.0068447161465883255, -0.005157562904059887, 0.018366476520895958, 0.010437924414873123, 0.04984576255083084, -0.0049973903223872185, 0.011222771368920803, 0.004442124627530575, 0.009391462430357933, -0.0009176564635708928, 0.02866024523973465, -0.024154052138328552, 0.02455982379615307, -0.028104979544878006, 0.004103092011064291, 0.021324332803487778, -0.0189858116209507, 0.028681602329015732, 0.02089720591902733, -0.0030352738685905933, -0.002732280408963561, 0.048478953540325165, 0.01080632209777832, 0.027037162333726883, -0.01742679625749588, 0.002965865656733513, 0.0059157139621675014, -0.004370046779513359, -0.0033209151588380337, 0.024687960743904114, 0.02989891543984413, 0.0008222201722674072, 0.034020695835351944, 0.011425657197833061, 0.01707441546022892, 0.01838783361017704, 0.028339900076389313, -0.0028430665843188763, 4.0877421270124614e-05, 0.03429832682013512, 0.03989369422197342, 0.004025675356388092, 0.023470647633075714, -0.04215747117996216, 0.004919973202049732, 0.016444403678178787, -0.0032595156226307154, 0.022424185648560524, -0.010186987929046154, -0.032717954367399216, 0.005149553995579481, 0.00398296257480979, 0.020021595060825348, 0.004410089924931526, -0.0047437832690775394, -0.005760880187153816, -0.015739643946290016, 0.029236868023872375, 0.003940249793231487, 0.020341940224170685, -0.03355085477232933, -0.002741623669862747, -0.02885245345532894, 9.89400505204685e-05, -0.0009456866537220776, 0.007378625217825174, -0.01110531110316515, -0.01097717322409153, -0.0009957406437024474, 0.004842556547373533, 0.023385223001241684, 0.007421337999403477, -0.008366357535123825, -0.006673865020275116, -0.007949908263981342, 0.01307009719312191, 0.01680746115744114, 0.013358408585190773, 0.012675004079937935, -0.025456789880990982, -0.0024613214191049337, 0.0326111726462841, 0.0068447161465883255, 0.006615134887397289, -0.005165571346879005, 0.03651938959956169, -0.03720279410481453, -0.0315219983458519, 0.004701070487499237, 0.02169807069003582, 0.01567557454109192, 0.01785392314195633, 0.030667744576931, -0.029557213187217712, 0.009466210380196571, 0.002568103140220046, -0.005296379327774048, 0.010245717130601406, -0.013230269774794579, -0.013080775737762451, 0.015494044870138168, 0.025157801806926727, -0.0027469629421830177, -0.006839376874268055, -0.014661146327853203, -0.014874710701406002, -0.012044991366565228, -0.010128257796168327, 0.026375114917755127, -0.021815529093146324, 0.015739643946290016, 0.0128992460668087, -0.02526458352804184, -0.004252586979418993, 0.02987755835056305, -0.015696929767727852, 0.020406009629368782, -0.030731813982129097, -0.0013207579031586647, 0.008665346540510654, -0.011265483684837818, -0.0044875070452690125, -0.018846994265913963, -0.005670115817338228, 0.001207302208058536, 0.0017832567682489753, 0.028019554913043976, -0.005130867473781109, 0.03502444177865982, 0.010112239979207516, 0.009177898988127708, -0.005664776545017958, 0.0025200513191521168, 0.007560154423117638, 0.02362014353275299, -0.027378864586353302, 0.010907764546573162, -0.009653078392148018, 0.024367615580558777, -0.0005679458845406771, 0.006054530385881662, 0.009546296671032906, 0.01803545281291008, 0.024410327896475792, -0.0026682112365961075, 0.008361018262803555, -0.006519031245261431, 0.00901238713413477, 0.017202554270625114, -0.003059299662709236, 0.03374306112527847, -0.010592758655548096, -0.017448153346776962, -0.042221538722515106, 0.020790424197912216, -0.013700109906494617, 0.020598215982317924, -0.00638555409386754, 0.027656497433781624, -0.020459400489926338, 0.04322528839111328, -0.0021690060384571552, -0.014682503417134285, -0.017095772549510002, 0.03344407305121422, 0.020459400489926338, 0.009103151969611645, 0.03575056046247482, -0.006294789258390665, -0.007469389587640762, -0.0040817358531057835, -0.031756918877363205, 0.05813203379511833, -0.0025067036040127277, -0.018708178773522377, 0.031927771866321564, 0.03440510854125023, -0.001668466255068779, 0.014981492422521114, -0.0037960943300276995, -0.014031133614480495, -0.004367377143353224, -0.004594288766384125, -0.01664728857576847, -0.019711928442120552, 0.07389303296804428, 0.007330573163926601, 0.016241516917943954, -0.003662617178633809, 0.047624699771404266, -0.0029712046962231398, 0.03374306112527847, 0.03850553184747696, 0.02806226722896099, 0.005483247339725494, 0.00034370404318906367, 0.01319823507219553, -0.019306156784296036, -0.013881639577448368, 0.030005697160959244, -0.003654608502984047, -0.03730957582592964, 0.03442646563053131, -0.02840396948158741, 0.011158701963722706, -0.00045482389396056533, -0.021932989358901978, 0.016145413741469383, -0.0029044661205261946, -0.006022495683282614, -0.01847325824201107, -0.013454511761665344, -0.014319445006549358, -0.02359878644347191, 0.024196764454245567, -0.0016204144340008497, 0.02648189663887024, 0.0026401809882372618, 0.02752835862338543, -0.02727208100259304, 0.00044981850078329444, -0.020320584997534752, -0.02359878644347191, -0.020192446187138557, -0.014095203019678593, -0.003019256517291069, 0.0022824618499726057, 0.008788145147264004, 0.003243498271331191, 0.0020235157571733, -0.017021024599671364, 0.0012860537972301245, 0.021399080753326416, -0.036711595952510834, -0.009487566538155079, -0.02057686075568199, -0.007923212833702564, 0.02168739214539528, 0.009711808525025845, -0.022189266979694366, 0.0071757398545742035, -0.030390111729502678, -0.011233449913561344, -0.020790424197912216, 0.010913103818893433, -0.016775427386164665, 0.003841476747766137, -0.044293105602264404, -0.013870961032807827, -0.02535000815987587, -0.007106331642717123, 0.008056689985096455, 0.009263324551284313, 0.005347100552171469, 0.010443263687193394, -0.005096163135021925, -0.02562764100730419, 0.00823287945240736, 0.026610033586621284, -0.006038513034582138, 0.01639101281762123, -0.010411228984594345, 0.017992740496993065, -0.002827049233019352, 0.028468038886785507, 0.05138342082500458, 0.010421907529234886, 0.03466138616204262, -0.021046699956059456, 0.015825068578124046, 0.009781216271221638, 0.03876180946826935, -0.009738503955304623, -0.005755541380494833, -0.008227541111409664, -0.00285374466329813, 0.02466660551726818, 0.0063481805846095085, -0.025221871212124825, 0.027208013460040092, 0.016764748841524124, 0.03940249979496002, 0.024175409227609634, 0.015611505135893822, 0.032632529735565186, -0.024431684985756874, -0.016166770830750465, -0.024367615580558777, -0.009028404951095581, -0.02684495411813259, -0.011201415210962296, 0.02727208100259304, -0.010453942231833935, -0.047880977392196655, -0.02675952948629856, 0.001430876669473946, 0.0052056144922971725, -0.009722486138343811, 0.014757250435650349, -0.00988799799233675, -0.02840396948158741, -0.009968084283173084, 0.014362157322466373, 0.03269660100340843, 0.007661596871912479, 0.012685682624578476, 0.010784965939819813, 4.846894444199279e-05, 0.0014508982421830297, -0.003139386186376214, 0.013700109906494617, -0.01742679625749588, -0.007901855744421482, -0.014746571891009808, 0.016775427386164665, -0.015440654009580612, -0.028553463518619537, -0.005400491412729025, 0.04719757288694382, -0.016241516917943954, 0.02874567173421383, -0.0059424093924462795, -0.022765887901186943, 0.002965865656733513, 0.029087373986840248, 0.027827346697449684, -0.022360118106007576, -0.006983532104641199, 0.03169285133481026, -0.0005482580163516104, -0.003892197972163558, 0.012909924611449242, -0.021730104461312294, 0.007613545283675194, -0.008126097731292248, 0.02831854298710823]\n"
          ]
        }
      ],
      "source": [
        "query = \"AWSのS3からデータを読み込むためのDocument loaderはありますか？\"\n",
        "\n",
        "vector = embeddings.embed_query(query)\n",
        "print(len(vector))\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ARkHRrH58c"
      },
      "source": [
        "### Vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aKER-Xv8H58c",
        "outputId": "7feec9d4-25e9-4a74-b9a7-4cd78942146f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma==0.1.4\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.1.4) (0.3.44)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.1.4) (1.26.4)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading posthog-3.19.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.16.0)\n",
            "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (75.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.69.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (14.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.4.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n",
            "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.19.1-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53769 sha256=6118797ee625e5a0eb794e8ad14ff50494f45b5e1e05c3af2744fa23b4dc4afa\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, importlib-metadata, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, tokenizers, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.11 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.5.0 kubernetes-32.0.1 langchain-chroma-0.1.4 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 overrides-7.7.0 posthog-3.19.1 protobuf-5.29.3 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.46.1 tokenizers-0.20.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "515ea1c40a4c428d960aeb8e8449f8bc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain-chroma==0.1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Clb_bnH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miVLRMtdH58c"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SWSEHcNH58c"
      },
      "outputs": [],
      "source": [
        "query = \"AWSのS3からデータを読み込むためのDocument loaderはありますか？\"\n",
        "\n",
        "context_docs = retriever.invoke(query)\n",
        "print(f\"len = {len(context_docs)}\")\n",
        "\n",
        "first_doc = context_docs[0]\n",
        "print(f\"metadata = {first_doc.metadata}\")\n",
        "print(first_doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMgNHrwDH58c"
      },
      "source": [
        "### LCEL を使った RAG の Chain の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gHp7uyRH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "以下の文脈だけを踏まえて質問に回答してください。\n",
        "\n",
        "文脈: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "質問: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txLjdHWQH58c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "output = chain.invoke(query)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JihEpn1JH58d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}